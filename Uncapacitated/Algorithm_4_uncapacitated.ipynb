{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c54b4ef-db78-44b2-b6bb-f5f47b1b04ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUNNING REPLICATION 1/1, seed=0 ===\n",
      "\n",
      "Results on final holdout:\n",
      "                           Method  Cost  Precision (%)  Recall (%)  \\\n",
      "0              Constant Threshold   960      20.000000       100.0   \n",
      "1               Dynamic Threshold  1008      20.000000       100.0   \n",
      "2                Linear Threshold   960      20.000000       100.0   \n",
      "3                   Wait Till End   456     100.000000       100.0   \n",
      "4  Data-Driven DP (SPSA-catboost)   345      85.714286       100.0   \n",
      "\n",
      "   Avg. Treat Time  Sum-of-CV-Cost  \n",
      "0             0.00          3840.0  \n",
      "1             2.00          4032.0  \n",
      "2             0.00          3840.0  \n",
      "3            19.00          1855.0  \n",
      "4            12.75          1389.0  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS & SETTINGS\n",
    "###############################################################################\n",
    "FP_COST = 10     # Penalty for false positive\n",
    "FN_COST = 50     # Penalty for false negative (never treat but was sick)\n",
    "D_COST  = 1      # Penalty per time-step of delay if the patient is sick and untreated\n",
    "GAMMA   = 0.99   # Discount factor\n",
    "T_MAX   = 20     # Time horizon (0..T_MAX-1)\n",
    "FEATURE_COLS = [\"time\",\"EIT\",\"NIRS\",\"EIS\"]  # Adjust for your dataset\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "###############################################################################\n",
    "# 2. SPLIT DATA & HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def split_patients_kfold(df, n_splits=4, seed=0):\n",
    "    \"\"\"Shuffle unique patient IDs, then split into n_splits+1 groups.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pts = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pts)\n",
    "    \n",
    "    n = len(unique_pts)\n",
    "    splits = {}\n",
    "    for i in range(n_splits + 1):\n",
    "        start_idx = int(i * n / (n_splits + 1))\n",
    "        end_idx   = int((i + 1) * n / (n_splits + 1))\n",
    "        group_name = f\"G{i+1}\"\n",
    "        pid_subset = unique_pts[start_idx:end_idx]\n",
    "        splits[group_name] = set(pid_subset)\n",
    "    return splits\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    \"\"\"Return rows of df whose patient_id is in pid_set.\"\"\"\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "###############################################################################\n",
    "# 3. TRAIN & PREDICT CATBOOST\n",
    "###############################################################################\n",
    "def train_and_predict_model(depth_val, lr_val, df_train, df_val, feature_cols=FEATURE_COLS):\n",
    "    \"\"\"Train CatBoost and return predicted risk scores on df_val.\"\"\"\n",
    "    X_train = df_train[feature_cols]\n",
    "    y_train = df_train[\"label\"]\n",
    "    params = {\n",
    "        \"iterations\": 50,\n",
    "        \"depth\": int(round(depth_val)),\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"verbose\": False,\n",
    "        \"random_seed\": 42\n",
    "    }\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    risk_scores = model.predict_proba(df_val[feature_cols])[:,1]\n",
    "    return risk_scores\n",
    "\n",
    "###############################################################################\n",
    "# 4. COST EVALUATION BY SIMULATION\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df must have columns: [patient_id, time, label, predicted_risk, ...].\n",
    "    policy_func(patient_rows) -> integer time step to treat, or None if never treat.\n",
    "    Returns dict with total cost, precision, recall, etc.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pid, grp in df.groupby('patient_id'):\n",
    "        grp = grp.sort_values(\"time\")\n",
    "        label = grp[\"label\"].iloc[0]  # 0 or 1 at patient-level\n",
    "        treat_time = policy_func(grp)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treated\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "            treated_flag = 0\n",
    "            tt = None\n",
    "        else:\n",
    "            # treated at treat_time\n",
    "            treated_flag = 1\n",
    "            if label == 1:\n",
    "                # cost is D_COST * treat_time\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            tt = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"label\": label,\n",
    "            \"treated\": treated_flag,\n",
    "            \"treat_time\": tt,\n",
    "            \"cost\": cost,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp\n",
    "        })\n",
    "    df_res = pd.DataFrame(results)\n",
    "    total_cost = df_res[\"cost\"].sum()\n",
    "    treated_df = df_res[df_res[\"treated\"] == 1]\n",
    "    tp_sum = treated_df[\"tp\"].sum()\n",
    "    fp_sum = treated_df[\"fp\"].sum()\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df = df_res[df_res[\"label\"] == 1]\n",
    "    total_sick = len(sick_df)\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df[\"treat_time\"].dropna()\n",
    "        avg_tt = valid_tt.mean() if len(valid_tt)>0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        \"cost\": total_cost,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"avg_treatment_time\": avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 5. BENCHMARK POLICY FUNCTIONS (PARAMETRIC) + COST EVAL WRAPPERS\n",
    "###############################################################################\n",
    "## A) CONSTANT THRESHOLD\n",
    "def policy_func_constant_threshold(df_patient, thr):\n",
    "    \"\"\"Return first time t where predicted_risk >= thr, else None.\"\"\"\n",
    "    for _, row in df_patient.iterrows():\n",
    "        if row[\"predicted_risk\"] >= thr:\n",
    "            return int(row[\"time\"])\n",
    "    return None\n",
    "\n",
    "def evaluate_cost_constant_threshold(params, df_train, df_val):\n",
    "    \"\"\"\n",
    "    params = (depth, lr, thr)\n",
    "    1) Train CatBoost on df_train\n",
    "    2) Predict risk on df_val\n",
    "    3) Evaluate constant-thr policy\n",
    "    \"\"\"\n",
    "    depth, lr, thr = params\n",
    "    risk_val = train_and_predict_model(depth, lr, df_train, df_val)\n",
    "    df_val_eval = df_val.copy()\n",
    "    df_val_eval[\"predicted_risk\"] = risk_val\n",
    "    \n",
    "    def policy_func(grp):\n",
    "        return policy_func_constant_threshold(grp, thr)\n",
    "    \n",
    "    stats = simulate_policy(df_val_eval, policy_func)\n",
    "    return stats[\"cost\"]\n",
    "\n",
    "## B) DYNAMIC THRESHOLD\n",
    "def policy_func_dynamic_threshold(df_patient, thr_vec):\n",
    "    \"\"\"\n",
    "    thr_vec is length T_MAX. For each row, if risk >= thr_vec[t], treat at time t.\n",
    "    \"\"\"\n",
    "    for _, row in df_patient.iterrows():\n",
    "        t = int(row[\"time\"])\n",
    "        if t < len(thr_vec):\n",
    "            if row[\"predicted_risk\"] >= thr_vec[t]:\n",
    "                return t\n",
    "    return None\n",
    "\n",
    "def evaluate_cost_dynamic_threshold(params, df_train, df_val):\n",
    "    \"\"\"\n",
    "    params = (depth, lr, thr_0, ..., thr_{T-1}) => total length T_MAX+2\n",
    "    \"\"\"\n",
    "    depth = params[0]\n",
    "    lr    = params[1]\n",
    "    thr_vec = params[2:]  # T_MAX thresholds\n",
    "    \n",
    "    risk_val = train_and_predict_model(depth, lr, df_train, df_val)\n",
    "    df_val_eval = df_val.copy()\n",
    "    df_val_eval[\"predicted_risk\"] = risk_val\n",
    "    \n",
    "    def policy_func(grp):\n",
    "        return policy_func_dynamic_threshold(grp, thr_vec)\n",
    "    \n",
    "    stats = simulate_policy(df_val_eval, policy_func)\n",
    "    return stats[\"cost\"]\n",
    "\n",
    "## C) LINEAR THRESHOLD\n",
    "def policy_func_linear_threshold(df_patient, A, B):\n",
    "    \"\"\"\n",
    "    threshold = clip(A*t + B, 0, 1)\n",
    "    Return first time with predicted_risk >= threshold.\n",
    "    \"\"\"\n",
    "    for _, row in df_patient.iterrows():\n",
    "        t = row[\"time\"]\n",
    "        thr = A * t + B\n",
    "        thr = np.clip(thr, 0, 1)\n",
    "        if row[\"predicted_risk\"] >= thr:\n",
    "            return int(t)\n",
    "    return None\n",
    "\n",
    "def evaluate_cost_linear_threshold(params, df_train, df_val):\n",
    "    \"\"\"\n",
    "    params = (depth, lr, A, B)\n",
    "    \"\"\"\n",
    "    depth, lr, A, B = params\n",
    "    risk_val = train_and_predict_model(depth, lr, df_train, df_val)\n",
    "    df_val_eval = df_val.copy()\n",
    "    df_val_eval[\"predicted_risk\"] = risk_val\n",
    "    \n",
    "    def policy_func(grp):\n",
    "        return policy_func_linear_threshold(grp, A, B)\n",
    "    \n",
    "    stats = simulate_policy(df_val_eval, policy_func)\n",
    "    return stats[\"cost\"]\n",
    "\n",
    "## D) WAIT-TILL-END\n",
    "def policy_func_wait_till_end(df_patient, thr):\n",
    "    \"\"\"\n",
    "    Check only the final time step's predicted_risk; if >= thr => treat at final time.\n",
    "    \"\"\"\n",
    "    final_row = df_patient.iloc[-1]  # after sorting by time\n",
    "    if final_row[\"predicted_risk\"] >= thr:\n",
    "        return int(final_row[\"time\"])\n",
    "    return None\n",
    "\n",
    "def evaluate_cost_wait_till_end(params, df_train, df_val):\n",
    "    \"\"\"\n",
    "    params = (depth, lr, thr)\n",
    "    \"\"\"\n",
    "    depth, lr, thr = params\n",
    "    risk_val = train_and_predict_model(depth, lr, df_train, df_val)\n",
    "    df_val_eval = df_val.copy()\n",
    "    df_val_eval[\"predicted_risk\"] = risk_val\n",
    "    \n",
    "    def policy_func(grp):\n",
    "        return policy_func_wait_till_end(grp, thr)\n",
    "    \n",
    "    stats = simulate_policy(df_val_eval, policy_func)\n",
    "    return stats[\"cost\"]\n",
    "\n",
    "###############################################################################\n",
    "# 6. DATA-DRIVEN DP (SPSA-catboost)\n",
    "###############################################################################\n",
    "def assign_buckets(prob, n_buckets=5):\n",
    "    edges = np.linspace(0, 1, n_buckets+1)\n",
    "    for b in range(n_buckets):\n",
    "        if prob >= edges[b] and prob < edges[b+1]:\n",
    "            return b\n",
    "    return n_buckets-1\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    \"\"\"\n",
    "    df_train: must have columns [patient_id, time, predicted_risk, label].\n",
    "    Returns p_trans[t,b,b'] and p_sick[t,b].\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    bucket_counts = np.zeros((T, n_buckets))\n",
    "    sick_counts   = np.zeros((T, n_buckets))\n",
    "    \n",
    "    df_sorted = df_train.sort_values([\"patient_id\",\"time\"])\n",
    "    for pid, grp in df_sorted.groupby(\"patient_id\"):\n",
    "        grp = grp.sort_values(\"time\")\n",
    "        rows = grp.to_dict(\"records\")\n",
    "        for i, row in enumerate(rows):\n",
    "            t = int(row[\"time\"])\n",
    "            b = int(row[\"risk_bucket\"])\n",
    "            lbl= int(row[\"label\"])\n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            if i < len(rows)-1:\n",
    "                row_next = rows[i+1]\n",
    "                t_next   = int(row_next[\"time\"])\n",
    "                b_next   = int(row_next[\"risk_bucket\"])\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t, b, b_next] += 1.0\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_, b_, :].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_, b_, :] = transition_counts[t_, b_, :] / denom\n",
    "            else:\n",
    "                # if no data, assume self-loop\n",
    "                p_trans[t_, b_, b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets))\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_, b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_, b_] = sick_counts[t_, b_] / denom\n",
    "            else:\n",
    "                p_sick[t_, b_] = 0.0\n",
    "    \n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp(p_trans, p_sick, FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX):\n",
    "    \"\"\"\n",
    "    DP to handle cost = false positives, false negatives, and per-step delay D.\n",
    "    V[t,b] = minimal future cost if not yet treated at time t, bucket b.\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # Terminal boundary\n",
    "    for b in range(n_buckets):\n",
    "        cost_treat_T   = (1.0 - p_sick[T-1, b])*FP\n",
    "        cost_notreat_T = p_sick[T-1, b]*FN\n",
    "        V[T,b] = min(cost_treat_T, cost_notreat_T)\n",
    "    \n",
    "    # backward recursion\n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            cost_treat = (1 - p_sick[t,b])*FP\n",
    "            cost_wait_immediate = p_sick[t,b]*D\n",
    "            if t == T-1:\n",
    "                future_wait = V[T,b]\n",
    "            else:\n",
    "                future_wait = 0\n",
    "                for b_next in range(n_buckets):\n",
    "                    future_wait += p_trans[t,b,b_next]*V[t+1,b_next]\n",
    "            cost_wait = cost_wait_immediate + gamma * future_wait\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b] = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b] = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    return V, pi_\n",
    "\n",
    "def make_data_driven_dp_policy(V, pi_, p_sick, T=T_MAX):\n",
    "    \"\"\"\n",
    "    If pi_[t,b] = 1 => treat immediately at time t.\n",
    "    If we reach t=T with no action, do final treat-or-not by comparing \n",
    "    cost_treat_T vs. cost_notreat_T at the final bucket.\n",
    "    \"\"\"\n",
    "    def policy_func(df_patient):\n",
    "        df_patient = df_patient.sort_values(\"time\")\n",
    "        last_row = None\n",
    "        for _, row in df_patient.iterrows():\n",
    "            t = int(row[\"time\"])\n",
    "            if t < T:\n",
    "                b = int(row[\"risk_bucket\"])\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "            last_row = row\n",
    "        \n",
    "        # If no treat action up to T-1, handle final step:\n",
    "        if last_row is not None:\n",
    "            b_final = int(last_row[\"risk_bucket\"])\n",
    "            cost_treat_T   = (1.0 - p_sick[T-1, b_final])*FP_COST\n",
    "            cost_notreat_T = p_sick[T-1, b_final]*FN_COST\n",
    "            if cost_treat_T <= cost_notreat_T:\n",
    "                return int(last_row[\"time\"])\n",
    "            else:\n",
    "                return None\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def evaluate_cost_data_driven_dp(params, df_train, df_val):\n",
    "    \"\"\"\n",
    "    params = (depth, lr)\n",
    "    1) Train catboost on df_train\n",
    "    2) Predict on df_train => risk buckets => estimate DP transitions\n",
    "    3) Build DP, then apply policy to df_val\n",
    "    \"\"\"\n",
    "    depth, lr = params\n",
    "    \n",
    "    # 1) train on df_train\n",
    "    risk_train = train_and_predict_model(depth, lr, df_train, df_train)\n",
    "    df_train_dp = df_train.copy()\n",
    "    df_train_dp[\"predicted_risk\"] = risk_train\n",
    "    df_train_dp[\"risk_bucket\"]    = df_train_dp[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    # get transitions, p_sick\n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_train_dp, T=T_MAX, n_buckets=5)\n",
    "    V, pi_ = train_data_driven_dp(p_trans, p_sick, FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX)\n",
    "    \n",
    "    # 2) predict on df_val\n",
    "    risk_val = train_and_predict_model(depth, lr, df_train, df_val)\n",
    "    df_val_eval = df_val.copy()\n",
    "    df_val_eval[\"predicted_risk\"] = risk_val\n",
    "    df_val_eval[\"risk_bucket\"]    = df_val_eval[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    # 3) apply DP policy\n",
    "    dp_policy = make_data_driven_dp_policy(V, pi_, p_sick, T=T_MAX)\n",
    "    stats = simulate_policy(df_val_eval, dp_policy)\n",
    "    return stats[\"cost\"]\n",
    "\n",
    "###############################################################################\n",
    "# 7. GENERIC SPSA OPTIMIZATION\n",
    "###############################################################################\n",
    "def spsa_optimization(\n",
    "    cost_func,         # cost_func(params, df_train, df_val) -> scalar cost\n",
    "    df_train, df_val,\n",
    "    param_dim,         # dimension of param vector\n",
    "    param_bounds,      # list of (lower, upper) for each dimension\n",
    "    n_iterations=20,\n",
    "    alpha=0.602,\n",
    "    gamma=0.101,\n",
    "    a0=0.2,\n",
    "    c0=0.1,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    A general SPSA routine that operates on param_dim-dimensional parameter vectors.\n",
    "    param_bounds[i] = (lb_i, ub_i).\n",
    "    Returns best_params, best_cost.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Initialize param in the middle of each bound\n",
    "    init_params = []\n",
    "    for i in range(param_dim):\n",
    "        lb, ub = param_bounds[i]\n",
    "        init_params.append(0.5*(lb+ub))\n",
    "    params = np.array(init_params)\n",
    "    \n",
    "    best_params = params.copy()\n",
    "    best_cost   = float('inf')\n",
    "    \n",
    "    for k in range(1, n_iterations+1):\n",
    "        a_k = a0 / (k**alpha)\n",
    "        c_k = c0 / (k**gamma)\n",
    "        \n",
    "        delta = rng.choice([-1,1], size=param_dim)\n",
    "        params_plus  = params + c_k * delta\n",
    "        params_minus = params - c_k * delta\n",
    "        \n",
    "        # clip\n",
    "        for i in range(param_dim):\n",
    "            lb, ub = param_bounds[i]\n",
    "            params_plus[i]  = np.clip(params_plus[i], lb, ub)\n",
    "            params_minus[i] = np.clip(params_minus[i], lb, ub)\n",
    "        \n",
    "        # Evaluate cost\n",
    "        cost_plus  = cost_func(params_plus,  df_train, df_val)\n",
    "        cost_minus = cost_func(params_minus, df_train, df_val)\n",
    "        \n",
    "        # Approx gradient\n",
    "        g_k = (cost_plus - cost_minus)/(2.0*c_k) * delta\n",
    "        \n",
    "        # Update\n",
    "        params_new = params - a_k*g_k\n",
    "        # clip\n",
    "        for i in range(param_dim):\n",
    "            lb, ub = param_bounds[i]\n",
    "            params_new[i] = np.clip(params_new[i], lb, ub)\n",
    "        \n",
    "        cost_new = cost_func(params_new, df_train, df_val)\n",
    "        if cost_new < best_cost:\n",
    "            best_cost   = cost_new\n",
    "            best_params = params_new.copy()\n",
    "        params = params_new\n",
    "    \n",
    "    return best_params, best_cost\n",
    "\n",
    "###############################################################################\n",
    "# 8. CROSS-VALIDATION FOR EACH APPROACH, THEN TEST ON HOLDOUT (ALGORITHM 5)\n",
    "###############################################################################\n",
    "def cross_val_sum_of_cost(cost_func, candidate_params, group_dfs, n_splits=4):\n",
    "    \"\"\"\n",
    "    Re-evaluates a given set of hyperparameters across *all* folds G1..Gn, \n",
    "    summing up the cost on each fold's validation set.\n",
    "    \"\"\"\n",
    "    total_cost = 0.0\n",
    "    for val_i in range(1, n_splits+1):\n",
    "        df_val_i = group_dfs[f\"G{val_i}\"]\n",
    "        # Training is all groups except G{val_i}\n",
    "        train_sets_i = []\n",
    "        for j in range(1, n_splits+1):\n",
    "            if j != val_i:\n",
    "                train_sets_i.append(group_dfs[f\"G{j}\"])\n",
    "        df_train_i = pd.concat(train_sets_i, ignore_index=True)\n",
    "        \n",
    "        fold_cost = cost_func(candidate_params, df_train_i, df_val_i)\n",
    "        total_cost += fold_cost\n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "def run_experiment_algorithm5_fair(df_all, n_splits=4, seed=42, n_spsa_iters=20):\n",
    "    \"\"\"\n",
    "    We'll do the following for each of the 5 methods:\n",
    "      1) Split into n folds + 1 holdout\n",
    "      2) For each fold i=1..n, run SPSA to find best_params_i\n",
    "      3) Then pick (lambda*, mu*) that yields the *lowest sum of cost* over all folds\n",
    "      4) Retrain on union of folds (all training) and evaluate on holdout\n",
    "    \"\"\"\n",
    "    # 1) Split data\n",
    "    splits = split_patients_kfold(df_all, n_splits=n_splits, seed=seed)\n",
    "    group_dfs = {}\n",
    "    for g, pidset in splits.items():\n",
    "        group_dfs[g] = filter_by_group(df_all, pidset)\n",
    "    test_name = f\"G{n_splits+1}\"\n",
    "    df_test = group_dfs[test_name]\n",
    "    \n",
    "    # We'll define the 5 approaches & their param space\n",
    "    # 1) constant threshold -> param = (depth, lr, thr) => dim=3\n",
    "    # 2) dynamic threshold -> param = (depth, lr, thr_0..thr_{T-1}) => dim=2+T_MAX\n",
    "    # 3) linear threshold -> param = (depth, lr, A, B) => dim=4\n",
    "    # 4) wait till end -> param = (depth, lr, thr) => dim=3\n",
    "    # 5) data-driven dp -> param = (depth, lr) => dim=2\n",
    "    \n",
    "    method_defs = [\n",
    "      {\n",
    "        \"name\": \"Constant Threshold\",\n",
    "        \"cost_func\": evaluate_cost_constant_threshold,\n",
    "        \"dim\": 3,\n",
    "        \"bounds\": [(2.0, 8.0), (0.01, 0.3), (0.0, 1.0)],\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Dynamic Threshold\",\n",
    "        \"cost_func\": evaluate_cost_dynamic_threshold,\n",
    "        \"dim\": 2 + T_MAX,\n",
    "        \"bounds\": ([(2.0, 8.0), (0.01, 0.3)] + [(0.0,1.0)]*T_MAX),\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Linear Threshold\",\n",
    "        \"cost_func\": evaluate_cost_linear_threshold,\n",
    "        \"dim\": 4,\n",
    "        \"bounds\": [(2.0,8.0), (0.01,0.3), (-0.1,0.1), (0.0,1.0)],\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Wait Till End\",\n",
    "        \"cost_func\": evaluate_cost_wait_till_end,\n",
    "        \"dim\": 3,\n",
    "        \"bounds\": [(2.0, 8.0), (0.01, 0.3), (0.0, 1.0)],\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Data-Driven DP (SPSA-catboost)\",\n",
    "        \"cost_func\": evaluate_cost_data_driven_dp,\n",
    "        \"dim\": 2,\n",
    "        \"bounds\": [(2.0,8.0), (0.01,0.3)],\n",
    "      },\n",
    "    ]\n",
    "    \n",
    "    final_records = []\n",
    "    \n",
    "    # For each method, do cross-validation with SPSA\n",
    "    for mdef in method_defs:\n",
    "        method_name = mdef[\"name\"]\n",
    "        cost_func   = mdef[\"cost_func\"]\n",
    "        param_dim   = mdef[\"dim\"]\n",
    "        param_bounds= mdef[\"bounds\"]\n",
    "        \n",
    "        # Step 2: For each fold i in 1..n, run SPSA\n",
    "        fold_records = []\n",
    "        for i_val in range(1, n_splits+1):\n",
    "            df_val = group_dfs[f\"G{i_val}\"]\n",
    "            # train folds = all except i_val\n",
    "            train_sets = []\n",
    "            for j in range(1, n_splits+1):\n",
    "                if j != i_val:\n",
    "                    train_sets.append(group_dfs[f\"G{j}\"])\n",
    "            df_train_fold = pd.concat(train_sets, ignore_index=True)\n",
    "            \n",
    "            # run SPSA on this fold\n",
    "            best_params_fold, best_cost_fold = spsa_optimization(\n",
    "                cost_func,\n",
    "                df_train_fold, df_val,\n",
    "                param_dim, param_bounds,\n",
    "                n_iterations=n_spsa_iters,\n",
    "                seed=42\n",
    "            )\n",
    "            fold_records.append({\n",
    "                \"fold\": i_val,\n",
    "                \"best_params\": best_params_fold,\n",
    "                \"val_cost\": best_cost_fold\n",
    "            })\n",
    "        \n",
    "        # We now have n sets of best_params (one from each fold).\n",
    "        # Step 3: Re-evaluate each candidate on *all* folds, sum the cost, pick best.\n",
    "        sum_costs = []\n",
    "        for rec in fold_records:\n",
    "            candidate_params = rec[\"best_params\"]\n",
    "            total_cv_cost = cross_val_sum_of_cost(cost_func, candidate_params, group_dfs, n_splits)\n",
    "            sum_costs.append(total_cv_cost)\n",
    "        \n",
    "        # Find the param with minimal sum-of-CV-cost\n",
    "        best_idx = np.argmin(sum_costs)\n",
    "        chosen_params = fold_records[best_idx][\"best_params\"]\n",
    "        chosen_sum_cost = sum_costs[best_idx]\n",
    "        \n",
    "        # Step 4: Retrain on union of G1..G_n, evaluate on G_{n+1}\n",
    "        train_all = []\n",
    "        for i in range(1, n_splits+1):\n",
    "            train_all.append(group_dfs[f\"G{i}\"])\n",
    "        df_train_all = pd.concat(train_all, ignore_index=True)\n",
    "        \n",
    "        # Evaluate final holdout cost\n",
    "        final_test_cost = cost_func(chosen_params, df_train_all, df_test)\n",
    "        \n",
    "        # If you want precision/recall, let's do the actual simulation:\n",
    "        stats = simulate_method_on_test(\n",
    "            method_name, chosen_params,\n",
    "            df_train_all, df_test\n",
    "        )\n",
    "        \n",
    "        final_records.append({\n",
    "            \"Method\": method_name,\n",
    "            \"Cost\": stats[\"cost\"],\n",
    "            \"Precision (%)\": 100*stats[\"precision\"],\n",
    "            \"Recall (%)\": 100*stats[\"recall\"],\n",
    "            \"Avg. Treat Time\": stats[\"avg_treatment_time\"],\n",
    "            \"Sum-of-CV-Cost\": chosen_sum_cost  # just to see how good it was on CV\n",
    "        })\n",
    "    \n",
    "    df_final = pd.DataFrame(final_records)\n",
    "    return df_final\n",
    "\n",
    "def simulate_method_on_test(method_name, params, df_train_all, df_test):\n",
    "    \"\"\"\n",
    "    For each method, we do the same steps as in 'evaluate_cost_...'\n",
    "    but then actually run 'simulate_policy' to get precision, recall, etc.\n",
    "    \"\"\"\n",
    "    if method_name == \"Constant Threshold\":\n",
    "        depth, lr, thr = params\n",
    "        risk_test = train_and_predict_model(depth, lr, df_train_all, df_test)\n",
    "        df_test_eval = df_test.copy()\n",
    "        df_test_eval[\"predicted_risk\"] = risk_test\n",
    "        \n",
    "        def policy_func(grp):\n",
    "            return policy_func_constant_threshold(grp, thr)\n",
    "        stats = simulate_policy(df_test_eval, policy_func)\n",
    "        return stats\n",
    "    \n",
    "    elif method_name == \"Dynamic Threshold\":\n",
    "        depth = params[0]\n",
    "        lr    = params[1]\n",
    "        thr_vec = params[2:]\n",
    "        risk_test = train_and_predict_model(depth, lr, df_train_all, df_test)\n",
    "        df_test_eval = df_test.copy()\n",
    "        df_test_eval[\"predicted_risk\"] = risk_test\n",
    "        \n",
    "        def policy_func(grp):\n",
    "            return policy_func_dynamic_threshold(grp, thr_vec)\n",
    "        stats = simulate_policy(df_test_eval, policy_func)\n",
    "        return stats\n",
    "    \n",
    "    elif method_name == \"Linear Threshold\":\n",
    "        depth, lr, A, B = params\n",
    "        risk_test = train_and_predict_model(depth, lr, df_train_all, df_test)\n",
    "        df_test_eval = df_test.copy()\n",
    "        df_test_eval[\"predicted_risk\"] = risk_test\n",
    "        \n",
    "        def policy_func(grp):\n",
    "            return policy_func_linear_threshold(grp, A, B)\n",
    "        stats = simulate_policy(df_test_eval, policy_func)\n",
    "        return stats\n",
    "    \n",
    "    elif method_name == \"Wait Till End\":\n",
    "        depth, lr, thr = params\n",
    "        risk_test = train_and_predict_model(depth, lr, df_train_all, df_test)\n",
    "        df_test_eval = df_test.copy()\n",
    "        df_test_eval[\"predicted_risk\"] = risk_test\n",
    "        \n",
    "        def policy_func(grp):\n",
    "            return policy_func_wait_till_end(grp, thr)\n",
    "        stats = simulate_policy(df_test_eval, policy_func)\n",
    "        return stats\n",
    "    \n",
    "    elif method_name == \"Data-Driven DP (SPSA-catboost)\":\n",
    "        depth, lr = params\n",
    "        # train on df_train_all\n",
    "        risk_train = train_and_predict_model(depth, lr, df_train_all, df_train_all)\n",
    "        df_train_dp = df_train_all.copy()\n",
    "        df_train_dp[\"predicted_risk\"] = risk_train\n",
    "        df_train_dp[\"risk_bucket\"]    = df_train_dp[\"predicted_risk\"].apply(assign_buckets)\n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(df_train_dp, T=T_MAX, n_buckets=5)\n",
    "        V, pi_ = train_data_driven_dp(p_trans, p_sick, FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX)\n",
    "        \n",
    "        # predict on df_test\n",
    "        risk_test = train_and_predict_model(depth, lr, df_train_all, df_test)\n",
    "        df_test_eval = df_test.copy()\n",
    "        df_test_eval[\"predicted_risk\"] = risk_test\n",
    "        df_test_eval[\"risk_bucket\"]    = df_test_eval[\"predicted_risk\"].apply(assign_buckets)\n",
    "        \n",
    "        dp_policy = make_data_driven_dp_policy(V, pi_, p_sick, T=T_MAX)\n",
    "        stats = simulate_policy(df_test_eval, dp_policy)\n",
    "        return stats\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method_name}\")\n",
    "\n",
    "###############################################################################\n",
    "# 9. MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    df_all = pd.read_csv(\"dp_favoring_synthetic_patients.csv\")  # your dataset\n",
    "    \n",
    "    # run 1 replication with a given random seed\n",
    "    n_replications = 1\n",
    "    all_tables = []\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        seed_val = rep\n",
    "        print(f\"\\n=== RUNNING REPLICATION {rep+1}/{n_replications}, seed={seed_val} ===\")\n",
    "        \n",
    "        df_final = run_experiment_algorithm5_fair(\n",
    "            df_all, \n",
    "            n_splits=4,\n",
    "            seed=seed_val,\n",
    "            n_spsa_iters=20  # can be increased\n",
    "        )\n",
    "        print(\"\\nResults on final holdout:\")\n",
    "        print(df_final)\n",
    "        all_tables.append(df_final)\n",
    "    \n",
    "    # If multiple replications, aggregate them\n",
    "    if n_replications > 1:\n",
    "        df_agg = aggregate_results(all_tables)\n",
    "        print(\"\\n=== AGGREGATED RESULTS (Mean ± Std) ===\")\n",
    "        print(df_agg.to_string(index=False))\n",
    "\n",
    "def aggregate_results(list_of_tables):\n",
    "    \"\"\"\n",
    "    Aggregates multiple final_table DataFrames by computing mean ± std.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    data_accum = defaultdict(lambda: {\n",
    "        \"Cost\": [],\n",
    "        \"Precision\": [],\n",
    "        \"Recall\": [],\n",
    "        \"Time\": []\n",
    "    })\n",
    "    \n",
    "    for df_table in list_of_tables:\n",
    "        for idx in range(len(df_table)):\n",
    "            row = df_table.iloc[idx]\n",
    "            method = row[\"Method\"]\n",
    "            data_accum[method][\"Cost\"].append(row[\"Cost\"])\n",
    "            data_accum[method][\"Precision\"].append(row[\"Precision (%)\"])\n",
    "            data_accum[method][\"Recall\"].append(row[\"Recall (%)\"])\n",
    "            data_accum[method][\"Time\"].append(row[\"Avg. Treat Time\"])\n",
    "    \n",
    "    results = []\n",
    "    method_order = [\n",
    "        \"Constant Threshold\",\n",
    "        \"Dynamic Threshold\",\n",
    "        \"Linear Threshold\",\n",
    "        \"Wait Till End\",\n",
    "        \"Data-Driven DP (SPSA-catboost)\"\n",
    "    ]\n",
    "    for m in method_order:\n",
    "        arr_cost = np.array(data_accum[m][\"Cost\"])\n",
    "        arr_prec = np.array(data_accum[m][\"Precision\"])\n",
    "        arr_rec  = np.array(data_accum[m][\"Recall\"])\n",
    "        arr_time = np.array(data_accum[m][\"Time\"])\n",
    "        \n",
    "        cost_mean, cost_std   = arr_cost.mean(), arr_cost.std()\n",
    "        prec_mean, prec_std   = arr_prec.mean(), arr_prec.std()\n",
    "        rec_mean,  rec_std    = arr_rec.mean(),  arr_rec.std()\n",
    "        time_mean, time_std   = arr_time.mean(), arr_time.std()\n",
    "        \n",
    "        results.append({\n",
    "            \"Method\": m,\n",
    "            \"Cost\": f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "            \"Precision (%)\": f\"{prec_mean:.2f} ± {prec_std:.2f}\",\n",
    "            \"Recall (%)\": f\"{rec_mean:.2f} ± {rec_std:.2f}\",\n",
    "            \"Avg. Treat Time\": f\"{time_mean:.2f} ± {time_std:.2f}\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f30f6620-41ab-4e53-b900-a4261faab4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ALGORITHM 5 (SPSA) RESULTS (Unconstrained) ===\n",
      "                     Method  Cost  Precision (%)  Recall (%)  Avg Treat Time\n",
      "         Constant Threshold   399      51.785714  100.000000        2.303571\n",
      "        Dynamic Threshold-R   448      47.540984  100.000000        8.065574\n",
      "           Linear Threshold   448      46.774194  100.000000        2.064516\n",
      "              Wait Till End   610     100.000000   96.551724       20.000000\n",
      "Dynamic Threshold-DP (SPSA)   192      87.878788  100.000000        5.666667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALGORITHM 5 (SPSA) + BENCHMARK TABLE (Unconstrained Hemorrhage Diagnosis & Treatment)\n",
    "\n",
    "Requirements:\n",
    "  pip install numpy pandas scikit-learn catboost\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Sklearn models, metrics, etc.\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS\n",
    "###############################################################################\n",
    "FP_COST = 10\n",
    "FN_COST = 50\n",
    "D_COST  = 1\n",
    "T_MAX   = 21   # maximum discrete time steps (0..T_MAX-1)\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS (DATA SPLIT, METRICS, ETC.)\n",
    "###############################################################################\n",
    "def split_into_four_groups(df, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    \n",
    "    n = len(unique_pids)\n",
    "    i1 = int(0.25 * n)\n",
    "    i2 = int(0.50 * n)\n",
    "    i3 = int(0.75 * n)\n",
    "    \n",
    "    G1_pids = unique_pids[: i1]\n",
    "    G2_pids = unique_pids[i1 : i2]\n",
    "    G3_pids = unique_pids[i2 : i3]\n",
    "    G4_pids = unique_pids[i3 : ]\n",
    "    \n",
    "    G1 = df[df['patient_id'].isin(G1_pids)].copy()\n",
    "    G2 = df[df['patient_id'].isin(G2_pids)].copy()\n",
    "    G3 = df[df['patient_id'].isin(G3_pids)].copy()\n",
    "    G4 = df[df['patient_id'].isin(G4_pids)].copy()\n",
    "    return G1, G2, G3, G4\n",
    "\n",
    "\n",
    "def k_plus_1_splits(df, k=3, seed=0):\n",
    "    \"\"\"\n",
    "    For Algorithm 5 (SPSA), we often split the data into k+1 groups:\n",
    "      G1, G2, ..., Gk, G_{k+1}.\n",
    "    This is a k-fold style partition plus one final holdout.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    \n",
    "    n = len(unique_pids)\n",
    "    fold_size = int(n/(k+1))\n",
    "    \n",
    "    groups = []\n",
    "    start = 0\n",
    "    for i in range(k):\n",
    "        pids_fold = unique_pids[start : start+fold_size]\n",
    "        start += fold_size\n",
    "        g = df[df['patient_id'].isin(pids_fold)].copy()\n",
    "        groups.append(g)\n",
    "    \n",
    "    # final group is everything leftover\n",
    "    pids_fold = unique_pids[start:]\n",
    "    g = df[df['patient_id'].isin(pids_fold)].copy()\n",
    "    groups.append(g)\n",
    "    return groups\n",
    "\n",
    "\n",
    "def compute_auc_score(y_true, y_prob):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. POLICY SIMULATION & BENCHMARK POLICY METHODS\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df must contain columns:\n",
    "      - patient_id\n",
    "      - time\n",
    "      - risk_score\n",
    "      - label (0 or 1)\n",
    "\n",
    "    policy_func(patient_rows) -> treat_time (int) or None\n",
    "    Returns dict of cost, precision, recall, avg_treatment_time\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pid, grp in df.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        label = grp['label'].iloc[0]\n",
    "        \n",
    "        treat_time = policy_func(grp)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treated\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp   = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp   = 0\n",
    "            fp = 0\n",
    "            treat_flag = 0\n",
    "            ttime = None\n",
    "        else:\n",
    "            treat_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            ttime = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treat_flag,\n",
    "            'treat_time': ttime,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res     = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated']==1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df = df_res[df_res['label']==1]\n",
    "    total_sick = len(sick_df)\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt   = valid_tt.mean() if len(valid_tt) > 0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "# Benchmark threshold-based searches\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['risk_score'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_constant_threshold_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=20,\n",
    "                                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec):\n",
    "                if row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def linear_threshold_search(df,\n",
    "                            A_candidates=np.linspace(-0.05, 0.01, 7),\n",
    "                            B_candidates=np.linspace(0,0.8,7)):\n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A*t + B\n",
    "                    thr = np.clip(thr,0,1)\n",
    "                    if row['risk_score'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    return (best_A,best_B), best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A,B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A*t + B\n",
    "            thr = np.clip(thr,0,1)\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            final_t = patient_rows['time'].max()\n",
    "            final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "            if final_row['risk_score'] >= thr:\n",
    "                return int(final_t)\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_t = patient_rows['time'].max()\n",
    "        final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "        if final_row['risk_score'] >= thr:\n",
    "            return int(final_t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. DATA-DRIVEN DP (UNCONSTRAINED)\n",
    "###############################################################################\n",
    "def to_bucket(prob):\n",
    "    \"\"\"Map a probability into a bucket 0..4.\"\"\"\n",
    "    b = int(prob * 5)\n",
    "    return min(b, 4)\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows= grp.to_dict('records')\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            lbl = row['label']\n",
    "            \n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            \n",
    "            if i < len(rows)-1:\n",
    "                nxt = rows[i+1]\n",
    "                t_next = nxt['time']\n",
    "                b_next = nxt['risk_bucket']\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t,b,b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_,b_,:].sum()\n",
    "            if denom>0:\n",
    "                p_trans[t_,b_,:] = transition_counts[t_,b_,:] / denom\n",
    "            else:\n",
    "                p_trans[t_,b_,b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_,b_]\n",
    "            if denom>0:\n",
    "                p_sick[t_,b_] = sick_counts[t_,b_] / denom\n",
    "            else:\n",
    "                p_sick[t_,b_] = 0.0\n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                       FP=10, FN=50, D=1, gamma=0.99, T=20):\n",
    "    \"\"\"\n",
    "    Standard DP for unconstrained scenario:\n",
    "      V[t,b] = min( cost_treat_now, cost_wait )\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # boundary at t=T\n",
    "    for b in range(n_buckets):\n",
    "        cost_treat   = p_sick[T-1,b]*(D*(T-1)) + (1-p_sick[T-1,b])*FP\n",
    "        cost_notreat = p_sick[T-1,b]*FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    # fill from T-1 down to 0\n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1-p_sick[t,b])*FP\n",
    "            \n",
    "            # wait\n",
    "            if t == T-1:\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next]*V[t+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    return V, pi_\n",
    "\n",
    "def make_dp_policy(V, pi_, T=20):\n",
    "    \"\"\"Return a policy function that treats if pi[t,b]==1 at time t.\"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. (REFERENCE) ALGORITHM 0 FOR COMPARISON\n",
    "###############################################################################\n",
    "def train_and_select_best_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Trains multiple models (RF, GB, CatBoost) over small hyperparam grids,\n",
    "    picks best by AUC. (Used by Algorithm 0 only.)\n",
    "    \"\"\"\n",
    "    best_auc = -1.0\n",
    "    best_model = None\n",
    "    best_name  = None\n",
    "    \n",
    "    # Quick small grids:\n",
    "    RF_PARAM_GRID = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5]\n",
    "    }\n",
    "    GB_PARAM_GRID = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5]\n",
    "    }\n",
    "    CATBOOST_PARAM_GRID = {\n",
    "        'iterations': [50, 100],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'depth': [3, 5]\n",
    "    }\n",
    "    \n",
    "    for params in ParameterGrid(RF_PARAM_GRID):\n",
    "        rf = RandomForestClassifier(random_state=0, **params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        val_prob = rf.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = rf\n",
    "            best_name  = f\"RandomForest_{params}\"\n",
    "    \n",
    "    for params in ParameterGrid(GB_PARAM_GRID):\n",
    "        gb = GradientBoostingClassifier(random_state=0, **params)\n",
    "        gb.fit(X_train, y_train)\n",
    "        val_prob = gb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = gb\n",
    "            best_name  = f\"GradientBoosting_{params}\"\n",
    "    \n",
    "    for params in ParameterGrid(CATBOOST_PARAM_GRID):\n",
    "        cb = CatBoostClassifier(verbose=0, random_state=0, **params)\n",
    "        cb.fit(X_train, y_train)\n",
    "        val_prob = cb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = cb\n",
    "            best_name  = f\"CatBoost_{params}\"\n",
    "    \n",
    "    return best_model, best_auc, best_name\n",
    "\n",
    "def run_algorithm0_unconstrained(df_all, seed=0):\n",
    "    G1, G2, G3, G4 = split_into_four_groups(df_all, seed=seed)\n",
    "\n",
    "    X_train = G1[['EIT','NIRS','EIS']].values\n",
    "    y_train = G1['label'].values\n",
    "    X_val   = G2[['EIT','NIRS','EIS']].values\n",
    "    y_val   = G2['label'].values\n",
    "    \n",
    "    best_model, best_auc, best_name = train_and_select_best_model(\n",
    "        X_train, y_train, X_val, y_val\n",
    "    )\n",
    "    \n",
    "    G12 = pd.concat([G1, G2], ignore_index=True)\n",
    "    X_12 = G12[['EIT','NIRS','EIS']].values\n",
    "    y_12 = G12['label'].values\n",
    "    best_model.fit(X_12, y_12)\n",
    "    \n",
    "    # Prepare G3, G12\n",
    "    G3 = G3.copy()\n",
    "    prob_3 = best_model.predict_proba(G3[['EIT','NIRS','EIS']])[:,1]\n",
    "    G3['risk_score'] = prob_3\n",
    "    \n",
    "    G12 = G12.copy()\n",
    "    prob_12 = best_model.predict_proba(G12[['EIT','NIRS','EIS']])[:,1]\n",
    "    G12['risk_score'] = prob_12\n",
    "    \n",
    "    # TUNE threshold-based on G3\n",
    "    thr_const_g3, _ = constant_threshold_search(G3)\n",
    "    thr_vec_g3, _    = dynamic_threshold_random_search(G3, time_steps=T_MAX)\n",
    "    (A_lin_g3, B_lin_g3), _ = linear_threshold_search(G3)\n",
    "    thr_wte_g3, _    = wait_till_end_search(G3)\n",
    "    \n",
    "    # TUNE DP discount factor on G3\n",
    "    G12['risk_bucket'] = G12['risk_score'].apply(to_bucket)\n",
    "    p_trans, p_sick    = estimate_transition_and_sick_probs(G12, T=T_MAX, n_buckets=5)\n",
    "    \n",
    "    G3_dp = G3.copy()\n",
    "    G3_dp['risk_bucket'] = G3_dp['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    GAMMA_CANDIDATES = [0.95, 0.99]\n",
    "    best_gamma = None\n",
    "    best_cost_dp = float('inf')\n",
    "    best_V = None\n",
    "    best_pi= None\n",
    "    \n",
    "    for gamma_ in GAMMA_CANDIDATES:\n",
    "        V_temp, pi_temp = train_data_driven_dp_unconstrained(\n",
    "            p_trans, p_sick,\n",
    "            FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "            gamma=gamma_, T=T_MAX\n",
    "        )\n",
    "        dp_policy_temp = make_dp_policy(V_temp, pi_temp, T=T_MAX)\n",
    "        stats_temp = simulate_policy(G3_dp, dp_policy_temp)\n",
    "        \n",
    "        if stats_temp['cost'] < best_cost_dp:\n",
    "            best_cost_dp = stats_temp['cost']\n",
    "            best_gamma   = gamma_\n",
    "            best_V       = V_temp\n",
    "            best_pi      = pi_temp\n",
    "    \n",
    "    # Evaluate all on G4\n",
    "    G4 = G4.copy()\n",
    "    prob_4 = best_model.predict_proba(G4[['EIT','NIRS','EIS']])[:,1]\n",
    "    G4['risk_score'] = prob_4\n",
    "    \n",
    "    policy_const = make_constant_threshold_policy(thr_const_g3)\n",
    "    stats_const  = simulate_policy(G4, policy_const)\n",
    "    \n",
    "    policy_dyn   = make_dynamic_threshold_policy(thr_vec_g3)\n",
    "    stats_dyn    = simulate_policy(G4, policy_dyn)\n",
    "    \n",
    "    policy_lin   = make_linear_threshold_policy(A_lin_g3, B_lin_g3)\n",
    "    stats_lin    = simulate_policy(G4, policy_lin)\n",
    "    \n",
    "    policy_wte   = make_wait_till_end_policy(thr_wte_g3)\n",
    "    stats_wte    = simulate_policy(G4, policy_wte)\n",
    "    \n",
    "    dp_policy_final = make_dp_policy(best_V, best_pi, T=T_MAX)\n",
    "    G4_dp = G4.copy()\n",
    "    G4_dp['risk_bucket'] = G4_dp['risk_score'].apply(to_bucket)\n",
    "    stats_dp = simulate_policy(G4_dp, dp_policy_final)\n",
    "    \n",
    "    table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            'Dynamic Threshold-DP'\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Avg Treat Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    return table\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6. ALGORITHM 5 (SPSA) IMPLEMENTATION\n",
    "###############################################################################\n",
    "def spsa_optimize(cost_func, param_init, n_iter=20,\n",
    "                  alpha=0.602, gamma=0.101, a=0.1, c=0.1, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    p = param_init.copy()\n",
    "    best_p = p.copy()\n",
    "    best_cost = cost_func(p)\n",
    "    \n",
    "    for k in range(1, n_iter+1):\n",
    "        ak = a / (k**alpha)\n",
    "        ck = c / (k**gamma)\n",
    "        delta = rng.choice([-1,1], size=len(p))\n",
    "        \n",
    "        p_plus  = p + ck*delta\n",
    "        p_minus = p - ck*delta\n",
    "        \n",
    "        cost_plus  = cost_func(p_plus)\n",
    "        cost_minus = cost_func(p_minus)\n",
    "        \n",
    "        g_approx = (cost_plus - cost_minus)/(2.0*ck) * delta\n",
    "        \n",
    "        p = p - ak*g_approx\n",
    "        \n",
    "        current_cost = cost_func(p)\n",
    "        if current_cost < best_cost:\n",
    "            best_cost = current_cost\n",
    "            best_p = p.copy()\n",
    "    \n",
    "    return best_p, best_cost\n",
    "\n",
    "\n",
    "def parse_spsa_params(params):\n",
    "    \"\"\"\n",
    "    Map a real vector `params` into discrete hyperparams + DP gamma:\n",
    "      params[0]: model_type (0=RF,1=GB,2=CB)\n",
    "      params[1]: n_estimators [10..200]\n",
    "      params[2]: learning_rate [0.01..0.2]\n",
    "      params[3]: max_depth [2..10]\n",
    "      params[4]: gamma [0.90..0.999]\n",
    "    \"\"\"\n",
    "    p0 = int(round(np.clip(params[0], 0, 2)))\n",
    "    p1 = int(round(np.clip(params[1], 10, 200)))\n",
    "    p2 = float(np.clip(params[2], 0.01, 0.2))\n",
    "    p3 = int(round(np.clip(params[3], 2, 10)))\n",
    "    p4 = float(np.clip(params[4], 0.90, 0.999))\n",
    "    return (p0, p1, p2, p3, p4)\n",
    "\n",
    "\n",
    "def spsa_cost_function(param_vector, df_train, df_val):\n",
    "    \"\"\"\n",
    "    Decision-aware cost:\n",
    "      1) Parse param_vector => (model_type, n_estimators, learning_rate, max_depth, gamma)\n",
    "      2) Train model on df_train\n",
    "      3) Predict risk on df_val\n",
    "      4) Build DP with gamma (from param_vector) using transitions from df_train\n",
    "      5) Evaluate cost on df_val\n",
    "    \"\"\"\n",
    "    model_type, n_est, lr, m_depth, gamma_ = parse_spsa_params(param_vector)\n",
    "\n",
    "    X_tr = df_train[['EIT','NIRS','EIS']].values\n",
    "    y_tr = df_train['label'].values\n",
    "    \n",
    "    if model_type == 0:\n",
    "        clf = RandomForestClassifier(n_estimators=n_est, max_depth=m_depth, random_state=0)\n",
    "    elif model_type == 1:\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_est, learning_rate=lr,\n",
    "                                         max_depth=m_depth, random_state=0)\n",
    "    else:\n",
    "        clf = CatBoostClassifier(iterations=n_est, learning_rate=lr, depth=m_depth,\n",
    "                                 verbose=0, random_state=0)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    X_val = df_val[['EIT','NIRS','EIS']].values\n",
    "    prob_val = clf.predict_proba(X_val)[:,1]\n",
    "    df_val_ = df_val.copy()\n",
    "    df_val_['risk_score'] = prob_val\n",
    "    \n",
    "    # DP transitions from df_train\n",
    "    df_tr_ = df_train.copy()\n",
    "    train_probs = clf.predict_proba(df_tr_[['EIT','NIRS','EIS']])[:,1]\n",
    "    df_tr_['risk_score'] = train_probs\n",
    "    df_tr_['risk_bucket'] = df_tr_['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_tr_, T=T_MAX, n_buckets=5)\n",
    "    V, pi_ = train_data_driven_dp_unconstrained(\n",
    "        p_trans, p_sick,\n",
    "        FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "        gamma=gamma_, T=T_MAX\n",
    "    )\n",
    "    \n",
    "    df_val_['risk_bucket'] = df_val_['risk_score'].apply(to_bucket)\n",
    "    dp_policy = make_dp_policy(V, pi_, T=T_MAX)\n",
    "    stats = simulate_policy(df_val_, dp_policy)\n",
    "    return stats['cost']\n",
    "\n",
    "\n",
    "def run_algorithm5_spsa_unconstrained(df_all, k=3, seed=0, n_spsa_iter=20):\n",
    "    \"\"\"\n",
    "    SPSA Hyper-Parameter Tuning (Decision-Aware).\n",
    "    \n",
    "    1) Split data into k+1 groups: G1..Gk, G_{k+1}\n",
    "    2) For each fold i in [1..k]:\n",
    "         - define cost function that trains on G\\\\G_i, evaluates cost on G_i\n",
    "         - run SPSA to find best param_i\n",
    "    3) Among param_1..param_k, pick best overall param* with minimal sum of costs across folds\n",
    "    4) Evaluate param* on G_{k+1}.\n",
    "    5) For final report, we also show the five benchmark policies on the holdout\n",
    "       *using the same final trained model from param* (and its DP discount factor).\n",
    "    \"\"\"\n",
    "    groups = k_plus_1_splits(df_all, k=k, seed=seed)\n",
    "    # groups[0..k-1] => folds for cross-validation\n",
    "    # groups[k]      => final holdout\n",
    "    \n",
    "    # Step 2: For each fold, run SPSA\n",
    "    param_init = np.array([1.0, 50.0, 0.05, 3.0, 0.95], dtype=float)\n",
    "    \n",
    "    spsa_solutions = []\n",
    "    for i in range(1, k+1):\n",
    "        df_val = groups[i-1]\n",
    "        df_train_list = [groups[j] for j in range(k) if j != (i-1)]\n",
    "        df_train_ = pd.concat(df_train_list, ignore_index=True)\n",
    "        \n",
    "        def fold_cost_func(p):\n",
    "            return spsa_cost_function(p, df_train_, df_val)\n",
    "        \n",
    "        best_p_fold, best_c_fold = spsa_optimize(\n",
    "            fold_cost_func,\n",
    "            param_init,\n",
    "            n_iter=n_spsa_iter,\n",
    "            alpha=0.602,\n",
    "            gamma=0.101,\n",
    "            a=0.1,\n",
    "            c=0.1,\n",
    "            seed=seed+i\n",
    "        )\n",
    "        spsa_solutions.append( (best_p_fold, best_c_fold) )\n",
    "    \n",
    "    # Step 3: Among these k solutions, pick best overall param*\n",
    "    # with minimal sum of costs across all k folds\n",
    "    k_ = k\n",
    "    fold_cost_matrix = np.zeros((k_, k_), dtype=float)\n",
    "    for i in range(k_):\n",
    "        param_i = spsa_solutions[i][0]\n",
    "        for j in range(k_):\n",
    "            df_val_j = groups[j]\n",
    "            df_train_j_list = [groups[m] for m in range(k_) if m != j]\n",
    "            df_train_j = pd.concat(df_train_j_list, ignore_index=True)\n",
    "            c_ij = spsa_cost_function(param_i, df_train_j, df_val_j)\n",
    "            fold_cost_matrix[i,j] = c_ij\n",
    "    \n",
    "    total_cost_per_param = fold_cost_matrix.sum(axis=1)\n",
    "    best_index = np.argmin(total_cost_per_param)\n",
    "    best_param = spsa_solutions[best_index][0]\n",
    "    \n",
    "    # Step 4: Evaluate best_param on final holdout G_{k+1}\n",
    "    df_holdout = groups[k]\n",
    "    df_train_for_holdout = pd.concat(groups[:k], ignore_index=True)\n",
    "    \n",
    "    # We'll train the final ML model + DP discount factor = best_param,\n",
    "    # then produce a table of 5 methods on the holdout:\n",
    "    #   1) Constant Threshold\n",
    "    #   2) Dynamic Threshold-R\n",
    "    #   3) Linear Threshold\n",
    "    #   4) Wait Till End\n",
    "    #   5) Dynamic Threshold-DP (SPSA)\n",
    "    \n",
    "    # (A) Train final ML model using best_param on the union of the k folds\n",
    "    model_type, n_est, lr, m_depth, gamma_ = parse_spsa_params(best_param)\n",
    "\n",
    "    # Build the classifier\n",
    "    X_train2 = df_train_for_holdout[['EIT','NIRS','EIS']].values\n",
    "    y_train2 = df_train_for_holdout['label'].values\n",
    "    \n",
    "    if model_type == 0:\n",
    "        clf_final = RandomForestClassifier(n_estimators=n_est, max_depth=m_depth, random_state=0)\n",
    "    elif model_type == 1:\n",
    "        clf_final = GradientBoostingClassifier(n_estimators=n_est, learning_rate=lr,\n",
    "                                               max_depth=m_depth, random_state=0)\n",
    "    else:\n",
    "        clf_final = CatBoostClassifier(iterations=n_est, learning_rate=lr, depth=m_depth,\n",
    "                                       verbose=0, random_state=0)\n",
    "    clf_final.fit(X_train2, y_train2)\n",
    "    \n",
    "    # (B) For DP transitions, we also need risk scores on df_train_for_holdout\n",
    "    df_train2 = df_train_for_holdout.copy()\n",
    "    train_probs2 = clf_final.predict_proba(df_train2[['EIT','NIRS','EIS']])[:,1]\n",
    "    df_train2['risk_score'] = train_probs2\n",
    "    df_train2['risk_bucket'] = df_train2['risk_score'].apply(to_bucket)\n",
    "    p_trans2, p_sick2 = estimate_transition_and_sick_probs(df_train2, T=T_MAX, n_buckets=5)\n",
    "    \n",
    "    V_final, pi_final = train_data_driven_dp_unconstrained(\n",
    "        p_trans2, p_sick2,\n",
    "        FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "        gamma=gamma_, T=T_MAX\n",
    "    )\n",
    "    dp_policy_final = make_dp_policy(V_final, pi_final, T=T_MAX)\n",
    "    \n",
    "    # (C) Generate risk scores for holdout\n",
    "    df_holdout2 = df_holdout.copy()\n",
    "    holdout_probs = clf_final.predict_proba(df_holdout2[['EIT','NIRS','EIS']])[:,1]\n",
    "    df_holdout2['risk_score'] = holdout_probs\n",
    "    \n",
    "    # -- Now we do threshold-based searches on the \"training\" set or holdout?\n",
    "    # Typically you'd tune thresholds on a validation set. For demonstration,\n",
    "    # let's do exactly what Algorithm 0 does: tune threshold on \"training\",\n",
    "    # then evaluate on holdout.\n",
    "    \n",
    "    # TUNE threshold-based on df_train2:\n",
    "    thr_const, _ = constant_threshold_search(df_train2)\n",
    "    thr_vec, _    = dynamic_threshold_random_search(df_train2, time_steps=T_MAX)\n",
    "    (A_lin, B_lin), _ = linear_threshold_search(df_train2)\n",
    "    thr_wte, _    = wait_till_end_search(df_train2)\n",
    "    \n",
    "    # Evaluate all 5 on holdout\n",
    "    # 1) Constant Threshold\n",
    "    pol_const = make_constant_threshold_policy(thr_const)\n",
    "    stats_const = simulate_policy(df_holdout2, pol_const)\n",
    "    \n",
    "    # 2) Dynamic Threshold-R\n",
    "    pol_dyn = make_dynamic_threshold_policy(thr_vec)\n",
    "    stats_dyn = simulate_policy(df_holdout2, pol_dyn)\n",
    "    \n",
    "    # 3) Linear Threshold\n",
    "    pol_lin = make_linear_threshold_policy(A_lin, B_lin)\n",
    "    stats_lin = simulate_policy(df_holdout2, pol_lin)\n",
    "    \n",
    "    # 4) Wait Till End\n",
    "    pol_wte = make_wait_till_end_policy(thr_wte)\n",
    "    stats_wte = simulate_policy(df_holdout2, pol_wte)\n",
    "    \n",
    "    # 5) Dynamic Threshold-DP (SPSA)\n",
    "    df_holdout2_dp = df_holdout2.copy()\n",
    "    df_holdout2_dp['risk_bucket'] = df_holdout2_dp['risk_score'].apply(to_bucket)\n",
    "    stats_dp = simulate_policy(df_holdout2_dp, dp_policy_final)\n",
    "    \n",
    "    # Build final table\n",
    "    table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            'Dynamic Threshold-DP (SPSA)'\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Avg Treat Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Load synthetic data\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    \n",
    "    # Filter to time < T_MAX if needed\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # Check required columns\n",
    "    required = {'patient_id','time','EIT','NIRS','EIS','label'}\n",
    "    if not required.issubset(df_all.columns):\n",
    "        raise ValueError(f\"Your CSV must have columns at least: {required}. Found: {df_all.columns}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    #-----------------------------------------------------------------------\n",
    "    # RUN ALGORITHM 5 (SPSA) - DECISION-AWARE\n",
    "    # Produce the same kind of table with 5 policy lines\n",
    "    #-----------------------------------------------------------------------\n",
    "    table_alg5 = run_algorithm5_spsa_unconstrained(df_all, k=3, seed=42, n_spsa_iter=20)\n",
    "    print(\"\\n=== ALGORITHM 5 (SPSA) RESULTS (Unconstrained) ===\")\n",
    "    print(table_alg5.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a314ece-fe2b-4f6d-90e6-cc24181f255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running replicate 1/30 (seed=412) ===\n",
      "\n",
      "=== Running replicate 2/30 (seed=413) ===\n",
      "\n",
      "=== Running replicate 3/30 (seed=414) ===\n",
      "\n",
      "=== Running replicate 4/30 (seed=415) ===\n",
      "\n",
      "=== Running replicate 5/30 (seed=416) ===\n",
      "\n",
      "=== Running replicate 6/30 (seed=417) ===\n",
      "\n",
      "=== Running replicate 7/30 (seed=418) ===\n",
      "\n",
      "=== Running replicate 8/30 (seed=419) ===\n",
      "\n",
      "=== Running replicate 9/30 (seed=420) ===\n",
      "\n",
      "=== Running replicate 10/30 (seed=421) ===\n",
      "\n",
      "=== Running replicate 11/30 (seed=422) ===\n",
      "\n",
      "=== Running replicate 12/30 (seed=423) ===\n",
      "\n",
      "=== Running replicate 13/30 (seed=424) ===\n",
      "\n",
      "=== Running replicate 14/30 (seed=425) ===\n",
      "\n",
      "=== Running replicate 15/30 (seed=426) ===\n",
      "\n",
      "=== Running replicate 16/30 (seed=427) ===\n",
      "\n",
      "=== Running replicate 17/30 (seed=428) ===\n",
      "\n",
      "=== Running replicate 18/30 (seed=429) ===\n",
      "\n",
      "=== Running replicate 19/30 (seed=430) ===\n",
      "\n",
      "=== Running replicate 20/30 (seed=431) ===\n",
      "\n",
      "=== Running replicate 21/30 (seed=432) ===\n",
      "\n",
      "=== Running replicate 22/30 (seed=433) ===\n",
      "\n",
      "=== Running replicate 23/30 (seed=434) ===\n",
      "\n",
      "=== Running replicate 24/30 (seed=435) ===\n",
      "\n",
      "=== Running replicate 25/30 (seed=436) ===\n",
      "\n",
      "=== Running replicate 26/30 (seed=437) ===\n",
      "\n",
      "=== Running replicate 27/30 (seed=438) ===\n",
      "\n",
      "=== Running replicate 28/30 (seed=439) ===\n",
      "\n",
      "=== Running replicate 29/30 (seed=440) ===\n",
      "\n",
      "=== Running replicate 30/30 (seed=441) ===\n",
      "\n",
      "=== ALGORITHM 5 (SPSA) - MULTI-REPLICATE RESULTS (Unconstrained) ===\n",
      "Ran 30 replicates. Aggregated (mean ± std) results:\n",
      "                     Method           Cost Precision (%)    Recall (%) Avg Treat Time\n",
      "         Constant Threshold 463.10 ± 72.89  57.84 ± 7.56  99.27 ± 3.99    4.43 ± 1.35\n",
      "Dynamic Threshold-DP (SPSA) 280.00 ± 50.40  91.64 ± 6.43  99.71 ± 1.61    8.20 ± 0.85\n",
      "        Dynamic Threshold-R 495.43 ± 92.83  50.63 ± 9.42 100.00 ± 0.00    8.26 ± 0.89\n",
      "           Linear Threshold 533.73 ± 50.89  45.77 ± 4.90 100.00 ± 0.00    2.68 ± 0.40\n",
      "              Wait Till End 630.33 ± 84.47  97.62 ± 3.24  98.49 ± 2.33   20.00 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALGORITHM 5 (SPSA) FOR UNCONSTRAINED HEMORRHAGE DIAGNOSIS & TREATMENT\n",
    "- Multi-run version, aggregating mean ± std results across replicates.\n",
    "\n",
    "Requirements:\n",
    "  pip install numpy pandas scikit-learn catboost\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Sklearn models, metrics, etc.\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS\n",
    "###############################################################################\n",
    "FP_COST = 10\n",
    "FN_COST = 50\n",
    "D_COST  = 1\n",
    "T_MAX   = 21   # maximum discrete time steps (0..T_MAX-1)\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS (DATA SPLIT, METRICS, ETC.)\n",
    "###############################################################################\n",
    "def split_into_four_groups(df, seed=0):\n",
    "    \"\"\"\n",
    "    A quick 4-group split. (Not used by Algorithm 5 directly, \n",
    "    but shown here for reference if needed.)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    \n",
    "    n = len(unique_pids)\n",
    "    i1 = int(0.25 * n)\n",
    "    i2 = int(0.50 * n)\n",
    "    i3 = int(0.75 * n)\n",
    "    \n",
    "    G1_pids = unique_pids[: i1]\n",
    "    G2_pids = unique_pids[i1 : i2]\n",
    "    G3_pids = unique_pids[i2 : i3]\n",
    "    G4_pids = unique_pids[i3 : ]\n",
    "    \n",
    "    G1 = df[df['patient_id'].isin(G1_pids)].copy()\n",
    "    G2 = df[df['patient_id'].isin(G2_pids)].copy()\n",
    "    G3 = df[df['patient_id'].isin(G3_pids)].copy()\n",
    "    G4 = df[df['patient_id'].isin(G4_pids)].copy()\n",
    "    return G1, G2, G3, G4\n",
    "\n",
    "def k_plus_1_splits(df, k=3, seed=0):\n",
    "    \"\"\"\n",
    "    For Algorithm 5 (SPSA), we often split into (k+1) groups:\n",
    "      G1, G2, ..., Gk, G_{k+1}.\n",
    "    The first k groups are used like cross-validation folds;\n",
    "    the (k+1)-th is the final holdout.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    \n",
    "    n = len(unique_pids)\n",
    "    fold_size = int(n/(k+1))\n",
    "    \n",
    "    groups = []\n",
    "    start = 0\n",
    "    for i in range(k):\n",
    "        pids_fold = unique_pids[start : start+fold_size]\n",
    "        start += fold_size\n",
    "        g = df[df['patient_id'].isin(pids_fold)].copy()\n",
    "        groups.append(g)\n",
    "    \n",
    "    # final group is everything leftover\n",
    "    pids_fold = unique_pids[start:]\n",
    "    g = df[df['patient_id'].isin(pids_fold)].copy()\n",
    "    groups.append(g)\n",
    "    return groups\n",
    "\n",
    "def compute_auc_score(y_true, y_prob):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "###############################################################################\n",
    "# 3. POLICY SIMULATION & BENCHMARK POLICY METHODS\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df must contain columns:\n",
    "      - patient_id\n",
    "      - time\n",
    "      - risk_score\n",
    "      - label (0 or 1)\n",
    "\n",
    "    policy_func(patient_rows) -> treat_time (int) or None\n",
    "    Returns dict of cost, precision, recall, avg_treatment_time\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pid, grp in df.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        label = grp['label'].iloc[0]\n",
    "        \n",
    "        treat_time = policy_func(grp)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp   = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp   = 0\n",
    "            fp = 0\n",
    "            treat_flag = 0\n",
    "            ttime = None\n",
    "        else:\n",
    "            treat_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            ttime = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treat_flag,\n",
    "            'treat_time': ttime,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res     = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated']==1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df = df_res[df_res['label']==1]\n",
    "    total_sick = len(sick_df)\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt   = valid_tt.mean() if len(valid_tt) > 0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "# Benchmark threshold-based searches\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['risk_score'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_constant_threshold_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=20,\n",
    "                                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec):\n",
    "                if row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def linear_threshold_search(df,\n",
    "                            A_candidates=np.linspace(-0.05, 0.01, 7),\n",
    "                            B_candidates=np.linspace(0,0.8,7)):\n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A*t + B\n",
    "                    thr = np.clip(thr,0,1)\n",
    "                    if row['risk_score'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    return (best_A,best_B), best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A,B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A*t + B\n",
    "            thr = np.clip(thr,0,1)\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            final_t = patient_rows['time'].max()\n",
    "            final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "            if final_row['risk_score'] >= thr:\n",
    "                return int(final_t)\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_t = patient_rows['time'].max()\n",
    "        final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "        if final_row['risk_score'] >= thr:\n",
    "            return int(final_t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 4. DATA-DRIVEN DP (UNCONSTRAINED)\n",
    "###############################################################################\n",
    "def to_bucket(prob):\n",
    "    \"\"\"Map a probability into a bucket 0..4.\"\"\"\n",
    "    b = int(prob * 5)\n",
    "    return min(b, 4)\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows= grp.to_dict('records')\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            lbl = row['label']\n",
    "            \n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            \n",
    "            if i < len(rows)-1:\n",
    "                nxt = rows[i+1]\n",
    "                t_next = nxt['time']\n",
    "                b_next = nxt['risk_bucket']\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t,b,b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_,b_,:].sum()\n",
    "            if denom>0:\n",
    "                p_trans[t_,b_,:] = transition_counts[t_,b_,:] / denom\n",
    "            else:\n",
    "                p_trans[t_,b_,b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_,b_]\n",
    "            if denom>0:\n",
    "                p_sick[t_,b_] = sick_counts[t_,b_] / denom\n",
    "            else:\n",
    "                p_sick[t_,b_] = 0.0\n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                       FP=10, FN=50, D=1, gamma=0.99, T=20):\n",
    "    \"\"\"\n",
    "    Standard DP for unconstrained scenario:\n",
    "      V[t,b] = min( cost_treat_now, cost_wait )\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # boundary at t=T\n",
    "    for b in range(n_buckets):\n",
    "        cost_treat   = p_sick[T-1,b]*(D*(T-1)) + (1-p_sick[T-1,b])*FP\n",
    "        cost_notreat = p_sick[T-1,b]*FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    # fill from T-1 down to 0\n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1-p_sick[t,b])*FP\n",
    "            \n",
    "            # wait\n",
    "            if t == T-1:\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next]*V[t+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    return V, pi_\n",
    "\n",
    "def make_dp_policy(V, pi_, T=20):\n",
    "    \"\"\"Return a policy function that treats if pi[t,b]==1 at time t.\"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 5. SPSA IMPLEMENTATION\n",
    "###############################################################################\n",
    "def spsa_optimize(cost_func, param_init, n_iter=20,\n",
    "                  alpha=0.602, gamma=0.101, a=0.1, c=0.1, seed=0):\n",
    "    \"\"\"\n",
    "    Basic SPSA routine for iterative approximate gradient-based optimization.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    p = param_init.copy()\n",
    "    best_p = p.copy()\n",
    "    best_cost = cost_func(p)\n",
    "    \n",
    "    for k in range(1, n_iter+1):\n",
    "        ak = a / (k**alpha)\n",
    "        ck = c / (k**gamma)\n",
    "        delta = rng.choice([-1,1], size=len(p))\n",
    "        \n",
    "        p_plus  = p + ck*delta\n",
    "        p_minus = p - ck*delta\n",
    "        \n",
    "        cost_plus  = cost_func(p_plus)\n",
    "        cost_minus = cost_func(p_minus)\n",
    "        \n",
    "        g_approx = (cost_plus - cost_minus)/(2.0*ck) * delta\n",
    "        \n",
    "        p = p - ak*g_approx\n",
    "        \n",
    "        current_cost = cost_func(p)\n",
    "        if current_cost < best_cost:\n",
    "            best_cost = current_cost\n",
    "            best_p = p.copy()\n",
    "    \n",
    "    return best_p, best_cost\n",
    "\n",
    "def parse_spsa_params(params):\n",
    "    \"\"\"\n",
    "    Map a real vector `params` into discrete hyperparams + DP gamma:\n",
    "      params[0]: model_type (0=RF,1=GB,2=CB)\n",
    "      params[1]: n_estimators [10..200]\n",
    "      params[2]: learning_rate [0.01..0.2]\n",
    "      params[3]: max_depth [2..10]\n",
    "      params[4]: gamma [0.90..0.999]\n",
    "    \"\"\"\n",
    "    p0 = int(round(np.clip(params[0], 0, 2)))\n",
    "    p1 = int(round(np.clip(params[1], 10, 200)))\n",
    "    p2 = float(np.clip(params[2], 0.01, 0.2))\n",
    "    p3 = int(round(np.clip(params[3], 2, 10)))\n",
    "    p4 = float(np.clip(params[4], 0.90, 0.999))\n",
    "    return (p0, p1, p2, p3, p4)\n",
    "\n",
    "def spsa_cost_function(param_vector, df_train, df_val):\n",
    "    \"\"\"\n",
    "    Decision-aware cost:\n",
    "      1) Parse param_vector => (model_type, n_estimators, learning_rate, max_depth, gamma)\n",
    "      2) Train model on df_train\n",
    "      3) Predict risk on df_val\n",
    "      4) Build DP with gamma using transitions from df_train\n",
    "      5) Evaluate cost on df_val\n",
    "    \"\"\"\n",
    "    model_type, n_est, lr, m_depth, gamma_ = parse_spsa_params(param_vector)\n",
    "\n",
    "    X_tr = df_train[['EIT','NIRS','EIS']].values\n",
    "    y_tr = df_train['label'].values\n",
    "    \n",
    "    if model_type == 0:\n",
    "        clf = RandomForestClassifier(n_estimators=n_est, max_depth=m_depth, random_state=0)\n",
    "    elif model_type == 1:\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_est, learning_rate=lr,\n",
    "                                         max_depth=m_depth, random_state=0)\n",
    "    else:\n",
    "        clf = CatBoostClassifier(iterations=n_est, learning_rate=lr, depth=m_depth,\n",
    "                                 verbose=0, random_state=0)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    X_val = df_val[['EIT','NIRS','EIS']].values\n",
    "    prob_val = clf.predict_proba(X_val)[:,1]\n",
    "    df_val_ = df_val.copy()\n",
    "    df_val_['risk_score'] = prob_val\n",
    "    \n",
    "    # DP transitions from df_train\n",
    "    df_tr_ = df_train.copy()\n",
    "    train_probs = clf.predict_proba(df_tr_[['EIT','NIRS','EIS']])[:,1]\n",
    "    df_tr_['risk_score'] = train_probs\n",
    "    df_tr_['risk_bucket'] = df_tr_['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_tr_, T=T_MAX, n_buckets=5)\n",
    "    V, pi_ = train_data_driven_dp_unconstrained(\n",
    "        p_trans, p_sick,\n",
    "        FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "        gamma=gamma_, T=T_MAX\n",
    "    )\n",
    "    \n",
    "    df_val_['risk_bucket'] = df_val_['risk_score'].apply(to_bucket)\n",
    "    dp_policy = make_dp_policy(V, pi_, T=T_MAX)\n",
    "    stats = simulate_policy(df_val_, dp_policy)\n",
    "    return stats['cost']\n",
    "\n",
    "def run_algorithm5_spsa_unconstrained(df_all, k=3, seed=0, n_spsa_iter=20):\n",
    "    \"\"\"\n",
    "    SPSA Hyper-Parameter Tuning (Decision-Aware).\n",
    "    \n",
    "    1) Split data into k+1 groups: G1..Gk, G_{k+1}\n",
    "    2) For each fold i in [1..k]:\n",
    "         - define cost function that trains on G\\\\G_i, evaluates cost on G_i\n",
    "         - run SPSA to find best param_i\n",
    "    3) Among param_1..param_k, pick best overall param* with minimal sum of costs across folds\n",
    "    4) Evaluate param* on G_{k+1}.\n",
    "    5) For final report, we also show the five benchmark policies on the holdout\n",
    "       *using the same final trained model from param* (and its DP discount factor).\n",
    "    \"\"\"\n",
    "    # (0) Filter to time < T_MAX if needed\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # (1) Create k+1 splits\n",
    "    groups = k_plus_1_splits(df_all, k=k, seed=seed)\n",
    "    # groups[0..k-1] => folds for cross-validation\n",
    "    # groups[k]      => final holdout\n",
    "    \n",
    "    # (2) For each fold, run SPSA to get a candidate param_i\n",
    "    param_init = np.array([1.0, 50.0, 0.05, 3.0, 0.95], dtype=float)\n",
    "    \n",
    "    spsa_solutions = []\n",
    "    for i in range(1, k+1):\n",
    "        df_val = groups[i-1]\n",
    "        df_train_list = [groups[j] for j in range(k) if j != (i-1)]\n",
    "        df_train_ = pd.concat(df_train_list, ignore_index=True)\n",
    "        \n",
    "        def fold_cost_func(p):\n",
    "            return spsa_cost_function(p, df_train_, df_val)\n",
    "        \n",
    "        best_p_fold, best_c_fold = spsa_optimize(\n",
    "            fold_cost_func,\n",
    "            param_init,\n",
    "            n_iter=n_spsa_iter,\n",
    "            alpha=0.602,\n",
    "            gamma=0.101,\n",
    "            a=0.1,\n",
    "            c=0.1,\n",
    "            seed=seed+i\n",
    "        )\n",
    "        spsa_solutions.append( (best_p_fold, best_c_fold) )\n",
    "    \n",
    "    # (3) Among these k solutions, pick best overall param*\n",
    "    # with minimal sum of costs across all k folds\n",
    "    k_ = k\n",
    "    fold_cost_matrix = np.zeros((k_, k_), dtype=float)\n",
    "    for i in range(k_):\n",
    "        param_i = spsa_solutions[i][0]\n",
    "        for j in range(k_):\n",
    "            df_val_j = groups[j]\n",
    "            df_train_j_list = [groups[m] for m in range(k_) if m != j]\n",
    "            df_train_j = pd.concat(df_train_j_list, ignore_index=True)\n",
    "            c_ij = spsa_cost_function(param_i, df_train_j, df_val_j)\n",
    "            fold_cost_matrix[i,j] = c_ij\n",
    "    \n",
    "    total_cost_per_param = fold_cost_matrix.sum(axis=1)\n",
    "    best_index = np.argmin(total_cost_per_param)\n",
    "    best_param = spsa_solutions[best_index][0]\n",
    "    \n",
    "    # (4) Evaluate best_param on final holdout G_{k+1}\n",
    "    df_holdout = groups[k]\n",
    "    df_train_for_holdout = pd.concat(groups[:k], ignore_index=True)\n",
    "    \n",
    "    # We'll train the final ML model + DP discount factor = best_param,\n",
    "    # then produce a table of 5 methods on the holdout\n",
    "    model_type, n_est, lr, m_depth, gamma_ = parse_spsa_params(best_param)\n",
    "\n",
    "    # Build the classifier on df_train_for_holdout\n",
    "    X_train2 = df_train_for_holdout[['EIT','NIRS','EIS']].values\n",
    "    y_train2 = df_train_for_holdout['label'].values\n",
    "    \n",
    "    if model_type == 0:\n",
    "        clf_final = RandomForestClassifier(n_estimators=n_est, max_depth=m_depth, random_state=0)\n",
    "    elif model_type == 1:\n",
    "        clf_final = GradientBoostingClassifier(n_estimators=n_est, learning_rate=lr,\n",
    "                                               max_depth=m_depth, random_state=0)\n",
    "    else:\n",
    "        clf_final = CatBoostClassifier(iterations=n_est, learning_rate=lr, depth=m_depth,\n",
    "                                       verbose=0, random_state=0)\n",
    "    clf_final.fit(X_train2, y_train2)\n",
    "    \n",
    "    # DP transitions from df_train_for_holdout\n",
    "    df_train2 = df_train_for_holdout.copy()\n",
    "    train_probs2 = clf_final.predict_proba(df_train2[['EIT','NIRS','EIS']])[:,1]\n",
    "    df_train2['risk_score'] = train_probs2\n",
    "    df_train2['risk_bucket'] = df_train2['risk_score'].apply(to_bucket)\n",
    "    p_trans2, p_sick2 = estimate_transition_and_sick_probs(df_train2, T=T_MAX, n_buckets=5)\n",
    "    \n",
    "    V_final, pi_final = train_data_driven_dp_unconstrained(\n",
    "        p_trans2, p_sick2,\n",
    "        FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "        gamma=gamma_, T=T_MAX\n",
    "    )\n",
    "    dp_policy_final = make_dp_policy(V_final, pi_final, T=T_MAX)\n",
    "    \n",
    "    # Generate risk scores for holdout\n",
    "    df_holdout2 = df_holdout.copy()\n",
    "    holdout_probs = clf_final.predict_proba(df_holdout2[['EIT','NIRS','EIS']])[:,1]\n",
    "    df_holdout2['risk_score'] = holdout_probs\n",
    "    \n",
    "    # TUNE threshold-based policies on df_train2 for consistency\n",
    "    thr_const, _ = constant_threshold_search(df_train2)\n",
    "    thr_vec, _    = dynamic_threshold_random_search(df_train2, time_steps=T_MAX)\n",
    "    (A_lin, B_lin), _ = linear_threshold_search(df_train2)\n",
    "    thr_wte, _    = wait_till_end_search(df_train2)\n",
    "    \n",
    "    # Evaluate all on holdout\n",
    "    # 1) Constant Threshold\n",
    "    pol_const = make_constant_threshold_policy(thr_const)\n",
    "    stats_const = simulate_policy(df_holdout2, pol_const)\n",
    "    \n",
    "    # 2) Dynamic Threshold-R\n",
    "    pol_dyn = make_dynamic_threshold_policy(thr_vec)\n",
    "    stats_dyn = simulate_policy(df_holdout2, pol_dyn)\n",
    "    \n",
    "    # 3) Linear Threshold\n",
    "    pol_lin = make_linear_threshold_policy(A_lin, B_lin)\n",
    "    stats_lin = simulate_policy(df_holdout2, pol_lin)\n",
    "    \n",
    "    # 4) Wait Till End\n",
    "    pol_wte = make_wait_till_end_policy(thr_wte)\n",
    "    stats_wte = simulate_policy(df_holdout2, pol_wte)\n",
    "    \n",
    "    # 5) Dynamic Threshold-DP (SPSA)\n",
    "    df_holdout2_dp = df_holdout2.copy()\n",
    "    df_holdout2_dp['risk_bucket'] = df_holdout2_dp['risk_score'].apply(to_bucket)\n",
    "    stats_dp = simulate_policy(df_holdout2_dp, dp_policy_final)\n",
    "    \n",
    "    table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            'Dynamic Threshold-DP (SPSA)'\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Avg Treat Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return table\n",
    "\n",
    "###############################################################################\n",
    "# 6. MAIN - RUN MULTIPLE REPLICATES AND AGGREGATE\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Number of replicates to run\n",
    "    NUM_REPLICATES = 30\n",
    "    \n",
    "    # Load synthetic data\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    \n",
    "    # Optionally filter to time < T_MAX\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # Check required columns\n",
    "    required = {'patient_id','time','EIT','NIRS','EIS','label'}\n",
    "    if not required.issubset(df_all.columns):\n",
    "        raise ValueError(\n",
    "            f\"Your CSV must have columns at least: {required}. Found: {df_all.columns}\"\n",
    "        )\n",
    "    \n",
    "    # Run ALGORITHM 5 (SPSA) multiple times with different seeds\n",
    "    all_tables = []\n",
    "    for rep in range(NUM_REPLICATES):\n",
    "        seed_for_this_run = 412 + rep\n",
    "        print(f\"\\n=== Running replicate {rep+1}/{NUM_REPLICATES} (seed={seed_for_this_run}) ===\")\n",
    "        \n",
    "        result_table = run_algorithm5_spsa_unconstrained(\n",
    "            df_all, \n",
    "            k=3, \n",
    "            seed=seed_for_this_run, \n",
    "            n_spsa_iter=20\n",
    "        )\n",
    "        all_tables.append(result_table)\n",
    "    \n",
    "    # Combine all replicate results into one DataFrame\n",
    "    combined_df = pd.concat(all_tables, ignore_index=True)\n",
    "    \n",
    "    # Group by 'Method' and compute mean ± std for each numeric column\n",
    "    grouped = combined_df.groupby('Method')\n",
    "    \n",
    "    final_rows = []\n",
    "    for method, group_data in grouped:\n",
    "        cost_mean = group_data['Cost'].mean()\n",
    "        cost_std  = group_data['Cost'].std()\n",
    "        \n",
    "        prec_mean = group_data['Precision (%)'].mean()\n",
    "        prec_std  = group_data['Precision (%)'].std()\n",
    "        \n",
    "        rec_mean  = group_data['Recall (%)'].mean()\n",
    "        rec_std   = group_data['Recall (%)'].std()\n",
    "        \n",
    "        time_mean = group_data['Avg Treat Time'].mean()\n",
    "        time_std  = group_data['Avg Treat Time'].std()\n",
    "        \n",
    "        final_rows.append({\n",
    "            'Method': method,\n",
    "            'Cost': f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "            'Precision (%)': f\"{prec_mean:.2f} ± {prec_std:.2f}\",\n",
    "            'Recall (%)': f\"{rec_mean:.2f} ± {rec_std:.2f}\",\n",
    "            'Avg Treat Time': f\"{time_mean:.2f} ± {time_std:.2f}\"\n",
    "        })\n",
    "    \n",
    "    final_df = pd.DataFrame(final_rows)\n",
    "    \n",
    "    print(\"\\n=== ALGORITHM 5 (SPSA) - MULTI-REPLICATE RESULTS (Unconstrained) ===\")\n",
    "    print(f\"Ran {NUM_REPLICATES} replicates. Aggregated (mean ± std) results:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2143a4d-e2e7-4b35-990c-d66d4dad3b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
