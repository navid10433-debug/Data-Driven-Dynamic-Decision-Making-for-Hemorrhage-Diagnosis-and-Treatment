{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "279d1c9a-ad6c-4125-9577-799923d4c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final chosen hyperparams (lambda^*) ===\n",
      "('cat', (('depth', 3), ('iterations', 50), ('learning_rate', 0.05)))\n",
      "=== Final chosen discount factor (mu^*) ===\n",
      "0.99\n",
      "\n",
      "=== ALGORITHM 2 (FULL MATCH) RESULTS ===\n",
      "              Method  Cost  Precision (%)  Recall (%)  Avg Treat Time\n",
      "  Constant Threshold   253      58.333333  100.000000        2.888889\n",
      " Dynamic Threshold-R   187      70.000000  100.000000        6.133333\n",
      "    Linear Threshold   372      41.176471  100.000000        1.450980\n",
      "       Wait Till End   450     100.000000   95.238095       20.000000\n",
      "Dynamic Threshold-DP   181      91.304348  100.000000        7.869565\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Sklearn models, metrics, etc.\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "###############################################################################\n",
    "# GLOBAL PARAMETERS (same as your original)\n",
    "###############################################################################\n",
    "FP_COST = 10    # False positive cost\n",
    "FN_COST = 50    # False negative cost\n",
    "D_COST  = 1     # Delay cost per time\n",
    "T_MAX   = 21    # maximum discrete time steps (0..T_MAX-1)\n",
    "\n",
    "# We'll interpret mu as the discount factor gamma\n",
    "MU_CANDIDATES = [0.95, 0.99]\n",
    "\n",
    "# Example hyperparameter grids\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "GB_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "CATBOOST_PARAM_GRID = {\n",
    "    'iterations': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3, 5]\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# HELPER FUNCTIONS \n",
    "###############################################################################\n",
    "def split_into_nplus1_groups(df, n=4, seed=0):\n",
    "    \"\"\"Shuffle patient IDs and split ~evenly into (n+1) groups: G1..G_{n+1}.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    N = len(unique_pids)\n",
    "    group_size = int(np.ceil(N/(n+1)))\n",
    "    \n",
    "    groups = []\n",
    "    start_idx = 0\n",
    "    for i in range(n+1):\n",
    "        end_idx = min(start_idx+group_size, N)\n",
    "        group_pids = unique_pids[start_idx:end_idx]\n",
    "        group_df   = df[df['patient_id'].isin(group_pids)].copy()\n",
    "        groups.append(group_df)\n",
    "        start_idx = end_idx\n",
    "    return groups\n",
    "\n",
    "def compute_auc_score(y_true, y_prob):\n",
    "    \"\"\"Compute AUC safely. If only one class, return 0.5 to avoid errors.\"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df must contain columns: patient_id, time, label, risk_score\n",
    "    policy_func(patient_rows) -> treat_time (int) or None\n",
    "    \n",
    "    Returns dict of: cost, precision, recall, avg_treatment_time\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pid, grp in df.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        label = grp['label'].iloc[0]\n",
    "        treat_time = policy_func(grp)\n",
    "        \n",
    "        if treat_time is None:  # never treat\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp   = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp   = 0\n",
    "            fp = 0\n",
    "            treat_flag = 0\n",
    "            ttime = None\n",
    "        else:\n",
    "            treat_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            ttime = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treat_flag,\n",
    "            'treat_time': ttime,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated']==1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    \n",
    "    precision = 0.0\n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    \n",
    "    sick_df = df_res[df_res['label']==1]\n",
    "    total_sick = len(sick_df)\n",
    "    recall = 0.0\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    \n",
    "    avg_tt = 0.0\n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        if len(valid_tt) > 0:\n",
    "            avg_tt = valid_tt.mean()\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# THRESHOLD-based Policies\n",
    "###############################################################################\n",
    "def make_constant_threshold_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    for thr in thresholds:\n",
    "        policy = make_constant_threshold_policy(thr)\n",
    "        stats = simulate_policy(df, policy)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec) and row['risk_score'] >= thr_vec[t]:\n",
    "                return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=20,\n",
    "                                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        policy  = make_dynamic_threshold_policy(thr_vec)\n",
    "        stats   = simulate_policy(df, policy)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A, B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A*t + B\n",
    "            thr = np.clip(thr, 0, 1)\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def linear_threshold_search(df,\n",
    "                            A_candidates=np.linspace(-0.05, 0.01, 7),\n",
    "                            B_candidates=np.linspace(0,0.6,4)):\n",
    "    best_A, best_B = None, None\n",
    "    best_cost = float('inf')\n",
    "    best_stats= None\n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            policy = make_linear_threshold_policy(A, B)\n",
    "            stats  = simulate_policy(df, policy)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    return (best_A, best_B), best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_t = patient_rows['time'].max()\n",
    "        final_row = patient_rows[patient_rows['time'] == final_t].iloc[0]\n",
    "        if final_row['risk_score'] >= thr:\n",
    "            return int(final_t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    for thr in thresholds:\n",
    "        policy = make_wait_till_end_policy(thr)\n",
    "        stats  = simulate_policy(df, policy)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "###############################################################################\n",
    "# DP-related helpers\n",
    "###############################################################################\n",
    "def to_bucket(prob):\n",
    "    \"\"\"Map a probability into 5 discrete buckets [0..4].\"\"\"\n",
    "    b = int(prob * 5)\n",
    "    return min(b, 4)\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    \"\"\"\n",
    "    Estimate p_trans[t,b,b_next] and p_sick[t,b].\n",
    "    df_train must have columns: 'patient_id','time','risk_bucket','label'\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows = grp.to_dict('records')\n",
    "        for i, row in enumerate(rows):\n",
    "            t   = int(row['time'])\n",
    "            b   = int(row['risk_bucket'])\n",
    "            lbl = row['label']\n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            if i < len(rows)-1:\n",
    "                nxt = rows[i+1]\n",
    "                t_next = nxt['time']\n",
    "                b_next = nxt['risk_bucket']\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t,b,b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_,b_,:].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_,b_,:] = transition_counts[t_,b_,:] / denom\n",
    "            else:\n",
    "                p_trans[t_,b_,b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_,b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_,b_] = sick_counts[t_,b_] / denom\n",
    "            else:\n",
    "                p_sick[t_,b_] = 0.0\n",
    "    \n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                       FP=10, FN=50, D=1, gamma=0.99, T=20):\n",
    "    \"\"\"\n",
    "    Standard unconstrained DP for each bucket b at each time t:\n",
    "      V[t,b] = min( cost_treat_now, cost_wait )\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V  = np.zeros((T+1, n_buckets))\n",
    "    pi = np.zeros((T,   n_buckets), dtype=int)\n",
    "    \n",
    "    # boundary at t = T\n",
    "    for b in range(n_buckets):\n",
    "        # if we \"treat\" at the last time (T-1):\n",
    "        cost_treat   = p_sick[T-1,b] * (D*(T-1)) + (1 - p_sick[T-1,b])*FP\n",
    "        # if we \"do not treat\" at all:\n",
    "        cost_notreat = p_sick[T-1,b] * FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    # fill from t = T-1 down to 0\n",
    "    for t_ in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            cost_treat = p_sick[t_,b]*(D*t_) + (1 - p_sick[t_,b])*FP\n",
    "            # cost_wait:\n",
    "            if t_ == T-1:\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t_,b,b_next] * V[t_+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t_,b] = cost_treat\n",
    "                pi[t_,b] = 1  # treat now\n",
    "            else:\n",
    "                V[t_,b] = cost_wait\n",
    "                pi[t_,b] = 0  # wait\n",
    "    return V, pi\n",
    "\n",
    "def make_dp_policy(V, pi_, T=20):\n",
    "    \"\"\"\n",
    "    Return a policy function that treats at time t if pi_[t,bucket] == 1\n",
    "    \"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# ALGORITHM 2 (Fully Matching the Pseudo-code) with Fixes\n",
    "###############################################################################\n",
    "def run_algorithm2_full_match(df_all, n=4, seed=0):\n",
    "    \"\"\"\n",
    "    Attempts to implement Algorithm 2 from your snippet exactly,\n",
    "    but with:\n",
    "      - docstring fix\n",
    "      - 'lambda_cand' stored as a tuple so it's hashable\n",
    "    \"\"\"\n",
    "    # Keep only time < T_MAX\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # 1) Split\n",
    "    groups = split_into_nplus1_groups(df_all, n=n, seed=seed)\n",
    "    G_cv   = groups[:-1] # G1..Gn\n",
    "    G_test = groups[-1]  # G_{n+1}\n",
    "    \n",
    "    # Build \"lambda_candidates\" as tuples: (model_type, (sorted param items))\n",
    "    lambda_candidates = []\n",
    "    for p in ParameterGrid(RF_PARAM_GRID):\n",
    "        # param_tuple is something like (('max_depth',3),('n_estimators',50)) \n",
    "        param_tuple = tuple(sorted(p.items()))\n",
    "        lambda_candidates.append(('rf', param_tuple))\n",
    "    for p in ParameterGrid(GB_PARAM_GRID):\n",
    "        param_tuple = tuple(sorted(p.items()))\n",
    "        lambda_candidates.append(('gb', param_tuple))\n",
    "    for p in ParameterGrid(CATBOOST_PARAM_GRID):\n",
    "        param_tuple = tuple(sorted(p.items()))\n",
    "        lambda_candidates.append(('cat', param_tuple))\n",
    "    \n",
    "    def dict_from_param_tuple(param_tuple):\n",
    "        \"\"\"Convert tuple of (key,value) pairs back to dict.\"\"\"\n",
    "        return dict(param_tuple)\n",
    "    \n",
    "    # Helper function to train + compute \"AUCCost = 1 - AUC\"\n",
    "    def compute_AUCCost(lambda_cand, train_df, val_df):\n",
    "        \"\"\"\n",
    "        lambda_cand = (model_type, param_tuple)\n",
    "        train model on train_df, compute AUC on val_df => cost = 1 - AUC\n",
    "        \"\"\"\n",
    "        model_type, param_tuple = lambda_cand\n",
    "        param_dict = dict_from_param_tuple(param_tuple)\n",
    "        \n",
    "        X_train = train_df[['EIT','NIRS','EIS']].values\n",
    "        y_train = train_df['label'].values\n",
    "        X_val   = val_df[['EIT','NIRS','EIS']].values\n",
    "        y_val   = val_df['label'].values\n",
    "        \n",
    "        if model_type == 'rf':\n",
    "            mdl = RandomForestClassifier(random_state=0, **param_dict)\n",
    "        elif model_type == 'gb':\n",
    "            mdl = GradientBoostingClassifier(random_state=0, **param_dict)\n",
    "        else:\n",
    "            mdl = CatBoostClassifier(verbose=0, random_state=0, **param_dict)\n",
    "        \n",
    "        mdl.fit(X_train, y_train)\n",
    "        prob_val = mdl.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, prob_val)\n",
    "        return 1.0 - auc_val  # \"AUCCost\"\n",
    "    \n",
    "    # Helper to train final model + build DP => \"ActualCost\"\n",
    "    def compute_ActualCost(lambda_cand, mu, train_df, val_df):\n",
    "        \"\"\"\n",
    "        mu = discount factor\n",
    "        1) train model on train_df\n",
    "        2) get risk scores on val_df\n",
    "        3) make DP policy with discount=mu\n",
    "        4) simulate => cost\n",
    "        \"\"\"\n",
    "        model_type, param_tuple = lambda_cand\n",
    "        param_dict = dict_from_param_tuple(param_tuple)\n",
    "        \n",
    "        X_train = train_df[['EIT','NIRS','EIS']].values\n",
    "        y_train = train_df['label'].values\n",
    "        \n",
    "        if model_type == 'rf':\n",
    "            mdl = RandomForestClassifier(random_state=0, **param_dict)\n",
    "        elif model_type == 'gb':\n",
    "            mdl = GradientBoostingClassifier(random_state=0, **param_dict)\n",
    "        else:\n",
    "            mdl = CatBoostClassifier(verbose=0, random_state=0, **param_dict)\n",
    "        mdl.fit(X_train, y_train)\n",
    "        \n",
    "        # Risk scores\n",
    "        val_df_ = val_df.copy()\n",
    "        X_val   = val_df_[['EIT','NIRS','EIS']].values\n",
    "        prob_val= mdl.predict_proba(X_val)[:,1]\n",
    "        val_df_['risk_score'] = prob_val\n",
    "        val_df_['risk_bucket']= val_df_['risk_score'].apply(to_bucket)\n",
    "        \n",
    "        # Build DP from train\n",
    "        train_df_ = train_df.copy()\n",
    "        prob_tr   = mdl.predict_proba(train_df_[['EIT','NIRS','EIS']].values)[:,1]\n",
    "        train_df_['risk_score']  = prob_tr\n",
    "        train_df_['risk_bucket'] = train_df_['risk_score'].apply(to_bucket)\n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(train_df_,\n",
    "                                                             T=T_MAX,\n",
    "                                                             n_buckets=5)\n",
    "        V, pi_ = train_data_driven_dp_unconstrained(\n",
    "            p_trans, p_sick,\n",
    "            FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "            gamma=mu, T=T_MAX\n",
    "        )\n",
    "        dp_policy = make_dp_policy(V, pi_, T=T_MAX)\n",
    "        \n",
    "        stats = simulate_policy(val_df_, dp_policy)\n",
    "        return stats['cost']\n",
    "    \n",
    "    nfolds = len(G_cv)  # = n\n",
    "    G_all_cv = pd.concat(G_cv, ignore_index=True)  # union G1..Gn\n",
    "    \n",
    "    ###################################################################\n",
    "    # 2) Outer loop j = 1..n\n",
    "    ###################################################################\n",
    "    best_lambda_for_j = [None]*n\n",
    "    cost_j_mu = {j: {} for j in range(n)}\n",
    "    \n",
    "    for j in range(n):\n",
    "        G_outer_val = G_cv[j]\n",
    "        train_list = [G_cv[m] for m in range(n) if m != j]\n",
    "        G_outer_train = pd.concat(train_list, ignore_index=True)\n",
    "        \n",
    "        # (a) Inner cross among i != j\n",
    "        sum_auccost = {}\n",
    "        for lam in lambda_candidates:\n",
    "            sum_auccost[lam] = 0.0  # we can store because 'lam' is now hashable\n",
    "        \n",
    "        i_indices = [x for x in range(n) if x != j]\n",
    "        for i_ in i_indices:\n",
    "            G_inner_val = G_cv[i_]\n",
    "            inner_train_list = [G_cv[m] for m in range(n) if (m != j) and (m != i_)]\n",
    "            G_inner_train = pd.concat(inner_train_list, ignore_index=True)\n",
    "            \n",
    "            for lam in lambda_candidates:\n",
    "                cost_ij = compute_AUCCost(lam, G_inner_train, G_inner_val)\n",
    "                sum_auccost[lam] += cost_ij\n",
    "        \n",
    "        # pick lambda_j^*\n",
    "        best_lam_j = None\n",
    "        best_val = float('inf')\n",
    "        for lam in lambda_candidates:\n",
    "            if sum_auccost[lam] < best_val:\n",
    "                best_val = sum_auccost[lam]\n",
    "                best_lam_j = lam\n",
    "        best_lambda_for_j[j] = best_lam_j\n",
    "        \n",
    "        # (b) Retrain on G_outer_train with lambda_j^*, measure ActualCost for each mu\n",
    "        for mu_ in MU_CANDIDATES:\n",
    "            c_j_mu = compute_ActualCost(best_lam_j, mu_, G_outer_train, G_outer_val)\n",
    "            cost_j_mu[j][mu_] = c_j_mu\n",
    "    \n",
    "    # (c) pick final mu^* by summing cost_j_mu across j\n",
    "    mu_star = None\n",
    "    best_sum_cost = float('inf')\n",
    "    for mu_ in MU_CANDIDATES:\n",
    "        sum_c = 0.0\n",
    "        for j in range(n):\n",
    "            sum_c += cost_j_mu[j][mu_]\n",
    "        if sum_c < best_sum_cost:\n",
    "            best_sum_cost = sum_c\n",
    "            mu_star = mu_\n",
    "    \n",
    "    ###################################################################\n",
    "    # 4) \"Full cross\" pass #1: pick final lambda^*\n",
    "    ###################################################################\n",
    "    sum_auccost_all_i = {}\n",
    "    for lam in lambda_candidates:\n",
    "        sum_auccost_all_i[lam] = 0.0\n",
    "    \n",
    "    for i_ in range(n):\n",
    "        G_val_i = G_cv[i_]\n",
    "        G_train_i = pd.concat([G_cv[k] for k in range(n) if k != i_], ignore_index=True)\n",
    "        for lam in lambda_candidates:\n",
    "            c_auccost = compute_AUCCost(lam, G_train_i, G_val_i)\n",
    "            sum_auccost_all_i[lam] += c_auccost\n",
    "    \n",
    "    lambda_star = None\n",
    "    best_val2 = float('inf')\n",
    "    for lam in lambda_candidates:\n",
    "        if sum_auccost_all_i[lam] < best_val2:\n",
    "            best_val2 = sum_auccost_all_i[lam]\n",
    "            lambda_star = lam\n",
    "    \n",
    "    ###################################################################\n",
    "    # 5) \"Full cross\" pass #2: pick final mu^*\n",
    "    #    In practice, we already have mu_star from step (3).\n",
    "    #    We'll still do it to match the snippet's structure.\n",
    "    ###################################################################\n",
    "    cost_i_mu_2 = {}\n",
    "    for mu_ in MU_CANDIDATES:\n",
    "        cost_i_mu_2[mu_] = 0.0\n",
    "    \n",
    "    for i_ in range(n):\n",
    "        G_val_i = G_cv[i_]\n",
    "        G_train_i = pd.concat([G_cv[k] for k in range(n) if k != i_], ignore_index=True)\n",
    "        c_i_mu_dict = {}\n",
    "        for mu_ in MU_CANDIDATES:\n",
    "            c_i_mu = compute_ActualCost(lambda_star, mu_, G_train_i, G_val_i)\n",
    "            cost_i_mu_2[mu_] += c_i_mu\n",
    "    \n",
    "    mu_star_final = None\n",
    "    best_cost_2 = float('inf')\n",
    "    for mu_ in MU_CANDIDATES:\n",
    "        if cost_i_mu_2[mu_] < best_cost_2:\n",
    "            best_cost_2 = cost_i_mu_2[mu_]\n",
    "            mu_star_final = mu_\n",
    "    \n",
    "    final_lambda = lambda_star\n",
    "    final_mu     = mu_star_final\n",
    "    \n",
    "    ###################################################################\n",
    "    # 6) Retrain final model on G1..Gn with final_lambda,\n",
    "    #    build DP with final_mu, evaluate on G_{n+1}.\n",
    "    ###################################################################\n",
    "    G_cv_concat = pd.concat(G_cv, ignore_index=True)\n",
    "    \n",
    "    model_type_final, param_tuple_final = final_lambda\n",
    "    param_dict_final = dict_from_param_tuple(param_tuple_final)\n",
    "    if model_type_final == 'rf':\n",
    "        final_mdl = RandomForestClassifier(random_state=0, **param_dict_final)\n",
    "    elif model_type_final == 'gb':\n",
    "        final_mdl = GradientBoostingClassifier(random_state=0, **param_dict_final)\n",
    "    else:\n",
    "        final_mdl = CatBoostClassifier(verbose=0, random_state=0, **param_dict_final)\n",
    "    \n",
    "    X_cv_all = G_cv_concat[['EIT','NIRS','EIS']].values\n",
    "    y_cv_all = G_cv_concat['label'].values\n",
    "    final_mdl.fit(X_cv_all, y_cv_all)\n",
    "    \n",
    "    # Build final DP policy\n",
    "    df_dp_train = G_cv_concat.copy()\n",
    "    prob_dp_tr  = final_mdl.predict_proba(df_dp_train[['EIT','NIRS','EIS']].values)[:,1]\n",
    "    df_dp_train['risk_score']  = prob_dp_tr\n",
    "    df_dp_train['risk_bucket'] = df_dp_train['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    p_trans_final, p_sick_final = estimate_transition_and_sick_probs(df_dp_train,\n",
    "                                                                     T=T_MAX,\n",
    "                                                                     n_buckets=5)\n",
    "    V_final, pi_final = train_data_driven_dp_unconstrained(\n",
    "        p_trans_final, p_sick_final,\n",
    "        FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "        gamma=final_mu, T=T_MAX\n",
    "    )\n",
    "    dp_policy_final = make_dp_policy(V_final, pi_final, T=T_MAX)\n",
    "    \n",
    "    # Evaluate on G_{n+1}\n",
    "    G_test_eval = G_test.copy()\n",
    "    prob_test   = final_mdl.predict_proba(G_test_eval[['EIT','NIRS','EIS']].values)[:,1]\n",
    "    G_test_eval['risk_score'] = prob_test\n",
    "    G_test_eval['risk_bucket'] = G_test_eval['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    stats_dp = simulate_policy(G_test_eval, dp_policy_final)\n",
    "    \n",
    "    ###################################################################\n",
    "    # OPTIONAL: Evaluate threshold-based policies on the same G_{n+1}\n",
    "    ###################################################################\n",
    "    prob_cv_final = final_mdl.predict_proba(G_cv_concat[['EIT','NIRS','EIS']].values)[:,1]\n",
    "    G_cv_concat['risk_score'] = prob_cv_final\n",
    "    \n",
    "    # 1) Constant threshold\n",
    "    best_thr_const, _ = constant_threshold_search(G_cv_concat)\n",
    "    policy_const = make_constant_threshold_policy(best_thr_const)\n",
    "    stats_const  = simulate_policy(G_test_eval, policy_const)\n",
    "    \n",
    "    # 2) Dynamic threshold (random search)\n",
    "    best_thr_vec, _ = dynamic_threshold_random_search(G_cv_concat,\n",
    "                    time_steps=T_MAX-1,\n",
    "                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                    n_samples=200, seed=123)\n",
    "    policy_dyn = make_dynamic_threshold_policy(best_thr_vec)\n",
    "    stats_dyn  = simulate_policy(G_test_eval, policy_dyn)\n",
    "    \n",
    "    # 3) Linear threshold\n",
    "    (A_lin, B_lin), _ = linear_threshold_search(G_cv_concat)\n",
    "    policy_lin = make_linear_threshold_policy(A_lin, B_lin)\n",
    "    stats_lin  = simulate_policy(G_test_eval, policy_lin)\n",
    "    \n",
    "    # 4) Wait till end\n",
    "    best_thr_wte, _ = wait_till_end_search(G_cv_concat)\n",
    "    policy_wte = make_wait_till_end_policy(best_thr_wte)\n",
    "    stats_wte  = simulate_policy(G_test_eval, policy_wte)\n",
    "    \n",
    "    # Build final table\n",
    "    final_table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            'Dynamic Threshold-DP'\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Avg Treat Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== Final chosen hyperparams (lambda^*) ===\")\n",
    "    print(final_lambda)\n",
    "    print(\"=== Final chosen discount factor (mu^*) ===\")\n",
    "    print(final_mu)\n",
    "    \n",
    "    return final_table\n",
    "\n",
    "def main():\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    final_table = run_algorithm2_full_match(df_all, n=4, seed=42)\n",
    "    print(\"\\n=== ALGORITHM 2 (FULL MATCH) RESULTS ===\")\n",
    "    print(final_table.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33719622-2fc6-49c5-8775-26dd4999a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running replicate 1/30 (seed=100) ===\n",
      "\n",
      "=== Running replicate 2/30 (seed=101) ===\n",
      "\n",
      "=== Running replicate 3/30 (seed=102) ===\n",
      "\n",
      "=== Running replicate 4/30 (seed=103) ===\n",
      "\n",
      "=== Running replicate 5/30 (seed=104) ===\n",
      "\n",
      "=== Running replicate 6/30 (seed=105) ===\n",
      "\n",
      "=== Running replicate 7/30 (seed=106) ===\n",
      "\n",
      "=== Running replicate 8/30 (seed=107) ===\n",
      "\n",
      "=== Running replicate 9/30 (seed=108) ===\n",
      "\n",
      "=== Running replicate 10/30 (seed=109) ===\n",
      "\n",
      "=== Running replicate 11/30 (seed=110) ===\n",
      "\n",
      "=== Running replicate 12/30 (seed=111) ===\n",
      "\n",
      "=== Running replicate 13/30 (seed=112) ===\n",
      "\n",
      "=== Running replicate 14/30 (seed=113) ===\n",
      "\n",
      "=== Running replicate 15/30 (seed=114) ===\n",
      "\n",
      "=== Running replicate 16/30 (seed=115) ===\n",
      "\n",
      "=== Running replicate 17/30 (seed=116) ===\n",
      "\n",
      "=== Running replicate 18/30 (seed=117) ===\n",
      "\n",
      "=== Running replicate 19/30 (seed=118) ===\n",
      "\n",
      "=== Running replicate 20/30 (seed=119) ===\n",
      "\n",
      "=== Running replicate 21/30 (seed=120) ===\n",
      "\n",
      "=== Running replicate 22/30 (seed=121) ===\n",
      "\n",
      "=== Running replicate 23/30 (seed=122) ===\n",
      "\n",
      "=== Running replicate 24/30 (seed=123) ===\n",
      "\n",
      "=== Running replicate 25/30 (seed=124) ===\n",
      "\n",
      "=== Running replicate 26/30 (seed=125) ===\n",
      "\n",
      "=== Running replicate 27/30 (seed=126) ===\n",
      "\n",
      "=== Running replicate 28/30 (seed=127) ===\n",
      "\n",
      "=== Running replicate 29/30 (seed=128) ===\n",
      "\n",
      "=== Running replicate 30/30 (seed=129) ===\n",
      "\n",
      "=== ALGORITHM 2 (FULL MATCH) - AGGREGATED RESULTS ===\n",
      "              Method           Cost Precision (%)    Recall (%) Avg Treat Time\n",
      "  Constant Threshold 375.30 ± 51.29 56.94 ± 11.43 100.00 ± 0.00    4.31 ± 1.22\n",
      "Dynamic Threshold-DP 235.03 ± 51.22  93.23 ± 5.29 100.00 ± 0.00    8.94 ± 1.19\n",
      " Dynamic Threshold-R 320.23 ± 37.22  60.81 ± 9.11 100.00 ± 0.00    7.55 ± 1.35\n",
      "    Linear Threshold 495.27 ± 49.49  40.30 ± 6.61 100.00 ± 0.00    2.22 ± 0.51\n",
      "       Wait Till End 512.67 ± 92.51  96.72 ± 3.39  98.15 ± 2.28   20.00 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALGORITHM 2 (FULL MATCH) + MULTI-REPLICATE AGGREGATION\n",
    "\n",
    "We replicate the same structure as your simpler \"Algorithm 0\"\n",
    "by running multiple replicates (each with a different random seed),\n",
    "then aggregating the results (mean ± std).\n",
    "\n",
    "Requirements:\n",
    "  pip install numpy pandas scikit-learn catboost\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Sklearn models, metrics, etc.\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "###############################################################################\n",
    "# GLOBAL PARAMETERS\n",
    "###############################################################################\n",
    "FP_COST = 10    # False positive cost\n",
    "FN_COST = 50    # False negative cost\n",
    "D_COST  = 1     # Delay cost per time\n",
    "T_MAX   = 21    # maximum discrete time steps (0..T_MAX-1)\n",
    "\n",
    "# We'll interpret mu as the discount factor gamma\n",
    "MU_CANDIDATES = [0.95, 0.99]\n",
    "\n",
    "# Example hyperparameter grids\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "GB_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "CATBOOST_PARAM_GRID = {\n",
    "    'iterations': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3, 5]\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# HELPER FUNCTIONS \n",
    "###############################################################################\n",
    "def split_into_nplus1_groups(df, n=4, seed=0):\n",
    "    \"\"\"Shuffle patient IDs and split ~evenly into (n+1) groups: G1..G_{n+1}.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    N = len(unique_pids)\n",
    "    group_size = int(np.ceil(N/(n+1)))\n",
    "    \n",
    "    groups = []\n",
    "    start_idx = 0\n",
    "    for i in range(n+1):\n",
    "        end_idx = min(start_idx+group_size, N)\n",
    "        group_pids = unique_pids[start_idx:end_idx]\n",
    "        group_df   = df[df['patient_id'].isin(group_pids)].copy()\n",
    "        groups.append(group_df)\n",
    "        start_idx = end_idx\n",
    "    return groups\n",
    "\n",
    "def compute_auc_score(y_true, y_prob):\n",
    "    \"\"\"Compute AUC safely. If only one class, return 0.5 to avoid errors.\"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df must contain columns: patient_id, time, label, risk_score\n",
    "    policy_func(patient_rows) -> treat_time (int) or None\n",
    "    \n",
    "    Returns dict of: cost, precision, recall, avg_treatment_time\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pid, grp in df.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        label = grp['label'].iloc[0]\n",
    "        treat_time = policy_func(grp)\n",
    "        \n",
    "        if treat_time is None:  # never treat\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp   = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp   = 0\n",
    "            fp = 0\n",
    "            treat_flag = 0\n",
    "            ttime = None\n",
    "        else:\n",
    "            treat_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            ttime = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treat_flag,\n",
    "            'treat_time': ttime,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated']==1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    \n",
    "    precision = 0.0\n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    \n",
    "    sick_df = df_res[df_res['label']==1]\n",
    "    total_sick = len(sick_df)\n",
    "    recall = 0.0\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    \n",
    "    avg_tt = 0.0\n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        if len(valid_tt) > 0:\n",
    "            avg_tt = valid_tt.mean()\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# THRESHOLD-based Policies\n",
    "###############################################################################\n",
    "def make_constant_threshold_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    for thr in thresholds:\n",
    "        policy = make_constant_threshold_policy(thr)\n",
    "        stats = simulate_policy(df, policy)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec) and row['risk_score'] >= thr_vec[t]:\n",
    "                return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=20,\n",
    "                                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        policy  = make_dynamic_threshold_policy(thr_vec)\n",
    "        stats   = simulate_policy(df, policy)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A, B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A*t + B\n",
    "            thr = np.clip(thr, 0, 1)\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def linear_threshold_search(df,\n",
    "                            A_candidates=np.linspace(-0.05, 0.01, 7),\n",
    "                            B_candidates=np.linspace(0,0.6,4)):\n",
    "    best_A, best_B = None, None\n",
    "    best_cost = float('inf')\n",
    "    best_stats= None\n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            policy = make_linear_threshold_policy(A, B)\n",
    "            stats  = simulate_policy(df, policy)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    return (best_A, best_B), best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_t = patient_rows['time'].max()\n",
    "        final_row = patient_rows[patient_rows['time'] == final_t].iloc[0]\n",
    "        if final_row['risk_score'] >= thr:\n",
    "            return int(final_t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    for thr in thresholds:\n",
    "        policy = make_wait_till_end_policy(thr)\n",
    "        stats  = simulate_policy(df, policy)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "###############################################################################\n",
    "# DP-related helpers\n",
    "###############################################################################\n",
    "def to_bucket(prob):\n",
    "    \"\"\"Map a probability into 5 discrete buckets [0..4].\"\"\"\n",
    "    b = int(prob * 5)\n",
    "    return min(b, 4)\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    \"\"\"\n",
    "    Estimate p_trans[t,b,b_next] and p_sick[t,b].\n",
    "    df_train must have columns: 'patient_id','time','risk_bucket','label'\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows = grp.to_dict('records')\n",
    "        for i, row in enumerate(rows):\n",
    "            t   = int(row['time'])\n",
    "            b   = int(row['risk_bucket'])\n",
    "            lbl = row['label']\n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            if i < len(rows)-1:\n",
    "                nxt = rows[i+1]\n",
    "                t_next = nxt['time']\n",
    "                b_next = nxt['risk_bucket']\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t,b,b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_,b_,:].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_,b_,:] = transition_counts[t_,b_,:] / denom\n",
    "            else:\n",
    "                p_trans[t_,b_,b_] = 1.0  # if no data, assume self-transition\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_,b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_,b_] = sick_counts[t_,b_] / denom\n",
    "            else:\n",
    "                p_sick[t_,b_] = 0.0\n",
    "    \n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                       FP=10, FN=50, D=1, gamma=0.99, T=20):\n",
    "    \"\"\"\n",
    "    Standard unconstrained DP for each bucket b at each time t:\n",
    "      V[t,b] = min( cost_treat_now, cost_wait )\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V  = np.zeros((T+1, n_buckets))\n",
    "    pi = np.zeros((T,   n_buckets), dtype=int)\n",
    "    \n",
    "    # boundary at t = T\n",
    "    for b in range(n_buckets):\n",
    "        # if we \"treat\" at the last time (T-1):\n",
    "        cost_treat   = p_sick[T-1,b] * (D*(T-1)) + (1 - p_sick[T-1,b])*FP\n",
    "        # if we \"do not treat\" at all:\n",
    "        cost_notreat = p_sick[T-1,b] * FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    # fill from t = T-1 down to 0\n",
    "    for t_ in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            cost_treat = p_sick[t_,b]*(D*t_) + (1 - p_sick[t_,b])*FP\n",
    "            # cost_wait:\n",
    "            if t_ == T-1:\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t_,b,b_next] * V[t_+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t_,b] = cost_treat\n",
    "                pi[t_,b] = 1  # treat now\n",
    "            else:\n",
    "                V[t_,b] = cost_wait\n",
    "                pi[t_,b] = 0  # wait\n",
    "    return V, pi\n",
    "\n",
    "def make_dp_policy(V, pi_, T=20):\n",
    "    \"\"\"\n",
    "    Return a policy function that treats at time t if pi_[t,bucket] == 1\n",
    "    \"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# ALGORITHM 2 (FULL MATCH) FUNCTION\n",
    "###############################################################################\n",
    "def run_algorithm2_full_match(df_all, n=4, seed=0):\n",
    "    \"\"\"\n",
    "    Attempts to implement Algorithm 2 from your snippet exactly,\n",
    "    then produce a final table (single run).\n",
    "    Returns: final_table (DataFrame with 5 rows, one per method).\n",
    "    \"\"\"\n",
    "    # Keep only time < T_MAX\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # 1) Split\n",
    "    groups = split_into_nplus1_groups(df_all, n=n, seed=seed)\n",
    "    G_cv   = groups[:-1] # G1..Gn\n",
    "    G_test = groups[-1]  # G_{n+1}\n",
    "    \n",
    "    # Build \"lambda_candidates\"\n",
    "    lambda_candidates = []\n",
    "    for p in ParameterGrid(RF_PARAM_GRID):\n",
    "        param_tuple = tuple(sorted(p.items()))\n",
    "        lambda_candidates.append(('rf', param_tuple))\n",
    "    for p in ParameterGrid(GB_PARAM_GRID):\n",
    "        param_tuple = tuple(sorted(p.items()))\n",
    "        lambda_candidates.append(('gb', param_tuple))\n",
    "    for p in ParameterGrid(CATBOOST_PARAM_GRID):\n",
    "        param_tuple = tuple(sorted(p.items()))\n",
    "        lambda_candidates.append(('cat', param_tuple))\n",
    "    \n",
    "    def dict_from_param_tuple(param_tuple):\n",
    "        \"\"\"Convert tuple of (key,value) pairs back to dict.\"\"\"\n",
    "        return dict(param_tuple)\n",
    "    \n",
    "    # Helper function to train + compute \"AUCCost = 1 - AUC\"\n",
    "    def compute_AUCCost(lambda_cand, train_df, val_df):\n",
    "        \"\"\"\n",
    "        lambda_cand = (model_type, param_tuple)\n",
    "        train model on train_df, compute AUC on val_df => cost = 1 - AUC\n",
    "        \"\"\"\n",
    "        model_type, param_tuple = lambda_cand\n",
    "        param_dict = dict_from_param_tuple(param_tuple)\n",
    "        \n",
    "        X_train = train_df[['EIT','NIRS','EIS']].values\n",
    "        y_train = train_df['label'].values\n",
    "        X_val   = val_df[['EIT','NIRS','EIS']].values\n",
    "        y_val   = val_df['label'].values\n",
    "        \n",
    "        if model_type == 'rf':\n",
    "            mdl = RandomForestClassifier(random_state=0, **param_dict)\n",
    "        elif model_type == 'gb':\n",
    "            mdl = GradientBoostingClassifier(random_state=0, **param_dict)\n",
    "        else:\n",
    "            mdl = CatBoostClassifier(verbose=0, random_state=0, **param_dict)\n",
    "        \n",
    "        mdl.fit(X_train, y_train)\n",
    "        prob_val = mdl.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, prob_val)\n",
    "        return 1.0 - auc_val  # \"AUCCost\"\n",
    "    \n",
    "    # Helper to train final model + build DP => \"ActualCost\"\n",
    "    def compute_ActualCost(lambda_cand, mu, train_df, val_df):\n",
    "        \"\"\"\n",
    "        mu = discount factor\n",
    "        1) train model on train_df\n",
    "        2) get risk scores on val_df\n",
    "        3) make DP policy with discount=mu\n",
    "        4) simulate => cost\n",
    "        \"\"\"\n",
    "        model_type, param_tuple = lambda_cand\n",
    "        param_dict = dict_from_param_tuple(param_tuple)\n",
    "        \n",
    "        X_train = train_df[['EIT','NIRS','EIS']].values\n",
    "        y_train = train_df['label'].values\n",
    "        \n",
    "        if model_type == 'rf':\n",
    "            mdl = RandomForestClassifier(random_state=0, **param_dict)\n",
    "        elif model_type == 'gb':\n",
    "            mdl = GradientBoostingClassifier(random_state=0, **param_dict)\n",
    "        else:\n",
    "            mdl = CatBoostClassifier(verbose=0, random_state=0, **param_dict)\n",
    "        mdl.fit(X_train, y_train)\n",
    "        \n",
    "        # Risk scores\n",
    "        val_df_ = val_df.copy()\n",
    "        X_val   = val_df_[['EIT','NIRS','EIS']].values\n",
    "        prob_val= mdl.predict_proba(X_val)[:,1]\n",
    "        val_df_['risk_score'] = prob_val\n",
    "        val_df_['risk_bucket']= val_df_['risk_score'].apply(to_bucket)\n",
    "        \n",
    "        # Build DP from train\n",
    "        train_df_ = train_df.copy()\n",
    "        prob_tr   = mdl.predict_proba(train_df_[['EIT','NIRS','EIS']].values)[:,1]\n",
    "        train_df_['risk_score']  = prob_tr\n",
    "        train_df_['risk_bucket'] = train_df_['risk_score'].apply(to_bucket)\n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(train_df_,\n",
    "                                                             T=T_MAX,\n",
    "                                                             n_buckets=5)\n",
    "        V, pi_ = train_data_driven_dp_unconstrained(\n",
    "            p_trans, p_sick,\n",
    "            FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "            gamma=mu, T=T_MAX\n",
    "        )\n",
    "        dp_policy = make_dp_policy(V, pi_, T=T_MAX)\n",
    "        \n",
    "        stats = simulate_policy(val_df_, dp_policy)\n",
    "        return stats['cost']\n",
    "    \n",
    "    nfolds = len(G_cv)  # = n\n",
    "    G_all_cv = pd.concat(G_cv, ignore_index=True)  # union G1..Gn\n",
    "    \n",
    "    ###################################################################\n",
    "    # 2) Outer loop j = 1..n\n",
    "    ###################################################################\n",
    "    best_lambda_for_j = [None]*n\n",
    "    cost_j_mu = {j: {} for j in range(n)}\n",
    "    \n",
    "    for j in range(n):\n",
    "        G_outer_val = G_cv[j]\n",
    "        train_list = [G_cv[m] for m in range(n) if m != j]\n",
    "        G_outer_train = pd.concat(train_list, ignore_index=True)\n",
    "        \n",
    "        # (a) Inner cross among i != j\n",
    "        sum_auccost = {lam:0.0 for lam in lambda_candidates}\n",
    "        i_indices = [x for x in range(n) if x != j]\n",
    "        \n",
    "        for i_ in i_indices:\n",
    "            G_inner_val = G_cv[i_]\n",
    "            inner_train_list = [G_cv[m] for m in range(n) if (m != j) and (m != i_)]\n",
    "            G_inner_train = pd.concat(inner_train_list, ignore_index=True)\n",
    "            \n",
    "            for lam in lambda_candidates:\n",
    "                cost_ij = compute_AUCCost(lam, G_inner_train, G_inner_val)\n",
    "                sum_auccost[lam] += cost_ij\n",
    "        \n",
    "        # pick lambda_j^*\n",
    "        best_lam_j = None\n",
    "        best_val = float('inf')\n",
    "        for lam in lambda_candidates:\n",
    "            if sum_auccost[lam] < best_val:\n",
    "                best_val = sum_auccost[lam]\n",
    "                best_lam_j = lam\n",
    "        best_lambda_for_j[j] = best_lam_j\n",
    "        \n",
    "        # (b) Retrain on G_outer_train with lambda_j^*, measure ActualCost for each mu\n",
    "        for mu_ in MU_CANDIDATES:\n",
    "            c_j_mu = compute_ActualCost(best_lam_j, mu_, G_outer_train, G_outer_val)\n",
    "            cost_j_mu[j][mu_] = c_j_mu\n",
    "    \n",
    "    # (c) pick final mu^* by summing cost_j_mu across j\n",
    "    mu_star = None\n",
    "    best_sum_cost = float('inf')\n",
    "    for mu_ in MU_CANDIDATES:\n",
    "        sum_c = 0.0\n",
    "        for j in range(n):\n",
    "            sum_c += cost_j_mu[j][mu_]\n",
    "        if sum_c < best_sum_cost:\n",
    "            best_sum_cost = sum_c\n",
    "            mu_star = mu_\n",
    "    \n",
    "    ###################################################################\n",
    "    # 4) \"Full cross\" pass #1: pick final lambda^*\n",
    "    ###################################################################\n",
    "    sum_auccost_all_i = {lam:0.0 for lam in lambda_candidates}\n",
    "    for i_ in range(n):\n",
    "        G_val_i = G_cv[i_]\n",
    "        G_train_i = pd.concat([G_cv[k] for k in range(n) if k != i_], ignore_index=True)\n",
    "        for lam in lambda_candidates:\n",
    "            c_auccost = compute_AUCCost(lam, G_train_i, G_val_i)\n",
    "            sum_auccost_all_i[lam] += c_auccost\n",
    "    \n",
    "    lambda_star = None\n",
    "    best_val2 = float('inf')\n",
    "    for lam in lambda_candidates:\n",
    "        if sum_auccost_all_i[lam] < best_val2:\n",
    "            best_val2 = sum_auccost_all_i[lam]\n",
    "            lambda_star = lam\n",
    "    \n",
    "    ###################################################################\n",
    "    # 5) \"Full cross\" pass #2: pick final mu^*\n",
    "    #    (we already have mu_star from step (c),\n",
    "    #     but let's replicate the snippet logic)\n",
    "    ###################################################################\n",
    "    cost_i_mu_2 = {mu_: 0.0 for mu_ in MU_CANDIDATES}\n",
    "    \n",
    "    for i_ in range(n):\n",
    "        G_val_i = G_cv[i_]\n",
    "        G_train_i = pd.concat([G_cv[k] for k in range(n) if k != i_], ignore_index=True)\n",
    "        for mu_ in MU_CANDIDATES:\n",
    "            c_i_mu = compute_ActualCost(lambda_star, mu_, G_train_i, G_val_i)\n",
    "            cost_i_mu_2[mu_] += c_i_mu\n",
    "    \n",
    "    mu_star_final = None\n",
    "    best_cost_2 = float('inf')\n",
    "    for mu_ in MU_CANDIDATES:\n",
    "        if cost_i_mu_2[mu_] < best_cost_2:\n",
    "            best_cost_2 = cost_i_mu_2[mu_]\n",
    "            mu_star_final = mu_\n",
    "    \n",
    "    final_lambda = lambda_star\n",
    "    final_mu     = mu_star_final\n",
    "    \n",
    "    ###################################################################\n",
    "    # 6) Retrain final model on G1..Gn with final_lambda,\n",
    "    #    build DP with final_mu, evaluate on G_{n+1}.\n",
    "    ###################################################################\n",
    "    G_cv_concat = pd.concat(G_cv, ignore_index=True)\n",
    "    \n",
    "    model_type_final, param_tuple_final = final_lambda\n",
    "    param_dict_final = dict_from_param_tuple(param_tuple_final)\n",
    "    if model_type_final == 'rf':\n",
    "        final_mdl = RandomForestClassifier(random_state=0, **param_dict_final)\n",
    "    elif model_type_final == 'gb':\n",
    "        final_mdl = GradientBoostingClassifier(random_state=0, **param_dict_final)\n",
    "    else:\n",
    "        final_mdl = CatBoostClassifier(verbose=0, random_state=0, **param_dict_final)\n",
    "    \n",
    "    X_cv_all = G_cv_concat[['EIT','NIRS','EIS']].values\n",
    "    y_cv_all = G_cv_concat['label'].values\n",
    "    final_mdl.fit(X_cv_all, y_cv_all)\n",
    "    \n",
    "    # Build final DP policy\n",
    "    df_dp_train = G_cv_concat.copy()\n",
    "    prob_dp_tr  = final_mdl.predict_proba(df_dp_train[['EIT','NIRS','EIS']].values)[:,1]\n",
    "    df_dp_train['risk_score']  = prob_dp_tr\n",
    "    df_dp_train['risk_bucket'] = df_dp_train['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    p_trans_final, p_sick_final = estimate_transition_and_sick_probs(df_dp_train,\n",
    "                                                                     T=T_MAX,\n",
    "                                                                     n_buckets=5)\n",
    "    V_final, pi_final = train_data_driven_dp_unconstrained(\n",
    "        p_trans_final, p_sick_final,\n",
    "        FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "        gamma=final_mu, T=T_MAX\n",
    "    )\n",
    "    dp_policy_final = make_dp_policy(V_final, pi_final, T=T_MAX)\n",
    "    \n",
    "    # Evaluate on G_{n+1}\n",
    "    G_test_eval = G_test.copy()\n",
    "    prob_test   = final_mdl.predict_proba(G_test_eval[['EIT','NIRS','EIS']].values)[:,1]\n",
    "    G_test_eval['risk_score'] = prob_test\n",
    "    G_test_eval['risk_bucket'] = G_test_eval['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    stats_dp = simulate_policy(G_test_eval, dp_policy_final)\n",
    "    \n",
    "    ###################################################################\n",
    "    # OPTIONAL: Evaluate threshold-based policies on the same G_{n+1}\n",
    "    ###################################################################\n",
    "    # We'll do the threshold tuning on the entire G_cv_concat, \n",
    "    # then evaluate on G_test_eval for consistent comparison.\n",
    "    \n",
    "    prob_cv_final = final_mdl.predict_proba(G_cv_concat[['EIT','NIRS','EIS']].values)[:,1]\n",
    "    G_cv_concat['risk_score'] = prob_cv_final\n",
    "    \n",
    "    # 1) Constant threshold\n",
    "    best_thr_const, _ = constant_threshold_search(G_cv_concat)\n",
    "    policy_const = make_constant_threshold_policy(best_thr_const)\n",
    "    stats_const  = simulate_policy(G_test_eval, policy_const)\n",
    "    \n",
    "    # 2) Dynamic threshold (random search)\n",
    "    best_thr_vec, _ = dynamic_threshold_random_search(G_cv_concat,\n",
    "                    time_steps=T_MAX-1,\n",
    "                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                    n_samples=200, seed=123)\n",
    "    policy_dyn = make_dynamic_threshold_policy(best_thr_vec)\n",
    "    stats_dyn  = simulate_policy(G_test_eval, policy_dyn)\n",
    "    \n",
    "    # 3) Linear threshold\n",
    "    (A_lin, B_lin), _ = linear_threshold_search(G_cv_concat)\n",
    "    policy_lin = make_linear_threshold_policy(A_lin, B_lin)\n",
    "    stats_lin  = simulate_policy(G_test_eval, policy_lin)\n",
    "    \n",
    "    # 4) Wait till end\n",
    "    best_thr_wte, _ = wait_till_end_search(G_cv_concat)\n",
    "    policy_wte = make_wait_till_end_policy(best_thr_wte)\n",
    "    stats_wte  = simulate_policy(G_test_eval, policy_wte)\n",
    "    \n",
    "    # Build final table\n",
    "    final_table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            'Dynamic Threshold-DP'\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Avg Treat Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Print out the final choices (optional)\n",
    "    # print(\"=== Final chosen hyperparams (lambda^*) ===\", final_lambda)\n",
    "    # print(\"=== Final chosen discount factor (mu^*) ===\", final_mu)\n",
    "    \n",
    "    return final_table\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# MAIN: RUN MULTIPLE REPLICATES AND AGGREGATE\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Number of replicates to run\n",
    "    NUM_REPLICATES = 30  # You can adjust this as needed\n",
    "\n",
    "    # Read data\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    # Filter to time < T_MAX (if needed)\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "\n",
    "    # Check required columns\n",
    "    required = {'patient_id','time','EIT','NIRS','EIS','label'}\n",
    "    if not required.issubset(df_all.columns):\n",
    "        raise ValueError(\n",
    "            f\"Your CSV must have columns at least: {required}. Found: {df_all.columns}\"\n",
    "        )\n",
    "\n",
    "    all_tables = []\n",
    "    for rep in range(NUM_REPLICATES):\n",
    "        seed = 100 + rep  # or any other scheme\n",
    "        print(f\"\\n=== Running replicate {rep+1}/{NUM_REPLICATES} (seed={seed}) ===\")\n",
    "        \n",
    "        final_table = run_algorithm2_full_match(df_all, n=4, seed=seed)\n",
    "        all_tables.append(final_table)\n",
    "\n",
    "    # Now aggregate results across replicates\n",
    "    combined_df = pd.concat(all_tables, ignore_index=True)\n",
    "    grouped = combined_df.groupby('Method')\n",
    "\n",
    "    final_rows = []\n",
    "    for method, group_data in grouped:\n",
    "        cost_mean = group_data['Cost'].mean()\n",
    "        cost_std  = group_data['Cost'].std()\n",
    "\n",
    "        prec_mean = group_data['Precision (%)'].mean()\n",
    "        prec_std  = group_data['Precision (%)'].std()\n",
    "\n",
    "        rec_mean  = group_data['Recall (%)'].mean()\n",
    "        rec_std   = group_data['Recall (%)'].std()\n",
    "\n",
    "        time_mean = group_data['Avg Treat Time'].mean()\n",
    "        time_std  = group_data['Avg Treat Time'].std()\n",
    "\n",
    "        final_rows.append({\n",
    "            'Method': method,\n",
    "            'Cost': f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "            'Precision (%)': f\"{prec_mean:.2f} ± {prec_std:.2f}\",\n",
    "            'Recall (%)': f\"{rec_mean:.2f} ± {rec_std:.2f}\",\n",
    "            'Avg Treat Time': f\"{time_mean:.2f} ± {time_std:.2f}\"\n",
    "        })\n",
    "\n",
    "    final_df = pd.DataFrame(final_rows)\n",
    "    print(\"\\n=== ALGORITHM 2 (FULL MATCH) - AGGREGATED RESULTS ===\")\n",
    "    print(final_df.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89a48f-3269-4c1d-9c39-3bb9462e2752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
