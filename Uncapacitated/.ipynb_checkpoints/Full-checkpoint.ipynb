{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9598c95-6f0a-4828-a937-cd81d1e9aa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running replication 1/30 with seed=0\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 5, 'n_estimators': 100}, AUC=0.8552\n",
      "Best DP gamma on G3 = 0.99, cost=470.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 682\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28mprint\u001b[39m(final_results\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[8], line 676\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour CSV must have columns at least: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_all\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Run Algorithm 0 multiple times\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m final_results \u001b[38;5;241m=\u001b[39m run_multiple_replications(df_all, n_replications\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== FINAL RESULTS (Mean ± Std Dev over 30 Replications, Test on G4) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_results\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[8], line 621\u001b[0m, in \u001b[0;36mrun_multiple_replications\u001b[0;34m(df_all, n_replications)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning replication \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_replications\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with seed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# Run algorithm with current seed\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m table \u001b[38;5;241m=\u001b[39m run_algorithm0_unconstrained(df_all, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# Extract values for each method\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39miterrows():\n",
      "Cell \u001b[0;32mIn[8], line 538\u001b[0m, in \u001b[0;36mrun_algorithm0_unconstrained\u001b[0;34m(df_all, seed)\u001b[0m\n\u001b[1;32m    535\u001b[0m G4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m prob_4\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# (A) threshold-based policies\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m thr_const, stats_const \u001b[38;5;241m=\u001b[39m constant_threshold_search(G4)\n\u001b[1;32m    539\u001b[0m thr_vec, stats_dyn     \u001b[38;5;241m=\u001b[39m dynamic_threshold_random_search(G4, time_steps\u001b[38;5;241m=\u001b[39mT_MAX)\n\u001b[1;32m    540\u001b[0m (A_lin,B_lin), stats_lin \u001b[38;5;241m=\u001b[39m linear_threshold_search(G4)\n",
      "Cell \u001b[0;32mIn[8], line 227\u001b[0m, in \u001b[0;36mconstant_threshold_search\u001b[0;34m(df, thresholds)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m stats \u001b[38;5;241m=\u001b[39m simulate_policy(df, policy_func)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m best_cost:\n\u001b[1;32m    229\u001b[0m     best_cost \u001b[38;5;241m=\u001b[39m stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[8], line 141\u001b[0m, in \u001b[0;36msimulate_policy\u001b[0;34m(df, policy_func)\u001b[0m\n\u001b[1;32m    138\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid, grp \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 141\u001b[0m     grp \u001b[38;5;241m=\u001b[39m grp\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    142\u001b[0m     label \u001b[38;5;241m=\u001b[39m grp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    144\u001b[0m     treat_time \u001b[38;5;241m=\u001b[39m policy_func(grp)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:7189\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7183\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m lexsort_indexer(\n\u001b[1;32m   7184\u001b[0m         keys, orders\u001b[38;5;241m=\u001b[39mascending, na_position\u001b[38;5;241m=\u001b[39mna_position, key\u001b[38;5;241m=\u001b[39mkey\n\u001b[1;32m   7185\u001b[0m     )\n\u001b[1;32m   7186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(by):\n\u001b[1;32m   7187\u001b[0m     \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[0;32m-> 7189\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_label_or_level_values(by[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   7191\u001b[0m     \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[1;32m   7192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   7193\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m   7194\u001b[0m         \u001b[38;5;66;03m# \"Series\", variable has type \"ndarray\")\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:1907\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_label_reference(key, axis\u001b[38;5;241m=\u001b[39maxis):\n\u001b[1;32m   1906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_label_or_level_ambiguity(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m-> 1907\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxs(key, axis\u001b[38;5;241m=\u001b[39mother_axes[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_level_reference(key, axis\u001b[38;5;241m=\u001b[39maxis):\n\u001b[1;32m   1909\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4287\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_level:\n\u001b[0;32m-> 4287\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m   4288\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4078\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4071\u001b[0m \u001b[38;5;66;03m# GH#45316 Return view if key is not duplicated\u001b[39;00m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;66;03m# Only use drop_duplicates with duplicates for performance\u001b[39;00m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   4074\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique\n\u001b[1;32m   4075\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4076\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdrop_duplicates(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   4077\u001b[0m ):\n\u001b[0;32m-> 4078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(key)\n\u001b[1;32m   4080\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   4081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4639\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4635\u001b[0m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m   4638\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(item)\n\u001b[0;32m-> 4639\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(loc, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4641\u001b[0m     cache[item] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   4643\u001b[0m     \u001b[38;5;66;03m# for a chain\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4010\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   4006\u001b[0m \u001b[38;5;66;03m# icol\u001b[39;00m\n\u001b[1;32m   4007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4008\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[0;32m-> 4010\u001b[0m     col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39miget(i)\n\u001b[1;32m   4011\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_box_col_values(col_mgr, i)\n\u001b[1;32m   4013\u001b[0m     \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1017\u001b[0m, in \u001b[0;36mBlockManager.iget\u001b[0;34m(self, i, track_ref)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;124;03mReturn the data as a SingleBlockManager.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos[i]]\n\u001b[0;32m-> 1017\u001b[0m values \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39miget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblklocs[i])\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;66;03m# shortcut for select a single-dim from a 2-dim BM\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(values)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:196\u001b[0m, in \u001b[0;36mBaseBlockManager.blklocs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rebuild_blknos_and_blklocs()\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blknos\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblklocs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]:\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    See blknos.__doc__\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# Note: these can be altered by other BlockManager methods.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#algorithm 0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Sklearn models, metrics, etc.\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS\n",
    "###############################################################################\n",
    "FP_COST = 10\n",
    "FN_COST = 50\n",
    "D_COST  = 1\n",
    "T_MAX   = 21   # maximum discrete time steps (0..T_MAX-1)\n",
    "GAMMA_CANDIDATES = [0.95, 0.99]  # Example DP discount factors to try\n",
    "\n",
    "# For demonstration, we'll use a small hyperparameter grid for each ML model.\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "GB_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "CATBOOST_PARAM_GRID = {\n",
    "    'iterations': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3, 5]\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def split_into_four_groups(df, seed=0):\n",
    "    \"\"\"\n",
    "    Shuffle patient IDs and split ~evenly into four groups: G1, G2, G3, G4.\n",
    "    Used for Algorithm 0 (Standard Validation).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    \n",
    "    n = len(unique_pids)\n",
    "    i1 = int(0.25 * n)\n",
    "    i2 = int(0.50 * n)\n",
    "    i3 = int(0.75 * n)\n",
    "    \n",
    "    G1_pids = unique_pids[: i1]\n",
    "    G2_pids = unique_pids[i1 : i2]\n",
    "    G3_pids = unique_pids[i2 : i3]\n",
    "    G4_pids = unique_pids[i3 : ]\n",
    "    \n",
    "    G1 = df[df['patient_id'].isin(G1_pids)].copy()\n",
    "    G2 = df[df['patient_id'].isin(G2_pids)].copy()\n",
    "    G3 = df[df['patient_id'].isin(G3_pids)].copy()\n",
    "    G4 = df[df['patient_id'].isin(G4_pids)].copy()\n",
    "    \n",
    "    return G1, G2, G3, G4\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "def compute_auc_score(y_true, y_prob):\n",
    "    \"\"\"Compute AUC safely. If only one class, return 0.5.\"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "def train_and_select_best_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Trains multiple models (RandomForest, GB, CatBoost)\n",
    "    over small hyperparam grids, picks best by AUC.\n",
    "    \n",
    "    Returns: (best_model, best_auc, best_model_name)\n",
    "    \"\"\"\n",
    "    best_auc = -1.0\n",
    "    best_model = None\n",
    "    best_name  = None\n",
    "    \n",
    "    # 1) RandomForest\n",
    "    for params in ParameterGrid(RF_PARAM_GRID):\n",
    "        rf = RandomForestClassifier(random_state=0, **params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        val_prob = rf.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = rf\n",
    "            best_name  = f\"RandomForest_{params}\"\n",
    "    \n",
    "    # 2) GradientBoosting\n",
    "    for params in ParameterGrid(GB_PARAM_GRID):\n",
    "        gb = GradientBoostingClassifier(random_state=0, **params)\n",
    "        gb.fit(X_train, y_train)\n",
    "        val_prob = gb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = gb\n",
    "            best_name  = f\"GradientBoosting_{params}\"\n",
    "    \n",
    "    # 3) CatBoost\n",
    "    for params in ParameterGrid(CATBOOST_PARAM_GRID):\n",
    "        cb = CatBoostClassifier(verbose=0, random_state=0, **params)\n",
    "        cb.fit(X_train, y_train)\n",
    "        val_prob = cb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = cb\n",
    "            best_name  = f\"CatBoost_{params}\"\n",
    "    \n",
    "    return best_model, best_auc, best_name\n",
    "\n",
    "###############################################################################\n",
    "# 3. SIMULATE POLICY (Unconstrained)\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df must contain:\n",
    "      - patient_id\n",
    "      - time\n",
    "      - risk_score\n",
    "      - label (0 or 1)\n",
    "    policy_func(patient_rows) -> treat_time (int) or None\n",
    "    \n",
    "    Return dict of cost, precision, recall, avg_treatment_time\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pid, grp in df.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        label = grp['label'].iloc[0]\n",
    "        \n",
    "        treat_time = policy_func(grp)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treated\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp   = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp   = 0\n",
    "            fp = 0\n",
    "            treat_flag = 0\n",
    "            ttime = None\n",
    "        else:\n",
    "            treat_flag = 1\n",
    "            if label == 1:\n",
    "                # cost = D * treat_time\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            ttime = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treat_flag,\n",
    "            'treat_time': ttime,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res   = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated']==1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    if len(treated_df)>0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df = df_res[df_res['label']==1]\n",
    "    total_sick = len(sick_df)\n",
    "    if total_sick>0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df)>0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt   = valid_tt.mean() if len(valid_tt)>0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 4. BENCHMARK THRESHOLD-BASED POLICIES\n",
    "###############################################################################\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            # treat at first time we see risk_score >= thr\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['risk_score'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_constant_threshold_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=20,\n",
    "                                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec):\n",
    "                if row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def linear_threshold_search(df,\n",
    "                            A_candidates=np.linspace(-0.05, 0.01, 7),\n",
    "                            B_candidates=np.linspace(0,0.6,2)):\n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A*t + B\n",
    "                    thr = np.clip(thr,0,1)\n",
    "                    if row['risk_score'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            \n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    return (best_A,best_B), best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A,B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A*t + B\n",
    "            thr = np.clip(thr,0,1)\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            final_t = patient_rows['time'].max()\n",
    "            final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "            if final_row['risk_score'] >= thr:\n",
    "                return int(final_t)\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_t = patient_rows['time'].max()\n",
    "        final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "        if final_row['risk_score'] >= thr:\n",
    "            return int(final_t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. DATA-DRIVEN DP (UNCONSTRAINED)\n",
    "###############################################################################\n",
    "def to_bucket(prob):\n",
    "    \"\"\"Simple function to map prob into a 5-bucket scale [0..4].\"\"\"\n",
    "    b = int(prob * 5)\n",
    "    return min(b, 4)\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    \"\"\"\n",
    "    p_trans[t,b,b_next], p_sick[t,b]\n",
    "    df_train has columns: patient_id, time, risk_bucket, label\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows= grp.to_dict('records')\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            lbl = row['label']\n",
    "            \n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            \n",
    "            if i < len(rows)-1:\n",
    "                nxt = rows[i+1]\n",
    "                t_next = nxt['time']\n",
    "                b_next = nxt['risk_bucket']\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t,b,b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_,b_,:].sum()\n",
    "            if denom>0:\n",
    "                p_trans[t_,b_,:] = transition_counts[t_,b_,:] / denom\n",
    "            else:\n",
    "                p_trans[t_,b_,b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_,b_]\n",
    "            if denom>0:\n",
    "                p_sick[t_,b_] = sick_counts[t_,b_] / denom\n",
    "            else:\n",
    "                p_sick[t_,b_] = 0.0\n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                       FP=10, FN=50, D=1, gamma=0.99, T=20):\n",
    "    \"\"\"\n",
    "    Standard DP for unconstrained scenario:\n",
    "      V[t,b] = min( cost_treat_now, cost_wait )\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # boundary at t=T\n",
    "    for b in range(n_buckets):\n",
    "        cost_treat   = p_sick[T-1,b]*(D*(T-1)) + (1-p_sick[T-1,b])*FP\n",
    "        cost_notreat = p_sick[T-1,b]*FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1-p_sick[t,b])*FP\n",
    "            # wait\n",
    "            if t == T-1:\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next]*V[t+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    return V, pi_\n",
    "\n",
    "def make_dp_policy(V, pi_, T=20):\n",
    "    \"\"\"Return a policy function that treats if pi[t,b]==1.\"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            if t < T:\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 6. ALGORITHM 0 (STANDARD VALIDATION)\n",
    "###############################################################################\n",
    "def run_algorithm0_unconstrained(df_all, seed=0):\n",
    "    \"\"\"\n",
    "    1) Split df_all -> G1, G2, G3, G4\n",
    "    2) ML hyperparam search on (G1->G2)\n",
    "    3) Retrain best ML on G1+G2\n",
    "    4) DP hyperparam search on G3\n",
    "    5) Final evaluation on G4\n",
    "    \"\"\"\n",
    "    # Step 1: Split\n",
    "    G1, G2, G3, G4 = split_into_four_groups(df_all, seed=seed)\n",
    "    print(f\"G1: {G1['patient_id'].nunique()} patients | G2: {G2['patient_id'].nunique()} | \"\n",
    "          f\"G3: {G3['patient_id'].nunique()} | G4: {G4['patient_id'].nunique()}\")\n",
    "    \n",
    "    # Step 2: ML hyperparam search on (G1->G2)\n",
    "    X_train = G1[['EIT','NIRS','EIS']].values\n",
    "    y_train = G1['label'].values\n",
    "    \n",
    "    X_val   = G2[['EIT','NIRS','EIS']].values\n",
    "    y_val   = G2['label'].values\n",
    "    \n",
    "    best_model, best_auc, best_name = train_and_select_best_model(\n",
    "        X_train, y_train, X_val, y_val\n",
    "    )\n",
    "    print(f\"Best ML model on (G1->G2): {best_name}, AUC={best_auc:.4f}\")\n",
    "    \n",
    "    # Step 3: Retrain best ML on G1+G2\n",
    "    G12 = pd.concat([G1, G2], ignore_index=True)\n",
    "    X_12 = G12[['EIT','NIRS','EIS']].values\n",
    "    y_12 = G12['label'].values\n",
    "    # We'll do the same approach => train_and_select_best_model on itself \n",
    "    #   or just fit best_model to G1+G2 \n",
    "    #   (for simplicity, we directly refit best_model).\n",
    "    \n",
    "    best_model.fit(X_12, y_12)\n",
    "    \n",
    "    # Step 4: DP hyperparam search on G3 => produce risk scores from final ML\n",
    "    G3 = G3.copy()\n",
    "    X_3 = G3[['EIT','NIRS','EIS']].values\n",
    "    prob_3 = best_model.predict_proba(X_3)[:,1]\n",
    "    G3['risk_score'] = prob_3\n",
    "    \n",
    "    # We also need transitions from G12 => so produce risk_score for G12\n",
    "    G12 = G12.copy()\n",
    "    prob_12 = best_model.predict_proba(G12[['EIT','NIRS','EIS']])[:,1]\n",
    "    G12['risk_score'] = prob_12\n",
    "    \n",
    "    # For each candidate gamma => train DP => evaluate cost on G3 => pick best gamma\n",
    "    best_gamma = None\n",
    "    best_cost_dp = float('inf')\n",
    "    best_V = None\n",
    "    best_pi= None\n",
    "    \n",
    "    # Bucket the training data for DP\n",
    "    G12['risk_bucket'] = G12['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    for gamma_ in GAMMA_CANDIDATES:\n",
    "        # estimate transitions\n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(G12, T=T_MAX, n_buckets=5)\n",
    "        V_temp, pi_temp = train_data_driven_dp_unconstrained(\n",
    "            p_trans, p_sick, FP=FP_COST, FN=FN_COST,\n",
    "            D=D_COST, gamma=gamma_, T=T_MAX\n",
    "        )\n",
    "        # Evaluate on G3\n",
    "        #   also bucket G3\n",
    "        G3_temp = G3.copy()\n",
    "        G3_temp['risk_bucket'] = G3_temp['risk_score'].apply(to_bucket)\n",
    "        \n",
    "        dp_policy_func = make_dp_policy(V_temp, pi_temp, T=T_MAX)\n",
    "        stats_dp = simulate_policy(G3_temp, dp_policy_func)\n",
    "        \n",
    "        if stats_dp['cost'] < best_cost_dp:\n",
    "            best_cost_dp = stats_dp['cost']\n",
    "            best_gamma   = gamma_\n",
    "            best_V = V_temp\n",
    "            best_pi= pi_temp\n",
    "    \n",
    "    print(f\"Best DP gamma on G3 = {best_gamma}, cost={best_cost_dp:.2f}\")\n",
    "    \n",
    "    # Step 5: Evaluate on G4\n",
    "    G4 = G4.copy()\n",
    "    prob_4 = best_model.predict_proba(G4[['EIT','NIRS','EIS']])[:,1]\n",
    "    G4['risk_score'] = prob_4\n",
    "    \n",
    "    # (A) threshold-based policies\n",
    "    thr_const, stats_const = constant_threshold_search(G4)\n",
    "    thr_vec, stats_dyn     = dynamic_threshold_random_search(G4, time_steps=T_MAX)\n",
    "    (A_lin,B_lin), stats_lin = linear_threshold_search(G4)\n",
    "    thr_wte, stats_wte     = wait_till_end_search(G4)\n",
    "    \n",
    "    # (B) final DP policy using best_gamma => we already have best_V, best_pi\n",
    "    # re-check transitions from G12 if needed, but we already found them\n",
    "    # we just build final policy:\n",
    "    dp_policy_final = make_dp_policy(best_V, best_pi, T=T_MAX)\n",
    "    \n",
    "    G4_dp = G4.copy()\n",
    "    G4_dp['risk_bucket'] = G4_dp['risk_score'].apply(to_bucket)\n",
    "    stats_dp = simulate_policy(G4_dp, dp_policy_final)\n",
    "    \n",
    "    # Build final table\n",
    "    table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            f'Dynamic Threshold-DP (gamma={best_gamma})'\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Treatment Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return table\n",
    "\n",
    "###############################################################################\n",
    "# 7. RUN MULTIPLE REPLICATIONS\n",
    "###############################################################################\n",
    "def run_multiple_replications(df_all, n_replications=30):\n",
    "    \"\"\"\n",
    "    Run Algorithm 0 multiple times with different random seeds.\n",
    "    Compute mean and standard deviation for each metric.\n",
    "    \"\"\"\n",
    "    # Define standard method names for consistent reporting\n",
    "    standard_methods = [\n",
    "        'Constant Threshold',\n",
    "        'Dynamic Threshold-R',\n",
    "        'Linear Threshold',\n",
    "        'Wait Till End',\n",
    "        'Dynamic Threshold-DP'\n",
    "    ]\n",
    "    \n",
    "    # Initialize containers for each metric and method\n",
    "    precision_values = {method: [] for method in standard_methods}\n",
    "    cost_values = {method: [] for method in standard_methods}\n",
    "    recall_values = {method: [] for method in standard_methods}\n",
    "    treatment_time_values = {method: [] for method in standard_methods}\n",
    "    \n",
    "    for i in range(n_replications):\n",
    "        seed = i  # Use a different seed for each replication\n",
    "        print(f\"\\nRunning replication {i+1}/{n_replications} with seed={seed}\")\n",
    "        \n",
    "        # Run algorithm with current seed\n",
    "        table = run_algorithm0_unconstrained(df_all, seed=seed)\n",
    "        \n",
    "        # Extract values for each method\n",
    "        for _, row in table.iterrows():\n",
    "            method = row['Method']\n",
    "            \n",
    "            # Standardize method name (remove gamma value from DP method)\n",
    "            standard_method = method\n",
    "            if 'Dynamic Threshold-DP' in method:\n",
    "                standard_method = 'Dynamic Threshold-DP'\n",
    "            \n",
    "            if standard_method in standard_methods:\n",
    "                precision_values[standard_method].append(row['Precision (%)'])\n",
    "                cost_values[standard_method].append(row['Cost'])\n",
    "                recall_values[standard_method].append(row['Recall (%)'])\n",
    "                treatment_time_values[standard_method].append(row['Treatment Time'])\n",
    "    \n",
    "    # Compute statistics\n",
    "    final_data = []\n",
    "    for method in standard_methods:\n",
    "        if precision_values[method]:  # Check if we have data for this method\n",
    "            precision_mean = np.mean(precision_values[method])\n",
    "            precision_std = np.std(precision_values[method])\n",
    "            cost_mean = np.mean(cost_values[method])\n",
    "            cost_std = np.std(cost_values[method])\n",
    "            recall_mean = np.mean(recall_values[method])\n",
    "            recall_std = np.std(recall_values[method])\n",
    "            treat_time_mean = np.mean(treatment_time_values[method])\n",
    "            treat_time_std = np.std(treatment_time_values[method])\n",
    "            \n",
    "            final_data.append({\n",
    "                'Method': method,\n",
    "                'Precision (%)': f\"{precision_mean:.2f} ± {precision_std:.2f}\",\n",
    "                'Cost': f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "                'Recall (%)': f\"{recall_mean:.2f} ± {recall_std:.2f}\",\n",
    "                'Treatment Time': f\"{treat_time_mean:.2f} ± {treat_time_std:.2f}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(final_data)\n",
    "\n",
    "###############################################################################\n",
    "# 8. MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    \n",
    "    # If needed, filter df_all to time < T_MAX:\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # Check required columns:\n",
    "    required = {'patient_id','time','EIT','NIRS','EIS','label'}\n",
    "    if not required.issubset(df_all.columns):\n",
    "        raise ValueError(f\"Your CSV must have columns at least: {required}. Found: {df_all.columns}\")\n",
    "    \n",
    "    # Run Algorithm 0 multiple times\n",
    "    final_results = run_multiple_replications(df_all, n_replications=30)\n",
    "    \n",
    "    print(\"\\n=== FINAL RESULTS (Mean ± Std Dev over 30 Replications, Test on G4) ===\")\n",
    "    print(final_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "####algorithm 1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Sklearn models, metrics, etc.\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS\n",
    "###############################################################################\n",
    "FP_COST = 10\n",
    "FN_COST = 50\n",
    "D_COST  = 1\n",
    "GAMMA   = 0.99\n",
    "T_MAX   = 21   # maximum discrete time steps (0..T_MAX-1)\n",
    "\n",
    "\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "GB_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "CATBOOST_PARAM_GRID = {\n",
    "    'iterations': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3, 5]\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS: splitting, ML training, DP, etc.\n",
    "###############################################################################\n",
    "def make_folds(df, n_folds=5, seed=0):\n",
    "    \"\"\"\n",
    "    Semi Cross-Validation approach:\n",
    "      - We'll produce n_folds separate sets: G1, G2, ..., G_{n_folds}.\n",
    "      - We'll treat the last fold G_{n_folds} as the final holdout test.\n",
    "      - The first (n_folds - 1) folds are used in the \"semi cross-val\" loops.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # We'll shuffle patient IDs, then chunk them into n_folds groups\n",
    "    unique_pts = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pts)\n",
    "    \n",
    "    folds = []\n",
    "    fold_size = int(np.ceil(len(unique_pts) / n_folds))\n",
    "    \n",
    "    start_idx = 0\n",
    "    for k in range(n_folds):\n",
    "        end_idx = min(start_idx + fold_size, len(unique_pts))\n",
    "        fold_pids = unique_pts[start_idx:end_idx]\n",
    "        folds.append(set(fold_pids))\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    \"\"\"Returns the subset of df whose patient_id is in pid_set.\"\"\"\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "def compute_auc_score(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Safe AUC computation. If all y_true are the same class,\n",
    "    AUC is not well-defined, so we'll return 0.5 by default.\n",
    "    \"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "def train_and_select_best_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Trains multiple models (RandomForest, GradientBoosting, CatBoost)\n",
    "    over small hyperparameter grids, picks the best by AUC on (X_val,y_val).\n",
    "    \n",
    "    Returns:\n",
    "        best_model   (fitted model with best AUC)\n",
    "        best_auc     (float)\n",
    "        best_model_name (str, e.g. \"RandomForest\")\n",
    "    \"\"\"\n",
    "    best_auc = -1.0\n",
    "    best_model = None\n",
    "    best_name  = None\n",
    "    \n",
    "    # 1) RandomForest\n",
    "    for params in ParameterGrid(RF_PARAM_GRID):\n",
    "        rf = RandomForestClassifier(random_state=0, **params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        val_prob = rf.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc = auc_val\n",
    "            best_model = rf\n",
    "            best_name  = f\"RandomForest_{params}\"\n",
    "    \n",
    "    # 2) GradientBoosting\n",
    "    for params in ParameterGrid(GB_PARAM_GRID):\n",
    "        gb = GradientBoostingClassifier(random_state=0, **params)\n",
    "        gb.fit(X_train, y_train)\n",
    "        val_prob = gb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc = auc_val\n",
    "            best_model = gb\n",
    "            best_name  = f\"GradientBoosting_{params}\"\n",
    "    \n",
    "    # 3) CatBoost\n",
    "    for params in ParameterGrid(CATBOOST_PARAM_GRID):\n",
    "        # silent mode\n",
    "        cb = CatBoostClassifier(verbose=0, random_state=0, **params)\n",
    "        cb.fit(X_train, y_train, eval_set=(X_val,y_val), verbose=0)\n",
    "        val_prob = cb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc = auc_val\n",
    "            best_model = cb\n",
    "            best_name  = f\"CatBoost_{params}\"\n",
    "    \n",
    "    return best_model, best_auc, best_name\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. POLICY SIMULATION (Unconstrained) \n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    Evaluate total cost, precision, recall, avg_treatment_time under a \n",
    "    given policy_func. The policy_func is a function taking \n",
    "       policy_func(subDF_of_single_patient) -> treat_time or None\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pid, patient_rows in df.groupby('patient_id'):\n",
    "        patient_rows = patient_rows.sort_values('time')\n",
    "        \n",
    "        label = patient_rows['label'].iloc[0]  # 0 or 1\n",
    "        treat_time = policy_func(patient_rows)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treated\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp   = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp   = 0\n",
    "            fp = 0\n",
    "            treated_flag = 0\n",
    "            tt = None\n",
    "        else:\n",
    "            treated_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time  # delay cost\n",
    "                tp = 1\n",
    "                fp = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp = 0\n",
    "                fp = 1\n",
    "            tt = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treated_flag,\n",
    "            'treat_time': tt,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated'] == 1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df = df_res[df_res['label'] == 1]\n",
    "    total_sick = len(sick_df)\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt = valid_tt.mean() if len(valid_tt) > 0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 4. BENCHMARK POLICIES (Threshold-based)\n",
    "###############################################################################\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    \"\"\"Grid search over a set of constant thresholds.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0, 1, 21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['risk_score'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr = thr\n",
    "            best_stats = stats\n",
    "    \n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_constant_threshold_policy(thr):\n",
    "    \"\"\"Creates a policy that treats the patient at the first time whose risk_score >= thr.\"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=20,\n",
    "                                    threshold_candidates=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    \"\"\"\n",
    "    Randomly sample vectors of length time_steps from threshold_candidates,\n",
    "    pick the best by cost on df.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost = float('inf')\n",
    "    best_stats = None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec = thr_vec.copy()\n",
    "            best_stats = stats\n",
    "    \n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec):\n",
    "                if row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def linear_threshold_search(df, A_candidates=None, B_candidates=None):\n",
    "    \"\"\"\n",
    "    threshold(t) = clamp( A*t + B, [0,1] )\n",
    "    do grid search\n",
    "    \"\"\"\n",
    "    if A_candidates is None:\n",
    "        A_candidates = np.linspace(-0.05, 0.05, 11)\n",
    "    if B_candidates is None:\n",
    "        B_candidates = np.linspace(0, 1, 11)\n",
    "    \n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A*t + B\n",
    "                    thr = max(0, min(1, thr))\n",
    "                    if row['risk_score'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            \n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A = A\n",
    "                best_B = B\n",
    "                best_stats = stats\n",
    "    \n",
    "    return (best_A, best_B), best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A, B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A*t + B\n",
    "            thr = max(0, min(1, thr))\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    \"\"\"\n",
    "    Evaluate policy: treat only at final time if risk_score >= thr.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            # final row:\n",
    "            final_t = patient_rows['time'].max()\n",
    "            final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "            if final_row['risk_score'] >= thr:\n",
    "                return int(final_t)\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr = thr\n",
    "            best_stats = stats\n",
    "    \n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_t = patient_rows['time'].max()\n",
    "        final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "        if final_row['risk_score'] >= thr:\n",
    "            return int(final_t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 5. DATA-DRIVEN DP (Unconstrained)\n",
    "###############################################################################\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    \"\"\"\n",
    "    We'll estimate:\n",
    "      p_trans[t, b, b_next]: Probability that a patient in risk-bucket b at time t\n",
    "                             transitions to bucket b_next at time t+1\n",
    "      p_sick[t, b]: Probability that a patient is sick given that they are in bucket b at time t.\n",
    "    This is a naive aggregator (assuming Markov wrt bucket).\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "\n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    \n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows = grp.to_dict('records')\n",
    "        \n",
    "        for i in range(len(rows)):\n",
    "            t  = int(rows[i]['time'])\n",
    "            b  = int(rows[i]['risk_bucket'])\n",
    "            lb = int(rows[i]['label'])  # 0 or 1\n",
    "            if t < T:\n",
    "                bucket_counts[t, b] += 1\n",
    "                sick_counts[t, b]   += lb\n",
    "            \n",
    "            if i < len(rows) - 1:\n",
    "                # consider the next row if it's exactly t+1\n",
    "                t_next = int(rows[i+1]['time'])\n",
    "                b_next = int(rows[i+1]['risk_bucket'])\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t, b, b_next] += 1.0\n",
    "\n",
    "    # p_trans\n",
    "    n_buckets = bucket_counts.shape[1]\n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_, b_, :].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_, b_, :] = transition_counts[t_, b_, :] / denom\n",
    "            else:\n",
    "                # if no data, fallback to identity\n",
    "                p_trans[t_, b_, b_] = 1.0\n",
    "    \n",
    "    # p_sick\n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_, b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_, b_] = sick_counts[t_, b_] / denom\n",
    "            else:\n",
    "                p_sick[t_, b_] = 0.0\n",
    "    \n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                       FP=10, FN=50, D=1, gamma=0.99, T=20):\n",
    "    \"\"\"\n",
    "    We define states as (t, bucket), and actions: 0=wait, 1=treat now.\n",
    "    We'll do a simple backward recursion:\n",
    "       V[t,b] = min( cost_of_treat_now, cost_of_wait )\n",
    "    cost_of_treat_now = p_sick[t,b]* (D*t) + (1-p_sick[t,b])* FP\n",
    "    cost_of_wait      = gamma * E_{b_next}[ V[t+1, b_next] ]\n",
    "    At t=T, we define cost if not treated:\n",
    "       => p_sick[T-1,b]*FN  vs. cost_of_treat_now at T-1\n",
    "    We'll store the policy in pi_[t,b].\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    # Note: We'll define V[t,b] for t in [0..T], b in [0..n_buckets-1].\n",
    "    # But we actually only have transitions up to T-1 in p_trans.\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # boundary at t=T: if we haven't treated yet, the cost is:\n",
    "    # min( treat at T, not treat at all ).\n",
    "    # But let's define it simply as \"if not treat => FN\" or \"if treat => cost_treatNow\".\n",
    "    for b in range(n_buckets):\n",
    "        # \"treat now at time T\" => D*T?? but actually t goes up to T-1. \n",
    "        # We'll define an effective \"t = T\" as if it's the final step.\n",
    "        # so cost_treat = p_sick[T-1,b]*(D*(T-1)) + (1-p_sick[T-1,b])*FP\n",
    "        # cost_notreat  = p_sick[T-1,b]*FN\n",
    "        # We'll just do that here:\n",
    "        cost_treat  = p_sick[T-1,b]*(D*(T-1)) + (1 - p_sick[T-1,b])*FP\n",
    "        cost_notreat= p_sick[T-1,b]*FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    # now go backward:\n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # cost if treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1 - p_sick[t,b])*FP\n",
    "            \n",
    "            # cost if wait\n",
    "            if t == T-1:\n",
    "                # if wait at T-1, next is T => no transitions => V[T,b]\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                # compute expected cost from next state\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next]*V[t+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b] = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b] = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    \n",
    "    return V, pi_\n",
    "\n",
    "def make_data_driven_dp_policy_unconstrained(V, pi_, T=20):\n",
    "    \"\"\"\n",
    "    Creates a function that iterates over time steps of a patient.\n",
    "    As soon as DP says \"treat\" at (t,b), we do so and stop.\n",
    "    \"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        # naive approach: read each row in chronological order\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            if t < T:\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "        # if we never treat => None\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 6. ALGORITHM 1: SEMI CROSS-VALIDATION (Unconstrained)\n",
    "###############################################################################\n",
    "def semi_crossval_unconstrained(df_all, n_folds=5, seed=0):\n",
    "    \"\"\"\n",
    "    Implements the \"semi cross-validation\" approach for ML + DP \n",
    "    in the unconstrained scenario (Algorithm 1).\n",
    "    \n",
    "    Steps (schematic):\n",
    "      1) Create n_folds. \n",
    "         Let G_{n_folds} be final holdout. G_1..G_{n_folds-1} for \"semi-CV\".\n",
    "      2) For j in [1..(n_folds-1)]:\n",
    "          - Validation fold = G_j\n",
    "          - Training fold = union of G_k for k != j\n",
    "          - Among that \"training fold,\" we do an (n_folds-2)-fold approach \n",
    "            to select best ML hyperparams (AUC).\n",
    "            (In a simpler \"semi\" approach, we might skip an inner fold and just train ML on train set.)\n",
    "          - Evaluate DP hyperparams on G_j, store best result.\n",
    "      3) Aggregate or pick final hyperparams from these folds.\n",
    "      4) Retrain ML + DP on union G_1..G_{n_folds-1}, evaluate on G_{n_folds}.\n",
    "\n",
    "    For brevity, we do a simpler version:\n",
    "      - For each j in 0..(n_folds-2):\n",
    "         * Train ML on (all except G_j),\n",
    "         * Evaluate best threshold or best DP on G_j\n",
    "      - Then average or pick the median. \n",
    "      - Finally, evaluate on G_{n_folds-1} as holdout.\n",
    "    \"\"\"\n",
    "    folds = make_folds(df_all, n_folds=n_folds, seed=seed)\n",
    "    # final test:\n",
    "    test_fold_pid = folds[-1]\n",
    "    # the first n_folds-1 are the \"CV folds\"\n",
    "    cv_folds = folds[:-1]\n",
    "    \n",
    "    # Lists to store benchmark hyperparams found in each fold j\n",
    "    best_thr_const_list = []\n",
    "    best_dyn_vec_list   = []\n",
    "    best_linAB_list     = []\n",
    "    best_thr_wait_list  = []\n",
    "    best_dp_policies    = []\n",
    "    \n",
    "    for j, val_pid in enumerate(cv_folds):\n",
    "        # Validation fold j => G_j\n",
    "        df_val   = filter_by_group(df_all, val_pid)\n",
    "        \n",
    "        # Training = union of all other folds except j\n",
    "        train_pid = set()\n",
    "        for k, fold_pids in enumerate(cv_folds):\n",
    "            if k != j:\n",
    "                train_pid = train_pid.union(fold_pids)\n",
    "        df_train = filter_by_group(df_all, train_pid)\n",
    "        \n",
    "        # ============== (A) Train ML model on df_train => pick best by AUC on the same df_train ==============\n",
    "     \n",
    "        X_train = df_train[['EIT','NIRS','EIS']].values\n",
    "        y_train = df_train['label'].values\n",
    "        \n",
    "        # Just do a train/val = we can do a small split inside df_train, or \n",
    "        # let's do the entire df_train for training and the same df_train for ML selection \n",
    "        # (not ideal, but simpler).\n",
    "        X_val = df_train[['EIT','NIRS','EIS']].values\n",
    "        y_val = df_train['label'].values\n",
    "        \n",
    "        ml_model, best_auc, best_mname = train_and_select_best_model(X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Now, we apply this trained model to produce a risk_score for ALL ROWS in df_train+df_val\n",
    "        # So we can do threshold tuning, DP, etc.\n",
    "        # We'll store them back in the main df so we can do the policy searches.\n",
    "        # But be careful not to pollute folds with each other => for demonstration, it's simpler \n",
    "        # to do it just for df_val \"on the fly\" for cost evaluation.\n",
    "\n",
    "        # (B) Evaluate on VAL fold => get risk scores\n",
    "        X_val_fold = df_val[['EIT','NIRS','EIS']].values\n",
    "        val_probs  = ml_model.predict_proba(X_val_fold)[:,1]\n",
    "        df_val.loc[:,'risk_score'] = val_probs  # set the model-based risk\n",
    "\n",
    "        # We do the same for df_train because we need to estimate Markov transitions for the DP\n",
    "        X_train_fold = df_train[['EIT','NIRS','EIS']].values\n",
    "        train_probs  = ml_model.predict_proba(X_train_fold)[:,1]\n",
    "        df_train.loc[:,'risk_score'] = train_probs\n",
    "\n",
    "        # Also discretize into risk buckets again, e.g. 5 equally sized:\n",
    "        # We'll do a simple approach:  (0,0.2)->0, [0.2,0.4)->1, ...\n",
    "        def to_bucket(p):\n",
    "            return min(int(p*5), 4)\n",
    "        df_train.loc[:,'risk_bucket'] = df_train['risk_score'].apply(to_bucket)\n",
    "        df_val.loc[:,'risk_bucket']   = df_val['risk_score'].apply(to_bucket)\n",
    "\n",
    "        # ============= (C) Benchmark Policies on VAL fold =============\n",
    "        #  (C.1) Constant threshold\n",
    "        thr_c, _ = constant_threshold_search(df_train)  # or do it on df_train\n",
    "        best_thr_const_list.append(thr_c)\n",
    "        \n",
    "        #  (C.2) Dynamic threshold\n",
    "        thr_vec, _ = dynamic_threshold_random_search(df_train, \n",
    "                                                     time_steps=T_MAX,\n",
    "                                                     threshold_candidates=[0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                                     n_samples=200,\n",
    "                                                     seed=j)\n",
    "        best_dyn_vec_list.append(thr_vec)\n",
    "        \n",
    "        #  (C.3) Linear threshold\n",
    "        (A,B), _ = linear_threshold_search(df_train)\n",
    "        best_linAB_list.append((A,B))\n",
    "        \n",
    "        #  (C.4) Wait till end\n",
    "        thr_wte, _ = wait_till_end_search(df_train)\n",
    "        best_thr_wait_list.append(thr_wte)\n",
    "        \n",
    "        # ============= (D) DP Approach =============\n",
    "        # We'll fit the Markov chain from df_train => p_trans, p_sick\n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(df_train, T=T_MAX, n_buckets=5)\n",
    "        V, pi_ = train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                                    FP=FP_COST, FN=FN_COST, \n",
    "                                                    D=D_COST, gamma=GAMMA, T=T_MAX)\n",
    "        best_dp_policies.append((V, pi_))\n",
    "        \n",
    "    \n",
    "        \n",
    "        # End of fold j\n",
    "\n",
    "    # ----- (E) Combine or pick final hyperparams from these folds ------\n",
    "    # For demonstration, let's pick the average or the median from the sets we found:\n",
    "\n",
    "    thr_const_final = np.mean(best_thr_const_list)\n",
    "    \n",
    "    mid_idx = len(best_dyn_vec_list)//2\n",
    "    thr_dyn_final = best_dyn_vec_list[mid_idx]  # pick the \"middle\" one\n",
    "    \n",
    "    A_ave = np.mean([ab[0] for ab in best_linAB_list])\n",
    "    B_ave = np.mean([ab[1] for ab in best_linAB_list])\n",
    "    \n",
    "    thr_wait_final = np.mean(best_thr_wait_list)\n",
    "    \n",
    "    # For DP, let's pick the last fold's (V, pi_). \n",
    "    # Or we could store them all and pick the one with minimal val cost. \n",
    "    # We'll just pick the last for demonstration:\n",
    "    V_final, pi_final = best_dp_policies[-1]\n",
    "    \n",
    "    # ========== (F) Retrain ML model on all CV folds except test fold => final model ==========\n",
    "    train_pid_all = set()\n",
    "    for fold_pid in cv_folds:\n",
    "        train_pid_all = train_pid_all.union(fold_pid)\n",
    "    df_train_cv = filter_by_group(df_all, train_pid_all)\n",
    "    \n",
    "    X_train_cv = df_train_cv[['EIT','NIRS','EIS']].values\n",
    "    y_train_cv = df_train_cv['label'].values\n",
    "    \n",
    "    # We'll do the same \"train_and_select_best_model\" approach \n",
    "    # but we have no separate val set, so we'll just reuse X_train_cv for selection:\n",
    "    final_model, _, _ = train_and_select_best_model(X_train_cv, y_train_cv,\n",
    "                                                    X_train_cv, y_train_cv)\n",
    "    \n",
    "    # We'll produce final risk scores for test set G_{n_folds}.\n",
    "    df_test = filter_by_group(df_all, test_fold_pid).copy()\n",
    "    \n",
    "    X_test  = df_test[['EIT','NIRS','EIS']].values\n",
    "    test_probs = final_model.predict_proba(X_test)[:,1]\n",
    "    df_test.loc[:,'risk_score'] = test_probs\n",
    "    \n",
    "    # Re-bucket for DP or threshold logic\n",
    "    def to_bucket(p):\n",
    "        return min(int(p*5), 4)\n",
    "    df_test.loc[:,'risk_bucket'] = df_test['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    # (F.1) Build final policies from the chosen final hyperparams:\n",
    "    const_policy = make_constant_threshold_policy(thr_const_final)\n",
    "    dyn_policy   = make_dynamic_threshold_policy(thr_dyn_final)\n",
    "    lin_policy   = make_linear_threshold_policy(A_ave, B_ave)\n",
    "    wte_policy   = make_wait_till_end_policy(thr_wait_final)\n",
    "    dp_policy    = make_data_driven_dp_policy_unconstrained(V_final, pi_final, T=T_MAX)\n",
    "    \n",
    "    # (F.2) Evaluate on test set\n",
    "    stats_const = simulate_policy(df_test, const_policy)\n",
    "    stats_dyn   = simulate_policy(df_test, dyn_policy)\n",
    "    stats_lin   = simulate_policy(df_test, lin_policy)\n",
    "    stats_wte   = simulate_policy(df_test, wte_policy)\n",
    "    stats_dp    = simulate_policy(df_test, dp_policy)\n",
    "    \n",
    "    table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            'Dynamic Threshold-DP'\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Treatment Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return table\n",
    "\n",
    "###############################################################################\n",
    "# 7. RUN MULTIPLE REPLICATIONS \n",
    "###############################################################################\n",
    "def run_multiple_replications(df_all, n_replications=30, n_folds=5):\n",
    "    \"\"\"\n",
    "    Run Algorithm 1 (Semi Cross-Validation) multiple times with different random seeds.\n",
    "    Compute mean and standard deviation for each metric.\n",
    "    \"\"\"\n",
    "    # Define standard method names for consistent reporting\n",
    "    standard_methods = [\n",
    "        'Constant Threshold',\n",
    "        'Dynamic Threshold-R',\n",
    "        'Linear Threshold',\n",
    "        'Wait Till End',\n",
    "        'Dynamic Threshold-DP'\n",
    "    ]\n",
    "    \n",
    "    # Initialize containers for each metric and method\n",
    "    precision_values = {method: [] for method in standard_methods}\n",
    "    cost_values = {method: [] for method in standard_methods}\n",
    "    recall_values = {method: [] for method in standard_methods}\n",
    "    treatment_time_values = {method: [] for method in standard_methods}\n",
    "    \n",
    "    for i in range(n_replications):\n",
    "        seed = i  # Use a different seed for each replication\n",
    "        print(f\"\\nRunning replication {i+1}/{n_replications} with seed={seed}\")\n",
    "        \n",
    "        # Run algorithm with current seed\n",
    "        table = semi_crossval_unconstrained(df_all, n_folds=n_folds, seed=seed)\n",
    "        \n",
    "        # Extract values for each method\n",
    "        for _, row in table.iterrows():\n",
    "            method = row['Method']\n",
    "            \n",
    "            if method in standard_methods:\n",
    "                precision_values[method].append(row['Precision (%)'])\n",
    "                cost_values[method].append(row['Cost'])\n",
    "                recall_values[method].append(row['Recall (%)'])\n",
    "                treatment_time_values[method].append(row['Treatment Time'])\n",
    "    \n",
    "    # Compute statistics\n",
    "    final_data = []\n",
    "    for method in standard_methods:\n",
    "        if precision_values[method]:  # Check if we have data for this method\n",
    "            precision_mean = np.mean(precision_values[method])\n",
    "            precision_std = np.std(precision_values[method])\n",
    "            cost_mean = np.mean(cost_values[method])\n",
    "            cost_std = np.std(cost_values[method])\n",
    "            recall_mean = np.mean(recall_values[method])\n",
    "            recall_std = np.std(recall_values[method])\n",
    "            treat_time_mean = np.mean(treatment_time_values[method])\n",
    "            treat_time_std = np.std(treatment_time_values[method])\n",
    "            \n",
    "            final_data.append({\n",
    "                'Method': method,\n",
    "                'Precision (%)': f\"{precision_mean:.2f} ± {precision_std:.2f}\",\n",
    "                'Cost': f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "                'Recall (%)': f\"{recall_mean:.2f} ± {recall_std:.2f}\",\n",
    "                'Treatment Time': f\"{treat_time_mean:.2f} ± {treat_time_std:.2f}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(final_data)\n",
    "\n",
    "###############################################################################\n",
    "# 8. MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Load  CSV file\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # Just ensure columns exist:\n",
    "    required_cols = {'patient_id','time','risk_bucket','risk_score','EIT','NIRS','EIS','label'}\n",
    "    missing = required_cols - set(df_all.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\" CSV is missing columns: {missing}\")\n",
    "    \n",
    "    n_folds = 5\n",
    "    n_replications = 30\n",
    "    \n",
    "    final_results = run_multiple_replications(df_all, n_replications=n_replications, n_folds=n_folds)\n",
    "    \n",
    "    print(\"\\n=== FINAL RESULTS (Mean ± Std Dev over 30 Replications, Algorithm 1) ===\")\n",
    "    print(final_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "##algorithm 2\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS & SETTINGS\n",
    "###############################################################################\n",
    "FP_COST = 10    # Penalty for false positive treatment\n",
    "FN_COST = 50    # Penalty for false negative (never treated but was sick)\n",
    "D_COST  = 1     # Penalty per time-step of delay before treating a sick patient\n",
    "GAMMA   = 0.99  # Discount factor\n",
    "T_MAX   = 20    # Time horizon (discrete steps 0..T_MAX-1 for each patient)\n",
    "\n",
    "# Example features\n",
    "FEATURE_COLS = [\"time\", \"EIT\", \"NIRS\", \"EIS\"]\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS: SPLITTING & FILTERING\n",
    "###############################################################################\n",
    "def split_patients_kfold(df, n_splits=4, seed=0):\n",
    "    \"\"\"\n",
    "    Shuffle unique patient IDs, then split into (n_splits+1) groups:\n",
    "       G1,...,G_{n_splits}, G_{n_splits+1} (the final holdout).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pts = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pts)\n",
    "    \n",
    "    n = len(unique_pts)\n",
    "    splits = {}\n",
    "    \n",
    "    for i in range(n_splits + 1):\n",
    "        start_idx = int(i * n / (n_splits + 1))\n",
    "        end_idx   = int((i + 1) * n / (n_splits + 1))\n",
    "        group_name = f\"G{i+1}\"\n",
    "        splits[group_name] = set(unique_pts[start_idx:end_idx])\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    \"\"\"Return the rows of df whose patient_id is in pid_set.\"\"\"\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. ML TRAINING & RISK-SCORE PREDICTIONS\n",
    "###############################################################################\n",
    "def train_and_predict_model(model_type, hyperparams, df_train, df_val, feature_cols=FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    Train a classification model (model_type in {catboost, rf, gb})\n",
    "    with given hyperparams on df_train. Then return predicted probabilities\n",
    "    (risk scores) for df_val (aligned with df_val).\n",
    "    \"\"\"\n",
    "    X_train = df_train[feature_cols]\n",
    "    y_train = df_train['label']\n",
    "    \n",
    "    if model_type == \"catboost\":\n",
    "        model = CatBoostClassifier(**hyperparams, verbose=False)\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model_type == \"rf\":\n",
    "        model = RandomForestClassifier(**hyperparams, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model_type == \"gb\":\n",
    "        model = GradientBoostingClassifier(**hyperparams, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type={model_type}\")\n",
    "    \n",
    "    X_val = df_val[feature_cols]\n",
    "    risk_scores = model.predict_proba(X_val)[:, 1]  # Probability that label=1\n",
    "    return risk_scores\n",
    "\n",
    "def generate_risk_scores_via_cv(df_train_splits, i_val, model_list, param_grid_dict, feature_cols=FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    For cross-validation fold i_val, pick the best (model_type, hyperparams) by AUC.\n",
    "    \n",
    "    Returns:\n",
    "      best_val_scores (np.array): risk scores for df_train_splits[i_val]\n",
    "      best_model_type, best_hparams, best_auc\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    df_val = df_train_splits[i_val]\n",
    "    X_val  = df_val[feature_cols]\n",
    "    y_val  = df_val['label'].values\n",
    "    \n",
    "    # Build a single training set = union of all G_j except G_i_val\n",
    "    train_parts = []\n",
    "    for k, v_df in df_train_splits.items():\n",
    "        if k != i_val:\n",
    "            train_parts.append(v_df)\n",
    "    df_train_full = pd.concat(train_parts, ignore_index=True)\n",
    "    \n",
    "    best_model_type = None\n",
    "    best_hparams    = None\n",
    "    best_auc        = -999\n",
    "    best_val_scores = None\n",
    "    \n",
    "    # Evaluate each combination\n",
    "    for m_type in model_list:\n",
    "        for hyperparams in param_grid_dict[m_type]:\n",
    "            scores_val = train_and_predict_model(m_type, hyperparams, df_train_full, df_val, feature_cols=feature_cols)\n",
    "            auc_val = roc_auc_score(y_val, scores_val)\n",
    "            if auc_val > best_auc:\n",
    "                best_auc = auc_val\n",
    "                best_model_type = m_type\n",
    "                best_hparams    = hyperparams\n",
    "                best_val_scores = scores_val\n",
    "    \n",
    "    return best_val_scores, best_model_type, best_hparams, best_auc\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. SIMULATE POLICIES\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df has columns: patient_id, time, label, predicted_risk (optionally),\n",
    "    policy_func(patient_rows) -> treat_time (int) or None.\n",
    "\n",
    "    Returns: dict of {cost, precision, recall, avg_treatment_time}.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pid, rows in df.groupby('patient_id'):\n",
    "        rows = rows.sort_values('time')\n",
    "        label = rows['label'].iloc[0]  # 0 or 1\n",
    "        treat_time = policy_func(rows)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            treated_flag = 0\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "            else:\n",
    "                cost = 0\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            tt = None\n",
    "        else:\n",
    "            treated_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            tt = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treated_flag,\n",
    "            'treat_time': tt,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res   = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated'] == 1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    \n",
    "    precision = tp_sum / (tp_sum + fp_sum) if (tp_sum+fp_sum) > 0 else 0.0\n",
    "    sick_df = df_res[df_res['label'] == 1]\n",
    "    recall = tp_sum / len(sick_df) if len(sick_df) > 0 else 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt = valid_tt.mean() if len(valid_tt) > 0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 5. BENCHMARK DECISION POLICIES\n",
    "###############################################################################\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,0.5,10)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(rows):\n",
    "            for _, row in rows.iterrows():\n",
    "                if row['predicted_risk'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_thr, best_stats\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=T_MAX,\n",
    "                                    threshold_candidates=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats= None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(rows):\n",
    "            for _, row in rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['predicted_risk'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    return best_vec, best_stats\n",
    "\n",
    "def linear_threshold_search(df, A_candidates=None, B_candidates=None):\n",
    "    if A_candidates is None:\n",
    "        A_candidates = np.linspace(-0.05, 0.05, 5)\n",
    "    if B_candidates is None:\n",
    "        B_candidates = np.linspace(0, 0.5, 6)\n",
    "    \n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(rows):\n",
    "                for _, row in rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = np.clip(A*t + B, 0, 1)\n",
    "                    if row['predicted_risk'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            \n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_stats= stats\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "    \n",
    "    return (best_A,best_B), best_stats\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(rows):\n",
    "            final_row = rows.loc[rows['time'].idxmax()]\n",
    "            if final_row['predicted_risk'] >= thr:\n",
    "                return int(final_row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_thr, best_stats\n",
    "\n",
    "###############################################################################\n",
    "# 6. DATA-DRIVEN DP\n",
    "###############################################################################\n",
    "def assign_buckets(prob, n_buckets=5):\n",
    "    edges = np.linspace(0,1,n_buckets+1)\n",
    "    for b in range(n_buckets):\n",
    "        if edges[b] <= prob < edges[b+1]:\n",
    "            return b\n",
    "    return n_buckets - 1\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=T_MAX, n_buckets=5):\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "\n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows= grp.to_dict('records')\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            lbl = row['label']\n",
    "            \n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            \n",
    "            if i < len(rows)-1:\n",
    "                nxt = rows[i+1]\n",
    "                t_next = int(nxt['time'])\n",
    "                b_next = int(nxt['risk_bucket'])\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t,b,b_next] += 1\n",
    "\n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_, b_, :].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_, b_, :] = transition_counts[t_, b_, :] / denom\n",
    "            else:\n",
    "                p_trans[t_, b_, b_] = 1.0  # fallback\n",
    "\n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_, b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_, b_] = sick_counts[t_, b_] / denom\n",
    "            else:\n",
    "                p_sick[t_, b_] = 0.0\n",
    "    \n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp(p_trans, p_sick,\n",
    "                         FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX):\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    for b in range(n_buckets):\n",
    "        cost_treat   = p_sick[T-1,b]*(D*(T-1)) + (1 - p_sick[T-1,b])*FP\n",
    "        cost_notreat = p_sick[T-1,b]*FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1 - p_sick[t,b])*FP\n",
    "            if t == T-1:\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next] * V[t+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    return V, pi_\n",
    "\n",
    "def make_data_driven_dp_policy(V, pi_, T=T_MAX):\n",
    "    def policy_func(rows):\n",
    "        for _, row in rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                if pi_[t,b] == 1:  # treat\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. ALGORITHM 2 (Full CV) IMPLEMENTATION\n",
    "###############################################################################\n",
    "def run_experiment_algorithm2(\n",
    "    df_all,\n",
    "    n_splits=4,\n",
    "    seed=42,\n",
    "    model_list=(\"catboost\",\"rf\",\"gb\"),\n",
    "    param_grid_dict=None):\n",
    "    \"\"\"\n",
    "    Implement Algorithm 2 (Full Cross-Validation) with integrated ML + DP.\n",
    "\n",
    "    Returns:\n",
    "      final_table (pd.DataFrame): results on G_{n_splits+1}\n",
    "      df_val_stats (pd.DataFrame): details per fold in cross-validation\n",
    "    \"\"\"\n",
    "    if param_grid_dict is None:\n",
    "        # Example small grids\n",
    "        param_grid_dict = {\n",
    "            \"catboost\": [\n",
    "                {\"iterations\":50, \"depth\":3, \"learning_rate\":0.1},\n",
    "                {\"iterations\":50, \"depth\":4, \"learning_rate\":0.05},\n",
    "            ],\n",
    "            \"rf\": [\n",
    "                {\"n_estimators\":50, \"max_depth\":3},\n",
    "                {\"n_estimators\":100, \"max_depth\":5},\n",
    "            ],\n",
    "            \"gb\": [\n",
    "                {\"n_estimators\":50, \"max_depth\":3, \"learning_rate\":0.1},\n",
    "                {\"n_estimators\":100,\"max_depth\":3, \"learning_rate\":0.05},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # 1) Split => G1..G_{n_splits}, G_{n_splits+1}\n",
    "    splits = split_patients_kfold(df_all, n_splits=n_splits, seed=seed)\n",
    "    group_dfs = {}\n",
    "    for group_name, pid_set in splits.items():\n",
    "        sub_df = filter_by_group(df_all, pid_set)\n",
    "        group_dfs[group_name] = sub_df\n",
    "    \n",
    "    test_name = f\"G{n_splits+1}\"\n",
    "    df_test   = group_dfs[test_name]\n",
    "\n",
    "    # 2) Cross-validation on folds {G1..G_n}\n",
    "    all_val_stats = []\n",
    "    for i_val in range(1, n_splits+1):\n",
    "        val_name = f\"G{i_val}\"\n",
    "        df_val = group_dfs[val_name]\n",
    "\n",
    "        # pick best ML\n",
    "        val_scores, best_model_type, best_hparams, best_auc = generate_risk_scores_via_cv(\n",
    "            df_train_splits=group_dfs,\n",
    "            i_val=val_name,\n",
    "            model_list=model_list,\n",
    "            param_grid_dict=param_grid_dict,\n",
    "            feature_cols=FEATURE_COLS\n",
    "        )\n",
    "        df_val = df_val.copy()\n",
    "        df_val[\"predicted_risk\"] = val_scores\n",
    "        group_dfs[val_name] = df_val  # store predictions\n",
    "\n",
    "        # Evaluate benchmark policies on this fold\n",
    "        # (a) Constant\n",
    "        thr_const, stats_const = constant_threshold_search(df_val)\n",
    "        # (b) Dynamic threshold\n",
    "        thr_vec, stats_dyn = dynamic_threshold_random_search(df_val, seed=999+ i_val)\n",
    "        # (c) Linear threshold\n",
    "        (A_lin,B_lin), stats_lin = linear_threshold_search(df_val)\n",
    "        # (d) Wait-till-end\n",
    "        thr_wte, stats_wte = wait_till_end_search(df_val)\n",
    "        # (e) DP\n",
    "        #     1) train final_model on training folds => get risk for that training set => DP => apply to val\n",
    "        train_parts = []\n",
    "        for j in range(1, n_splits+1):\n",
    "            if j != i_val:\n",
    "                train_parts.append(group_dfs[f\"G{j}\"])\n",
    "        df_train_fold = pd.concat(train_parts, ignore_index=True).copy()\n",
    "        \n",
    "        # Retrain \"best model\" on df_train_fold => produce risk => bucket => DP\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        X_train_f = df_train_fold[FEATURE_COLS]\n",
    "        y_train_f = df_train_fold['label']\n",
    "        if best_model_type == \"catboost\":\n",
    "            final_model = CatBoostClassifier(**best_hparams, verbose=False)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        elif best_model_type == \"rf\":\n",
    "            final_model = RandomForestClassifier(**best_hparams, random_state=42)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        else:\n",
    "            final_model = GradientBoostingClassifier(**best_hparams, random_state=42)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        \n",
    "        df_train_fold[\"predicted_risk\"] = final_model.predict_proba(X_train_f)[:,1]\n",
    "        df_train_fold[\"risk_bucket\"]    = df_train_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "        \n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(df_train_fold, T=T_MAX, n_buckets=5)\n",
    "        V, pi_ = train_data_driven_dp(p_trans, p_sick, \n",
    "                                      FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX)\n",
    "        df_val_dp = df_val.copy()\n",
    "        df_val_dp[\"risk_bucket\"] = df_val_dp[\"predicted_risk\"].apply(assign_buckets)\n",
    "        dp_policy = make_data_driven_dp_policy(V, pi_, T=T_MAX)\n",
    "        stats_dp = simulate_policy(df_val_dp, dp_policy)\n",
    "        \n",
    "        all_val_stats.append({\n",
    "            \"fold\": i_val,\n",
    "            \"best_model_type\": best_model_type,\n",
    "            \"best_hparams\": best_hparams,\n",
    "            \"AUC_val\": roc_auc_score(df_val['label'], df_val['predicted_risk']),\n",
    "\n",
    "            \"const_cost\": stats_const[\"cost\"],\n",
    "            \"dyn_cost\":   stats_dyn[\"cost\"],\n",
    "            \"lin_cost\":   stats_lin[\"cost\"],\n",
    "            \"wte_cost\":   stats_wte[\"cost\"],\n",
    "            \"dp_cost\":    stats_dp[\"cost\"],\n",
    "        })\n",
    "    \n",
    "    df_val_stats = pd.DataFrame(all_val_stats)\n",
    "\n",
    "    # 3) Pick a single final model from among folds or do a separate logic.\n",
    "    #    For simplicity, pick the fold that had the best dp_cost:\n",
    "    best_fold_idx = df_val_stats[\"dp_cost\"].idxmin()\n",
    "    fold_rec = df_val_stats.loc[best_fold_idx]\n",
    "    final_model_type = fold_rec[\"best_model_type\"]\n",
    "    final_hparams    = fold_rec[\"best_hparams\"]\n",
    "    \n",
    "    # 4) Retrain on G1..G_n => evaluate on G_{n+1}\n",
    "    train_all = []\n",
    "    for i in range(1, n_splits+1):\n",
    "        train_all.append(group_dfs[f\"G{i}\"])\n",
    "    df_train_all = pd.concat(train_all, ignore_index=True).copy()\n",
    "    \n",
    "    X_train_all = df_train_all[FEATURE_COLS]\n",
    "    y_train_all = df_train_all[\"label\"]\n",
    "    \n",
    "    if final_model_type == \"catboost\":\n",
    "        final_model = CatBoostClassifier(**final_hparams, verbose=False)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    elif final_model_type == \"rf\":\n",
    "        final_model = RandomForestClassifier(**final_hparams, random_state=42)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    else:\n",
    "        final_model = GradientBoostingClassifier(**final_hparams, random_state=42)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "\n",
    "    df_test = df_test.copy()\n",
    "    df_test[\"predicted_risk\"] = final_model.predict_proba(df_test[FEATURE_COLS])[:,1]\n",
    "    \n",
    "    # Evaluate final table\n",
    "    # (a) Constant\n",
    "    thr_const, stats_const = constant_threshold_search(df_test)\n",
    "    # (b) Dynamic\n",
    "    thr_vec, stats_dyn = dynamic_threshold_random_search(df_test)\n",
    "    # (c) Linear\n",
    "    (A_lin,B_lin), stats_lin = linear_threshold_search(df_test)\n",
    "    # (d) Wait-till-end\n",
    "    thr_wte, stats_wte = wait_till_end_search(df_test)\n",
    "    # (e) DP\n",
    "    df_train_all[\"predicted_risk\"] = final_model.predict_proba(df_train_all[FEATURE_COLS])[:,1]\n",
    "    df_train_all[\"risk_bucket\"]    = df_train_all[\"predicted_risk\"].apply(assign_buckets)\n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_train_all, T=T_MAX, n_buckets=5)\n",
    "    V, pi_ = train_data_driven_dp(p_trans, p_sick, \n",
    "                                  FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX)\n",
    "    df_test_dp = df_test.copy()\n",
    "    df_test_dp[\"risk_bucket\"] = df_test_dp[\"predicted_risk\"].apply(assign_buckets)\n",
    "    dp_policy_func = make_data_driven_dp_policy(V, pi_, T=T_MAX)\n",
    "    stats_dp = simulate_policy(df_test_dp, dp_policy_func)\n",
    "\n",
    "    final_table = pd.DataFrame({\n",
    "        \"Method\": [\n",
    "            \"Constant Threshold\",\n",
    "            \"Dynamic Threshold-R\",\n",
    "            \"Linear Threshold\",\n",
    "            \"Wait Till End\",\n",
    "            \"Dynamic Threshold-DP (DataDriven)\"\n",
    "        ],\n",
    "        \"Precision (%)\": [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        \"Cost\": [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        \"Recall (%)\": [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        \"Treatment Time\": [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    return final_table, df_val_stats\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. RUN-ONCE FUNCTION (with a given seed)\n",
    "###############################################################################\n",
    "def run_experiment_once(df_all, seed=42, n_splits=4):\n",
    "    \"\"\"\n",
    "    Runs the full Algorithm 2 cross-validation approach for a single random seed.\n",
    "    Returns the final test table (5 methods) and the cross-validation details.\n",
    "    \"\"\"\n",
    "    final_table, df_cv_details = run_experiment_algorithm2(\n",
    "        df_all=df_all,\n",
    "        n_splits=n_splits,\n",
    "        seed=seed,\n",
    "        model_list=(\"catboost\",\"rf\",\"gb\"),\n",
    "        param_grid_dict=None  # default small grid\n",
    "    )\n",
    "    return final_table, df_cv_details\n",
    "\n",
    "###############################################################################\n",
    "# 9. MAIN: 30 Replications\n",
    "###############################################################################\n",
    "def main():\n",
    "    # 1) Load the data once\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "\n",
    "    # df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "\n",
    "    N_REPS = 30\n",
    "    methods = [\n",
    "        \"Constant Threshold\",\n",
    "        \"Dynamic Threshold-R\",\n",
    "        \"Linear Threshold\",\n",
    "        \"Wait Till End\",\n",
    "        \"Dynamic Threshold-DP (DataDriven)\"\n",
    "    ]\n",
    "    \n",
    "    # 2) Data structure to hold results across runs\n",
    "    results_over_runs = {\n",
    "        m: {'precision': [], 'cost': [], 'recall': [], 'time': []}\n",
    "        for m in methods\n",
    "    }\n",
    "    \n",
    "    # 3) Loop over 30 seeds\n",
    "    for rep in range(N_REPS):\n",
    "        seed_value = 1000 + rep  # or any scheme you like\n",
    "        print(f\"\\n=== RUN {rep+1}/{N_REPS}, seed={seed_value} ===\")\n",
    "        \n",
    "        final_table, _ = run_experiment_once(df_all, seed=seed_value, n_splits=4)\n",
    "        \n",
    "        # final_table has columns: Method, Precision (%), Cost, Recall (%), Treatment Time\n",
    "        # We'll accumulate them in results_over_runs\n",
    "        for idx, row in final_table.iterrows():\n",
    "            m = row[\"Method\"]\n",
    "            results_over_runs[m][\"precision\"].append(row[\"Precision (%)\"])\n",
    "            results_over_runs[m][\"cost\"].append(row[\"Cost\"])\n",
    "            results_over_runs[m][\"recall\"].append(row[\"Recall (%)\"])\n",
    "            results_over_runs[m][\"time\"].append(row[\"Treatment Time\"])\n",
    "    \n",
    "    # 4) Compute mean ± std dev across the 30 runs\n",
    "    final_rows = []\n",
    "    for i, m in enumerate(methods):\n",
    "        prec_arr = np.array(results_over_runs[m][\"precision\"])\n",
    "        cost_arr = np.array(results_over_runs[m][\"cost\"])\n",
    "        rec_arr  = np.array(results_over_runs[m][\"recall\"])\n",
    "        time_arr = np.array(results_over_runs[m][\"time\"])\n",
    "        \n",
    "        prec_mean, prec_std = prec_arr.mean(), prec_arr.std()\n",
    "        cost_mean, cost_std = cost_arr.mean(), cost_arr.std()\n",
    "        rec_mean,  rec_std  = rec_arr.mean(),  rec_arr.std()\n",
    "        time_mean, time_std = time_arr.mean(), time_arr.std()\n",
    "        \n",
    "        final_rows.append([\n",
    "            m,\n",
    "            f\"{prec_mean:.2f} ± {prec_std:.2f}\",\n",
    "            f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "            f\"{rec_mean:.2f} ± {rec_std:.2f}\",\n",
    "            f\"{time_mean:.2f} ± {time_std:.2f}\"\n",
    "        ])\n",
    "    \n",
    "    # 5) Print final summary\n",
    "    print(\"\\n=== FINAL RESULTS (Mean ± Std Dev over 30 Replications) ===\")\n",
    "    print(\"{:<28s} {:>18s} {:>18s} {:>18s} {:>18s}\".format(\n",
    "        \"Method\", \"Precision(%)\", \"Cost\", \"Recall(%)\", \"Avg Time\"))\n",
    "    for row in final_rows:\n",
    "        m, prec_str, cost_str, rec_str, time_str = row\n",
    "        print(f\"{m:<28s} {prec_str:>18s} {cost_str:>18s} {rec_str:>18s} {time_str:>18s}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "#algorithm 3\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS & SETTINGS\n",
    "###############################################################################\n",
    "FP_COST = 10    # Penalty for false positive (treating a healthy patient)\n",
    "FN_COST = 50    # Penalty for false negative (never treating a sick patient)\n",
    "D_COST  = 1     # Penalty per time-step of delay in treating a sick patient\n",
    "GAMMA   = 0.99  # Default discount factor for DP (may be overridden in DP tuning)\n",
    "T_MAX   = 20    # Time horizon (discrete steps 0..T_MAX-1 for each patient)\n",
    "\n",
    "FEATURE_COLS = [\"time\", \"EIT\", \"NIRS\", \"EIS\"]\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS FOR DATA SPLITTING & FILTERING\n",
    "###############################################################################\n",
    "def split_patients_kfold(df, n_splits=4, seed=0):\n",
    "    \"\"\"\n",
    "    Shuffle unique patient IDs, then split into (n_splits+1) groups G1,...,G_{n_splits},G_{n_splits+1}.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pts = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pts)\n",
    "    \n",
    "    n = len(unique_pts)\n",
    "    splits = {}\n",
    "    \n",
    "    # Partition into n_splits+1 roughly equal groups\n",
    "    for i in range(n_splits + 1):\n",
    "        start_idx = int(i * n / (n_splits + 1))\n",
    "        end_idx   = int((i + 1) * n / (n_splits + 1))\n",
    "        group_name = f\"G{i+1}\"\n",
    "        splits[group_name] = set(unique_pts[start_idx:end_idx])\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    \"\"\"Return the subset of df whose patient_id is in pid_set.\"\"\"\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. ML TRAINING & RISK-SCORE PREDICTIONS\n",
    "###############################################################################\n",
    "def train_and_predict_model(\n",
    "    model_type,\n",
    "    hyperparams,\n",
    "    df_train,\n",
    "    df_val,\n",
    "    feature_cols=FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    Train a classification model (CatBoost/RF/GB) on df_train and return predicted\n",
    "    probabilities for df_val. `hyperparams` is a dict of model-specific hyper-parameters.\n",
    "    \"\"\"\n",
    "    X_train = df_train[feature_cols]\n",
    "    y_train = df_train['label']\n",
    "    \n",
    "    if model_type == \"catboost\":\n",
    "        model = CatBoostClassifier(**hyperparams, verbose=False)\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model_type == \"rf\":\n",
    "        model = RandomForestClassifier(**hyperparams, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model_type == \"gb\":\n",
    "        model = GradientBoostingClassifier(**hyperparams, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type={model_type}\")\n",
    "    \n",
    "    X_val = df_val[feature_cols]\n",
    "    risk_scores = model.predict_proba(X_val)[:,1]  # Probability label=1\n",
    "    return risk_scores\n",
    "\n",
    "\n",
    "def select_best_ml_hyperparams_by_auc(\n",
    "    df_train_splits,\n",
    "    val_split_name,\n",
    "    model_list,\n",
    "    param_grid_dict,\n",
    "    feature_cols=FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    Perform a grid search over (model_type, hyperparams) to maximize AUC on the \n",
    "    validation set = df_train_splits[val_split_name].\n",
    "    Return:\n",
    "       best_model_type, best_hyperparams, best_auc, val_preds (predicted_risk for the val set).\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    df_val = df_train_splits[val_split_name].copy()\n",
    "    \n",
    "    # Combine all other folds for training\n",
    "    train_df_list = []\n",
    "    for k, v_df in df_train_splits.items():\n",
    "        if k != val_split_name:\n",
    "            train_df_list.append(v_df)\n",
    "    df_train_full = pd.concat(train_df_list, ignore_index=True)\n",
    "    \n",
    "    X_val = df_val[feature_cols]\n",
    "    y_val = df_val['label'].values\n",
    "    \n",
    "    best_model_type = None\n",
    "    best_hparams    = None\n",
    "    best_auc        = -999\n",
    "    best_preds      = None\n",
    "    \n",
    "    # Grid search across all candidate (model_type, hyperparam)\n",
    "    for model_type in model_list:\n",
    "        for hyperparams in param_grid_dict[model_type]:\n",
    "            # Train on df_train_full, predict on df_val\n",
    "            preds = train_and_predict_model(\n",
    "                model_type=model_type,\n",
    "                hyperparams=hyperparams,\n",
    "                df_train=df_train_full,\n",
    "                df_val=df_val,\n",
    "                feature_cols=feature_cols\n",
    "            )\n",
    "            auc_val = roc_auc_score(y_val, preds)\n",
    "            if auc_val > best_auc:\n",
    "                best_auc = auc_val\n",
    "                best_model_type = model_type\n",
    "                best_hparams    = hyperparams\n",
    "                best_preds      = preds\n",
    "    \n",
    "    return best_model_type, best_hparams, best_auc, best_preds\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. POLICY SIMULATION (Compute cost, precision, recall, etc.)\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df has columns: [patient_id, time, label, predicted_risk].\n",
    "    policy_func(patient_rows) -> an integer in [0..T_MAX-1] for the \n",
    "        time step of treatment, or None if never treated.\n",
    "    Returns a dict with keys {cost, precision, recall, avg_treatment_time}, etc.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pid, patient_rows in df.groupby('patient_id'):\n",
    "        patient_rows = patient_rows.sort_values('time')\n",
    "        label = patient_rows['label'].iloc[0]  # 0 or 1 (healthy vs sick)\n",
    "        treat_time = policy_func(patient_rows)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treat\n",
    "            treated_flag = 0\n",
    "            if label == 1:\n",
    "                cost = FN_COST  # missed a sick patient\n",
    "            else:\n",
    "                cost = 0\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            tt = None\n",
    "        else:\n",
    "            # treat at treat_time\n",
    "            treated_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time  # delay cost\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            tt = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treated_flag,\n",
    "            'treat_time': tt,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated'] == 1]\n",
    "    tp_sum     = treated_df['tp'].sum()\n",
    "    fp_sum     = treated_df['fp'].sum()\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df   = df_res[df_res['label'] == 1]\n",
    "    total_sick= len(sick_df)\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt   = valid_tt.mean() if len(valid_tt) > 0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. BENCHMARK STRATEGIES (Constant Threshold, Dynamic, Linear, Wait-Till-End)\n",
    "###############################################################################\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    \"\"\"\n",
    "    Try a grid of constant thresholds for the entire time horizon,\n",
    "    pick the one minimizing cost on df. Return (best_threshold, best_stats).\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0, 0.5, 8)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['predicted_risk'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_thr, best_stats\n",
    "\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=T_MAX,\n",
    "                                    threshold_candidates=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                    n_samples=100,\n",
    "                                    seed=0):\n",
    "    \"\"\"\n",
    "    Sample random time-varying thresholds (one threshold per time step),\n",
    "    measure cost, pick the best. For demonstration.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['predicted_risk'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_vec, best_stats\n",
    "\n",
    "\n",
    "def linear_threshold_search(df, A_candidates=None, B_candidates=None):\n",
    "    \"\"\"\n",
    "    threshold(t) = A*t + B, clipped to [0,1].\n",
    "    Search over A_candidates x B_candidates, pick the best cost.\n",
    "    \"\"\"\n",
    "    if A_candidates is None:\n",
    "        A_candidates = np.linspace(-0.05, 0.05, 3)\n",
    "    if B_candidates is None:\n",
    "        B_candidates = np.linspace(0,1,5)\n",
    "    \n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A * t + B\n",
    "                    thr = max(0, min(1, thr))  # clip to [0,1]\n",
    "                    if row['predicted_risk'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            \n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    \n",
    "    return (best_A, best_B), best_stats\n",
    "\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    \"\"\"\n",
    "    Treat (if at all) only at the final time step, with a single threshold.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0, 1, 21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            # Look at the final time row\n",
    "            final_row = patient_rows.loc[patient_rows['time'].idxmax()]\n",
    "            if final_row['predicted_risk'] >= thr:\n",
    "                return int(final_row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6. DATA-DRIVEN DP (Bucketed)\n",
    "###############################################################################\n",
    "def assign_buckets(prob, n_buckets=5):\n",
    "    \"\"\"\n",
    "    Convert predicted probability into a discrete bucket 0..(n_buckets-1).\n",
    "    E.g. edges for 5 buckets = [0,0.2,0.4,0.6,0.8,1.0]\n",
    "    \"\"\"\n",
    "    edges = np.linspace(0, 1, n_buckets+1)\n",
    "    for b in range(n_buckets):\n",
    "        if edges[b] <= prob < edges[b+1]:\n",
    "            return b\n",
    "    return n_buckets-1  # fallback if prob=1.0\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=T_MAX, n_buckets=5):\n",
    "    \"\"\"\n",
    "    Given df_train with 'predicted_risk' & 'label' & 'time' (0..T-1),\n",
    "    compute p_trans[t,b,b'] = P(bucket_{t+1}=b' | bucket_t=b),\n",
    "    and p_sick[t,b] = Probability of being sick in (t,b).\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    bucket_counts     = np.zeros((T, n_buckets))\n",
    "    sick_counts       = np.zeros((T, n_buckets))\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    \n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        rows = grp.to_dict('records')\n",
    "        for i, row in enumerate(rows):\n",
    "            t   = int(row['time'])\n",
    "            b   = int(row['risk_bucket'])\n",
    "            lbl = int(row['label'])\n",
    "            if t < T:\n",
    "                bucket_counts[t, b] += 1\n",
    "                sick_counts[t, b]   += lbl\n",
    "            \n",
    "            if i < len(rows) - 1:\n",
    "                row_next = rows[i+1]\n",
    "                t_next = int(row_next['time'])\n",
    "                b_next = int(row_next['risk_bucket'])\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t, b, b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_, b_, :].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_, b_, :] = transition_counts[t_, b_, :] / denom\n",
    "            else:\n",
    "                # if no data, remain in the same bucket with prob=1\n",
    "                p_trans[t_, b_, b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets))\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_, b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_, b_] = sick_counts[t_, b_] / denom\n",
    "            else:\n",
    "                p_sick[t_, b_] = 0.0\n",
    "    \n",
    "    return p_trans, p_sick\n",
    "\n",
    "\n",
    "def train_data_driven_dp(p_trans, p_sick,\n",
    "                         FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "                         gamma=GAMMA, T=T_MAX):\n",
    "    \"\"\"\n",
    "    Standard backward DP for the bucket-based approach:\n",
    "      - V[t,b] = min( cost of treating now, cost of waiting )\n",
    "    Return V, pi_ (value function and policy).\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # Terminal cost at t=T\n",
    "    # If we reach time T in bucket b, the next step is \"end\" => we can choose treat or not\n",
    "    for b in range(n_buckets):\n",
    "        cost_treat = p_sick[T-1,b]* (D*(T-1)) + (1 - p_sick[T-1,b])*FP\n",
    "        cost_skip  = p_sick[T-1,b]*FN\n",
    "        V[T,b]     = min(cost_treat, cost_skip)\n",
    "    \n",
    "    # Backward recursion\n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # Option A: Treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1 - p_sick[t,b])*FP\n",
    "            \n",
    "            # Option B: Wait => expected cost of next state\n",
    "            if t == T-1:\n",
    "                # next step is t=T\n",
    "                exp_future = V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next] * V[t+1,b_next]\n",
    "            cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    \n",
    "    return V, pi_\n",
    "\n",
    "def make_data_driven_dp_policy(V, pi_, T=T_MAX):\n",
    "    \"\"\"\n",
    "    Returns a function that uses pi_[t,b] to decide when to treat.\n",
    "    \"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                action = pi_[t,b]  # 0=wait, 1=treat\n",
    "                if action == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. DP Hyper-Parameter Search for Algorithm 3\n",
    "###############################################################################\n",
    "def dp_param_search(df_train_fold, df_val_fold,\n",
    "                    dp_param_grid,  # list of dicts, e.g. [{'gamma':0.95}, {'gamma':0.99}]\n",
    "                    T=T_MAX):\n",
    "    \"\"\"\n",
    "    Given a training fold & validation fold, we try each DP param set in dp_param_grid,\n",
    "    build a DP policy, and measure cost on the validation fold.\n",
    "    \n",
    "    Return the best_dp_params, the cost, and the predicted risk for the validation set\n",
    "    (the validation set already should have 'predicted_risk' from the chosen ML).\n",
    "    \"\"\"\n",
    "    # For DP, we need to:\n",
    "    #  - compute discrete buckets in the training fold\n",
    "    #  - estimate transitions\n",
    "    #  - run DP for each param set\n",
    "    #  - apply the resulting policy on the validation fold\n",
    "    #  - measure cost\n",
    "    \n",
    "    # 1) Assign buckets to training fold\n",
    "    df_train_fold = df_train_fold.copy()\n",
    "    df_train_fold[\"risk_bucket\"] = df_train_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    # 2) Estimate transitions\n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_train_fold, T=T)\n",
    "    \n",
    "    best_params = None\n",
    "    best_cost   = float('inf')\n",
    "    best_stats  = None\n",
    "    \n",
    "    # Assign buckets to val fold too (for policy simulation)\n",
    "    df_val_fold = df_val_fold.copy()\n",
    "    df_val_fold[\"risk_bucket\"] = df_val_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    for param_dict in dp_param_grid:\n",
    "        gamma_ = param_dict.get(\"gamma\", GAMMA)\n",
    "        # Potentially we could also vary D, FP, FN, etc. if included in the dictionary\n",
    "        D_  = param_dict.get(\"D\", D_COST)\n",
    "        FP_ = param_dict.get(\"FP\", FP_COST)\n",
    "        FN_ = param_dict.get(\"FN\", FN_COST)\n",
    "        \n",
    "        # 3) Train DP\n",
    "        V, pi_ = train_data_driven_dp(p_trans, p_sick,\n",
    "                                      FP=FP_, FN=FN_, D=D_, gamma=gamma_, T=T)\n",
    "        dp_policy_func = make_data_driven_dp_policy(V, pi_, T=T)\n",
    "        \n",
    "        # 4) Evaluate on df_val_fold\n",
    "        stats = simulate_policy(df_val_fold, dp_policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost  = stats['cost']\n",
    "            best_params= param_dict\n",
    "            best_stats = stats\n",
    "    \n",
    "    return best_params, best_stats\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. ALGORITHM 3: SEQUENTIAL OPTIMIZATION\n",
    "###############################################################################\n",
    "def run_experiment_algorithm3(\n",
    "    df_all,\n",
    "    n_splits=4,\n",
    "    seed=42,\n",
    "    model_list=(\"catboost\",\"rf\",\"gb\"),\n",
    "    ml_param_grid=None,\n",
    "    dp_param_grid=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Implement Algorithm 3 (Sequential Optimization):\n",
    "      1) Cross-validate ML hyperparams (AUC-based).\n",
    "      2) For each fold's chosen ML model, cross-validate DP hyperparams (cost-based).\n",
    "      3) Summarize the sets of (mu) found in each fold => define mu_{all}^*.\n",
    "      4) Use all folds again to pick final ML hyperparams (by AUC).\n",
    "      5) Then, with that ML fixed, pick final DP hyperparams from mu_{all}^* by cost.\n",
    "      6) Retrain on G1..G_n with chosen ML, produce predicted_risk, run final DP,\n",
    "         evaluate on G_{n+1}.\n",
    "    \n",
    "    Because there are multiple ways to interpret the text-block pseudo-code,\n",
    "    this function follows the step-by-step logic common in \"sequential\" \n",
    "    (non-decision-aware => then DP decision) style. \n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    if ml_param_grid is None:\n",
    "        # A small default grid for demonstration\n",
    "        ml_param_grid = {\n",
    "            \"catboost\": [\n",
    "                {\"iterations\":50, \"depth\":3, \"learning_rate\":0.1},\n",
    "                {\"iterations\":50, \"depth\":4, \"learning_rate\":0.05},\n",
    "            ],\n",
    "            \"rf\": [\n",
    "                {\"n_estimators\":50, \"max_depth\":3},\n",
    "                {\"n_estimators\":100,\"max_depth\":5},\n",
    "            ],\n",
    "            \"gb\": [\n",
    "                {\"n_estimators\":50, \"max_depth\":3, \"learning_rate\":0.1},\n",
    "                {\"n_estimators\":100,\"max_depth\":3, \"learning_rate\":0.05},\n",
    "            ]\n",
    "        }\n",
    "    if dp_param_grid is None:\n",
    "        # Example DP param grid: vary gamma, or vary others\n",
    "        dp_param_grid = [\n",
    "            {\"gamma\": 0.95},\n",
    "            {\"gamma\": 0.99},\n",
    "        ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nRunning Algorithm 3 (Sequential Optimization) with {n_splits} folds...\")\n",
    "    \n",
    "    # 1) Split data => G1..G_{n_splits}, G_{n_splits+1}\n",
    "    splits = split_patients_kfold(df_all, n_splits=n_splits, seed=seed)\n",
    "    group_dfs = {}\n",
    "    for group_name, pid_set in splits.items():\n",
    "        sub_df = filter_by_group(df_all, pid_set)\n",
    "        group_dfs[group_name] = sub_df\n",
    "    \n",
    "    test_name = f\"G{n_splits+1}\"\n",
    "    df_test   = group_dfs[test_name]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (A) CROSS-VALIDATE ML => pick best ML hyperparams for each fold\n",
    "    # -------------------------------------------------------------------------\n",
    "    ml_cv_details = []\n",
    "    \n",
    "    for i_val in range(1, n_splits+1):\n",
    "        val_name = f\"G{i_val}\"\n",
    "        \n",
    "        # (A1) Find best ML hyperparams by AUC\n",
    "        best_model_type, best_hparams, best_auc, val_preds = select_best_ml_hyperparams_by_auc(\n",
    "            df_train_splits=group_dfs,\n",
    "            val_split_name=val_name,\n",
    "            model_list=model_list,\n",
    "            param_grid_dict=ml_param_grid,\n",
    "            feature_cols=FEATURE_COLS\n",
    "        )\n",
    "        \n",
    "        # (A2) Store predicted_risk for that validation set\n",
    "        df_val = group_dfs[val_name].copy()\n",
    "        df_val[\"predicted_risk\"] = val_preds\n",
    "        \n",
    "        # Save it back\n",
    "        group_dfs[val_name] = df_val\n",
    "        \n",
    "        ml_cv_details.append({\n",
    "            \"fold\": i_val,\n",
    "            \"best_model_type\": best_model_type,\n",
    "            \"best_hparams\": best_hparams,\n",
    "            \"AUC_val\": best_auc\n",
    "        })\n",
    "    \n",
    "    df_ml_cv_details = pd.DataFrame(ml_cv_details)\n",
    "    # Summarize which ML hyperparams got chosen by each fold ...\n",
    "    # (We will re-check them in the next step.)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (B) For each fold's chosen ML, do a DP hyper-param search => pick DP param\n",
    "    # -------------------------------------------------------------------------\n",
    "    dp_cv_details = []\n",
    "    \n",
    "    for i_val in range(1, n_splits+1):\n",
    "        val_name  = f\"G{i_val}\"\n",
    "        best_rec  = df_ml_cv_details[df_ml_cv_details['fold'] == i_val].iloc[0]\n",
    "        ml_model_type = best_rec[\"best_model_type\"]\n",
    "        ml_hparams    = best_rec[\"best_hparams\"]\n",
    "        \n",
    "        # 1) Retrain that ML on \"training folds except G_i_val\" => get predicted_risk\n",
    "        #    for the union (train_folds) = G\\G_i\n",
    "        train_folds = []\n",
    "        for j in range(1, n_splits+1):\n",
    "            if j != i_val:\n",
    "                train_folds.append(group_dfs[f\"G{j}\"])\n",
    "        df_train_fold = pd.concat(train_folds, ignore_index=True).copy()\n",
    "        \n",
    "        # Train & predict on df_train_fold itself for DP transitions\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        \n",
    "        X_train_f = df_train_fold[FEATURE_COLS]\n",
    "        y_train_f = df_train_fold['label']\n",
    "        \n",
    "        # Rebuild the model\n",
    "        if ml_model_type == \"catboost\":\n",
    "            final_model = CatBoostClassifier(**ml_hparams, verbose=False)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        elif ml_model_type == \"rf\":\n",
    "            final_model = RandomForestClassifier(**ml_hparams, random_state=42)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        else:\n",
    "            final_model = GradientBoostingClassifier(**ml_hparams, random_state=42)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        \n",
    "        # Store predictions in df_train_fold\n",
    "        df_train_fold[\"predicted_risk\"] = final_model.predict_proba(X_train_f)[:,1]\n",
    "        \n",
    "        # 2) DP hyper-param search on this fold, using the same \"train => val\" logic\n",
    "        #    Validation set is group_dfs[val_name], which already has *some* predicted risk \n",
    "        #    but that risk was from the *best model for i_val.* We should unify it carefully.\n",
    "        \n",
    "        # Actually, to be consistent: The DP sees the same final model that we have for i_val.\n",
    "        # So let's do a fresh predicted risk for df_val as well. (Because we want consistent \n",
    "        # train->val usage for DP.)\n",
    "        \n",
    "        df_val_fold = group_dfs[val_name].copy()\n",
    "        X_val_fold  = df_val_fold[FEATURE_COLS]\n",
    "        df_val_fold[\"predicted_risk\"] = final_model.predict_proba(X_val_fold)[:,1]\n",
    "        \n",
    "        best_dp_params, best_dp_stats = dp_param_search(\n",
    "            df_train_fold=df_train_fold,\n",
    "            df_val_fold=df_val_fold,\n",
    "            dp_param_grid=dp_param_grid,\n",
    "            T=T_MAX\n",
    "        )\n",
    "        \n",
    "        dp_cv_details.append({\n",
    "            \"fold\": i_val,\n",
    "            \"chosen_ML_model\": ml_model_type,\n",
    "            \"chosen_ML_hparams\": ml_hparams,\n",
    "            \"chosen_DP_params\": best_dp_params,\n",
    "            \"dp_val_cost\": best_dp_stats[\"cost\"],\n",
    "            \"dp_val_prec\": best_dp_stats[\"precision\"],\n",
    "            \"dp_val_rec\":  best_dp_stats[\"recall\"],\n",
    "            \"dp_val_avgTT\": best_dp_stats[\"avg_treatment_time\"]\n",
    "        })\n",
    "    \n",
    "    df_dp_cv_details = pd.DataFrame(dp_cv_details)\n",
    "    \n",
    "    # Collect all DP param sets that got chosen: mu(j) for j=1..n\n",
    "    mu_all_star = []\n",
    "    for _, row_ in df_dp_cv_details.iterrows():\n",
    "        # each fold might have chosen a dictionary like {\"gamma\":0.95}\n",
    "        mu_all_star.append(row_[\"chosen_DP_params\"])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (C) Now do a second pass to pick the final ML hyperparams \\lambda^*\n",
    "    #     across all folds (by AUC).\n",
    "    # -------------------------------------------------------------------------\n",
    "    # The simplest approach: we do a standard cross-validation again for ML \n",
    "    # but ignoring DP for the moment, because this is \"sequential\" approach.\n",
    "    # => Essentially the same method we used in step (A), but summarizing now \n",
    "    #    across all folds. We'll pick the single best (model_type, hyperparams)\n",
    "    #    that leads to highest average AUC across G1..G_n.\n",
    "    \n",
    "    # We'll accumulate fold-level AUC for each candidate, then pick the best overall.\n",
    "    candidate_list = []\n",
    "    for model_type in model_list:\n",
    "        for hyperparams in ml_param_grid[model_type]:\n",
    "            candidate_list.append((model_type, hyperparams))\n",
    "    \n",
    "    results_auc_cv = []\n",
    "    for (mtype, mhp) in candidate_list:\n",
    "        fold_aucs = []\n",
    "        for i_val in range(1, n_splits+1):\n",
    "            val_name = f\"G{i_val}\"\n",
    "            # Train on G\\G_i\n",
    "            train_folds = []\n",
    "            for j in range(1, n_splits+1):\n",
    "                if j != i_val:\n",
    "                    train_folds.append(group_dfs[f\"G{j}\"])\n",
    "            df_train_fold = pd.concat(train_folds, ignore_index=True)\n",
    "            \n",
    "            # Train model\n",
    "            X_train_f = df_train_fold[FEATURE_COLS]\n",
    "            y_train_f = df_train_fold['label']\n",
    "            \n",
    "            if mtype == \"catboost\":\n",
    "                tmp_model = CatBoostClassifier(**mhp, verbose=False)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            elif mtype == \"rf\":\n",
    "                tmp_model = RandomForestClassifier(**mhp, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            else:\n",
    "                tmp_model = GradientBoostingClassifier(**mhp, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            \n",
    "            # Predict on validation G_i\n",
    "            df_val_fold = group_dfs[val_name]\n",
    "            X_val_fold  = df_val_fold[FEATURE_COLS]\n",
    "            val_preds   = tmp_model.predict_proba(X_val_fold)[:,1]\n",
    "            \n",
    "            auc_val = roc_auc_score(df_val_fold['label'], val_preds)\n",
    "            fold_aucs.append(auc_val)\n",
    "        \n",
    "        avg_auc = np.mean(fold_aucs)\n",
    "        results_auc_cv.append({\n",
    "            \"model_type\": mtype,\n",
    "            \"hyperparams\": mhp,\n",
    "            \"avg_auc\": avg_auc\n",
    "        })\n",
    "    \n",
    "    df_results_auc_cv = pd.DataFrame(results_auc_cv)\n",
    "    # pick best by avg_auc\n",
    "    best_row = df_results_auc_cv.loc[df_results_auc_cv['avg_auc'].idxmax()]\n",
    "    final_ml_type   = best_row[\"model_type\"]\n",
    "    final_ml_params = best_row[\"hyperparams\"]\n",
    "    final_ml_auc    = best_row[\"avg_auc\"]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (D) Next, with that final ML type/params fixed, we pick the best DP hyperparams \n",
    "    #     from the union mu_all_star we collected above.\n",
    "    #     We'll evaluate each candidate in mu_all_star with a new cross-validation \n",
    "    #     pass for cost, but with the final ML in place.\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Because multiple folds might produce duplicates in mu_all_star, we can deduplicate:\n",
    "    import json\n",
    "    unique_mu = []\n",
    "    seen_strs = set()\n",
    "    for mu_dict in mu_all_star:\n",
    "        s = json.dumps(mu_dict, sort_keys=True)\n",
    "        if s not in seen_strs:\n",
    "            seen_strs.add(s)\n",
    "            unique_mu.append(mu_dict)\n",
    "    \n",
    "    dp_candidates = unique_mu\n",
    "    \n",
    "    # Evaluate each dp_candidates in cross-validation with final ML\n",
    "    #  => for each fold i_val, we do: train final ML on G\\G_i => predict => \n",
    "    #     run the DP with param from dp_candidates => measure cost => average across folds\n",
    "    results_dp_cv = []\n",
    "    \n",
    "    for dp_params in dp_candidates:\n",
    "        fold_costs = []\n",
    "        for i_val in range(1, n_splits+1):\n",
    "            val_name = f\"G{i_val}\"\n",
    "            # Train final ML on G\\G_i\n",
    "            train_folds = []\n",
    "            for j in range(1, n_splits+1):\n",
    "                if j != i_val:\n",
    "                    train_folds.append(group_dfs[f\"G{j}\"])\n",
    "            df_train_fold = pd.concat(train_folds, ignore_index=True).copy()\n",
    "            \n",
    "            X_train_f = df_train_fold[FEATURE_COLS]\n",
    "            y_train_f = df_train_fold['label']\n",
    "            \n",
    "            if final_ml_type == \"catboost\":\n",
    "                tmp_model = CatBoostClassifier(**final_ml_params, verbose=False)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            elif final_ml_type == \"rf\":\n",
    "                tmp_model = RandomForestClassifier(**final_ml_params, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            else:\n",
    "                tmp_model = GradientBoostingClassifier(**final_ml_params, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            \n",
    "            df_train_fold[\"predicted_risk\"] = tmp_model.predict_proba(X_train_f)[:,1]\n",
    "            \n",
    "            # Build DP for dp_params\n",
    "            df_val_fold = group_dfs[val_name].copy()\n",
    "            X_val_fold  = df_val_fold[FEATURE_COLS]\n",
    "            df_val_fold[\"predicted_risk\"] = tmp_model.predict_proba(X_val_fold)[:,1]\n",
    "            \n",
    "            # train DP on df_train_fold\n",
    "            df_train_fold[\"risk_bucket\"] = df_train_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "            p_trans, p_sick = estimate_transition_and_sick_probs(df_train_fold, T=T_MAX)\n",
    "            \n",
    "            gamma_ = dp_params.get(\"gamma\", GAMMA)\n",
    "            D_  = dp_params.get(\"D\", D_COST)\n",
    "            FP_ = dp_params.get(\"FP\", FP_COST)\n",
    "            FN_ = dp_params.get(\"FN\", FN_COST)\n",
    "            \n",
    "            V, pi_ = train_data_driven_dp(\n",
    "                p_trans, p_sick,\n",
    "                FP=FP_, FN=FN_, D=D_, gamma=gamma_, T=T_MAX\n",
    "            )\n",
    "            policy_func = make_data_driven_dp_policy(V, pi_, T=T_MAX)\n",
    "            \n",
    "            # evaluate cost on df_val_fold\n",
    "            df_val_fold[\"risk_bucket\"] = df_val_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "            stats = simulate_policy(df_val_fold, policy_func)\n",
    "            fold_costs.append(stats['cost'])\n",
    "        \n",
    "        avg_cost = np.mean(fold_costs)\n",
    "        results_dp_cv.append({\n",
    "            \"dp_params\": dp_params,\n",
    "            \"avg_cost\": avg_cost\n",
    "        })\n",
    "    \n",
    "    df_results_dp_cv = pd.DataFrame(results_dp_cv)\n",
    "    best_dp_idx = df_results_dp_cv['avg_cost'].idxmin()\n",
    "    final_dp_params = df_results_dp_cv.loc[best_dp_idx, \"dp_params\"]\n",
    "    final_dp_cost   = df_results_dp_cv.loc[best_dp_idx, \"avg_cost\"]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final chosen ML: {final_ml_type} {final_ml_params}, avg AUC={final_ml_auc:.3f}\")\n",
    "        print(f\"Final chosen DP params: {final_dp_params}, avg cost={final_dp_cost:.3f}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (E) Retrain on G1..G_n with final ML => evaluate on G_{n+1}\n",
    "    # -------------------------------------------------------------------------\n",
    "    train_all = []\n",
    "    for i in range(1, n_splits+1):\n",
    "        train_all.append(group_dfs[f\"G{i}\"])\n",
    "    df_train_all = pd.concat(train_all, ignore_index=True).copy()\n",
    "    \n",
    "    X_train_all = df_train_all[FEATURE_COLS]\n",
    "    y_train_all = df_train_all['label']\n",
    "    \n",
    "    if final_ml_type == \"catboost\":\n",
    "        final_model = CatBoostClassifier(**final_ml_params, verbose=False)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    elif final_ml_type == \"rf\":\n",
    "        final_model = RandomForestClassifier(**final_ml_params, random_state=42)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    else:\n",
    "        final_model = GradientBoostingClassifier(**final_ml_params, random_state=42)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    \n",
    "    # Predict risk for train to build DP transitions\n",
    "    df_train_all[\"predicted_risk\"] = final_model.predict_proba(X_train_all)[:,1]\n",
    "    df_train_all[\"risk_bucket\"]    = df_train_all[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    # Build DP with final_dp_params\n",
    "    gamma_ = final_dp_params.get(\"gamma\", GAMMA)\n",
    "    D_  = final_dp_params.get(\"D\", D_COST)\n",
    "    FP_ = final_dp_params.get(\"FP\", FP_COST)\n",
    "    FN_ = final_dp_params.get(\"FN\", FN_COST)\n",
    "    \n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_train_all, T=T_MAX)\n",
    "    V, pi_ = train_data_driven_dp(\n",
    "        p_trans, p_sick,\n",
    "        FP=FP_, FN=FN_, D=D_, gamma=gamma_, T=T_MAX\n",
    "    )\n",
    "    dp_final_policy = make_data_driven_dp_policy(V, pi_, T=T_MAX)\n",
    "    \n",
    "    # Evaluate on G_{n+1}\n",
    "    df_test_eval = df_test.copy()\n",
    "    X_test_eval  = df_test_eval[FEATURE_COLS]\n",
    "    df_test_eval[\"predicted_risk\"] = final_model.predict_proba(X_test_eval)[:,1]\n",
    "    \n",
    "    # Benchmark methods:\n",
    "    best_thr_const, stats_const = constant_threshold_search(df_test_eval)\n",
    "    best_dyn_vec, stats_dyn     = dynamic_threshold_random_search(df_test_eval)\n",
    "    (bestA,bestB), stats_lin    = linear_threshold_search(df_test_eval)\n",
    "    best_thr_wte, stats_wte     = wait_till_end_search(df_test_eval)\n",
    "    \n",
    "    df_test_eval[\"risk_bucket\"] = df_test_eval[\"predicted_risk\"].apply(assign_buckets)\n",
    "    stats_dp = simulate_policy(df_test_eval, dp_final_policy)\n",
    "    \n",
    "    final_table = pd.DataFrame({\n",
    "        \"Method\": [\n",
    "            \"Constant Threshold\",\n",
    "            \"Dynamic Threshold-R\",\n",
    "            \"Linear Threshold\",\n",
    "            \"Wait Till End\",\n",
    "            \"Dynamic Threshold-DP\"\n",
    "        ],\n",
    "        \"Precision (%)\": [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        \"Cost\": [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        \"Recall (%)\": [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        \"Treatment Time\": [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"ml_cv_details\": df_ml_cv_details,\n",
    "        \"dp_cv_details\": df_dp_cv_details,\n",
    "        \"ml_final_choice\": (final_ml_type, final_ml_params, final_ml_auc),\n",
    "        \"dp_final_choice\": (final_dp_params, final_dp_cost),\n",
    "        \"test_results_table\": final_table\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 9. RUN MULTIPLE REPLICATIONS\n",
    "###############################################################################\n",
    "def run_multiple_replications(df_all, n_replications=30, n_splits=4):\n",
    "    \"\"\"\n",
    "    Run Algorithm 3 multiple times with different random seeds.\n",
    "    Compute mean and standard deviation for each metric.\n",
    "    \"\"\"\n",
    "    # Define standard method names for consistent reporting\n",
    "    standard_methods = [\n",
    "        'Constant Threshold',\n",
    "        'Dynamic Threshold-R',\n",
    "        'Linear Threshold',\n",
    "        'Wait Till End',\n",
    "        'Dynamic Threshold-DP'\n",
    "    ]\n",
    "    \n",
    "    # Initialize containers for each metric and method\n",
    "    precision_values = {method: [] for method in standard_methods}\n",
    "    cost_values = {method: [] for method in standard_methods}\n",
    "    recall_values = {method: [] for method in standard_methods}\n",
    "    treatment_time_values = {method: [] for method in standard_methods}\n",
    "    \n",
    "    for i in range(n_replications):\n",
    "        seed = i  # Use a different seed for each replication\n",
    "        print(f\"\\nRunning replication {i+1}/{n_replications} with seed={seed}\")\n",
    "        \n",
    "        # Run Algorithm 3 with current seed\n",
    "        results = run_experiment_algorithm3(\n",
    "            df_all=df_all, \n",
    "            n_splits=n_splits, \n",
    "            seed=seed,\n",
    "            verbose=False  # Turn off verbose output for cleaner console\n",
    "        )\n",
    "        \n",
    "        # Extract final test results table\n",
    "        test_table = results[\"test_results_table\"]\n",
    "        \n",
    "        # Extract values for each method\n",
    "        for _, row in test_table.iterrows():\n",
    "            method = row['Method']\n",
    "            \n",
    "            if method in standard_methods:\n",
    "                precision_values[method].append(row['Precision (%)'])\n",
    "                cost_values[method].append(row['Cost'])\n",
    "                recall_values[method].append(row['Recall (%)'])\n",
    "                treatment_time_values[method].append(row['Treatment Time'])\n",
    "    \n",
    "    # Compute statistics\n",
    "    final_data = []\n",
    "    for method in standard_methods:\n",
    "        if precision_values[method]:  # Check if we have data for this method\n",
    "            precision_mean = np.mean(precision_values[method])\n",
    "            precision_std = np.std(precision_values[method])\n",
    "            cost_mean = np.mean(cost_values[method])\n",
    "            cost_std = np.std(cost_values[method])\n",
    "            recall_mean = np.mean(recall_values[method])\n",
    "            recall_std = np.std(recall_values[method])\n",
    "            treat_time_mean = np.mean(treatment_time_values[method])\n",
    "            treat_time_std = np.std(treatment_time_values[method])\n",
    "            \n",
    "            final_data.append({\n",
    "                'Method': method,\n",
    "                'Precision (%)': f\"{precision_mean:.2f} ± {precision_std:.2f}\",\n",
    "                'Cost': f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "                'Recall (%)': f\"{recall_mean:.2f} ± {recall_std:.2f}\",\n",
    "                'Treatment Time': f\"{treat_time_mean:.2f} ± {treat_time_std:.2f}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(final_data)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 10. MAIN SCRIPT \n",
    "###############################################################################\n",
    "def main():\n",
    "   \n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    print(f\"Total patients: {df_all['patient_id'].nunique()}\")\n",
    "    print(f\"Columns: {list(df_all.columns)}\")\n",
    "\n",
    "    # 2) Run multiple replications\n",
    "    n_replications = 30\n",
    "    n_splits = 4\n",
    "    final_results = run_multiple_replications(df_all, n_replications=n_replications, n_splits=n_splits)\n",
    "    \n",
    "    # 3) Print final results\n",
    "    print(f\"\\n=== FINAL RESULTS (Mean ± Std Dev over {n_replications} Replications, Algorithm 3) ===\")\n",
    "    print(final_results.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#algorithm 5\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS & SETTINGS\n",
    "###############################################################################\n",
    "FP_COST = 10    # Penalty for false positive treatment\n",
    "FN_COST = 50    # Penalty for false negative (never treated but was sick)\n",
    "D_COST  = 1     # Penalty per time-step of delay before treating a sick patient\n",
    "GAMMA   = 0.99  # Discount factor\n",
    "T_MAX   = 20    # Time horizon (discrete steps 0..T_MAX-1 for each patient)\n",
    "\n",
    "FEATURE_COLS = [\"time\", \"EIT\", \"NIRS\", \"EIS\"]  # Adjust for your dataset columns\n",
    "\n",
    "###############################################################################\n",
    "# 2. DATA SPLITTING & HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def split_patients_kfold(df, n_splits=4, seed=0):\n",
    "    \"\"\"\n",
    "    Shuffle unique patient IDs, then split into n_splits+1 groups:\n",
    "       G1,...,G_{n_splits}, G_{n_splits+1} (the final holdout).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pts = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pts)\n",
    "    \n",
    "    n = len(unique_pts)\n",
    "    splits = {}\n",
    "    \n",
    "    for i in range(n_splits + 1):\n",
    "        start_idx = int(i * n / (n_splits + 1))\n",
    "        end_idx   = int((i + 1) * n / (n_splits + 1))\n",
    "        group_name = f\"G{i+1}\"\n",
    "        pid_subset = unique_pts[start_idx:end_idx]\n",
    "        splits[group_name] = set(pid_subset)\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    \"\"\"Return rows of df whose patient_id is in pid_set.\"\"\"\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "###############################################################################\n",
    "# 3. ML TRAINING & RISK-SCORE GENERATION\n",
    "###############################################################################\n",
    "def train_and_predict_model(\n",
    "    depth_val,\n",
    "    lr_val,\n",
    "    df_train,\n",
    "    df_val,\n",
    "    feature_cols=FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    Train a CatBoost model with specified (depth, learning_rate), \n",
    "    then return the predicted probabilities (risk scores) for df_val.\n",
    "    \"\"\"\n",
    "    X_train = df_train[feature_cols]\n",
    "    y_train = df_train['label']\n",
    "    \n",
    "    # We fix some CatBoost parameters for demonstration:\n",
    "    params = {\n",
    "        \"iterations\": 50,\n",
    "        \"depth\": int(round(depth_val)),  # ensure integer\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"verbose\": False,\n",
    "        \"random_seed\": 42,\n",
    "    }\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    X_val = df_val[feature_cols]\n",
    "    risk_scores = model.predict_proba(X_val)[:,1]  # Probability of label=1\n",
    "    return risk_scores\n",
    "\n",
    "###############################################################################\n",
    "# 4. POLICY SIMULATION & COST CALCULATION\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df has columns: [patient_id, time, label, predicted_risk, ...]\n",
    "    policy_func(patient_rows) -> integer time step to treat, or None if never treat.\n",
    "    Returns dict of {cost, precision, recall, avg_treatment_time}.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pid, patient_rows in df.groupby('patient_id'):\n",
    "        patient_rows = patient_rows.sort_values('time')\n",
    "        label = patient_rows['label'].iloc[0]  # 0 or 1\n",
    "        treat_time = policy_func(patient_rows)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treated\n",
    "            if label == 1:\n",
    "                cost = FN_COST  # missed sick\n",
    "            else:\n",
    "                cost = 0\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            treated_flag = 0\n",
    "            tt = None\n",
    "        else:\n",
    "            treated_flag = 1\n",
    "            # If label=1 => cost is D_COST * treat_time\n",
    "            # else => cost is FP_COST\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            tt = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treated_flag,\n",
    "            'treat_time': tt,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated'] == 1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df = df_res[df_res['label'] == 1]\n",
    "    total_sick = len(sick_df)\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt = valid_tt.mean() if len(valid_tt) > 0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 5. BENCHMARK POLICIES\n",
    "###############################################################################\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,0.5,8)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['predicted_risk'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_constant_threshold_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['predicted_risk'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=T_MAX,\n",
    "                                    threshold_candidates=[0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['predicted_risk'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec):\n",
    "                if row['predicted_risk'] >= thr_vec[t]:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "def linear_threshold_search(df, A_candidates=None, B_candidates=None):\n",
    "    if A_candidates is None:\n",
    "        A_candidates = np.linspace(-0.05, 0.05, 5)\n",
    "    if B_candidates is None:\n",
    "        B_candidates = np.linspace(0, 0.5, 6)\n",
    "    \n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A * t + B\n",
    "                    thr = np.clip(thr, 0, 1)\n",
    "                    if row['predicted_risk'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            \n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    return (best_A, best_B), best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A, B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A * t + B\n",
    "            thr = np.clip(thr, 0, 1)\n",
    "            if row['predicted_risk'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0, 1, 21)\n",
    "    best_thr, best_cost = None, float('inf')\n",
    "    best_stats = None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            # only check final time\n",
    "            final_row = patient_rows.loc[patient_rows['time'].idxmax()]\n",
    "            if final_row['predicted_risk'] >= thr:\n",
    "                return int(final_row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_row = patient_rows.loc[patient_rows['time'].idxmax()]\n",
    "        if final_row['predicted_risk'] >= thr:\n",
    "            return int(final_row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 6. DATA-DRIVEN DP (Unconstrained)\n",
    "###############################################################################\n",
    "def assign_buckets(prob, n_buckets=5):\n",
    "    edges = np.linspace(0, 1, n_buckets+1)\n",
    "    for b in range(n_buckets):\n",
    "        if prob >= edges[b] and prob < edges[b+1]:\n",
    "            return b\n",
    "    return n_buckets-1  # fallback\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    \"\"\"\n",
    "    df_train has columns: [patient_id, time, predicted_risk, label].\n",
    "    We build p_trans[t,b,b'] = Probability of (b' | b) from t->t+1\n",
    "    p_sick[t,b] = Probability that a patient in (t,b) is sick.\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    bucket_counts     = np.zeros((T, n_buckets))\n",
    "    sick_counts       = np.zeros((T, n_buckets))\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows = grp.to_dict('records')\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            lbl= int(row['label'])\n",
    "            if t < T:\n",
    "                bucket_counts[t, b] += 1\n",
    "                sick_counts[t, b]   += lbl\n",
    "            if i < len(rows) - 1:\n",
    "                row_next = rows[i+1]\n",
    "                t_next   = int(row_next['time'])\n",
    "                b_next   = int(row_next['risk_bucket'])\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t, b, b_next] += 1.0\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_, b_, :].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_, b_, :] = transition_counts[t_, b_, :] / denom\n",
    "            else:\n",
    "                p_trans[t_, b_, b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets))\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_, b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_, b_] = sick_counts[t_, b_] / denom\n",
    "            else:\n",
    "                p_sick[t_, b_] = 0.0\n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp(p_trans, p_sick, FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX):\n",
    "    \"\"\"\n",
    "    Standard backward recursion for a single-patient unconstrained DP.\n",
    "    V[t,b] = minimal future cost from time t, bucket b.\n",
    "    pi_[t,b] in {0,1} => 1 = treat, 0 = wait.\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # Terminal cost at t=T: treat or not treat if still in bucket b\n",
    "    for b in range(n_buckets):\n",
    "        # Because we only have T steps 0..T-1, interpret \"terminal\" at t=T carefully.\n",
    "        # We'll treat the cost if we decide to wait through time T-1 to the end:\n",
    "        cost_treat   = p_sick[T-1,b]*(D*(T-1)) + (1-p_sick[T-1,b])*FP\n",
    "        cost_notreat = p_sick[T-1,b]*FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    # backward recursion\n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # cost if treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1 - p_sick[t,b])*FP\n",
    "            \n",
    "            # cost if wait\n",
    "            if t == T-1:\n",
    "                exp_future = V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next]* V[t+1,b_next]\n",
    "            cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    return V, pi_\n",
    "\n",
    "def make_data_driven_dp_policy(V, pi_, T=T_MAX):\n",
    "    \"\"\"\n",
    "    Returns a function that decides treat (time) or wait based on DP policy.\n",
    "    \"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                action = pi_[t,b]\n",
    "                if action == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 7. SPSA OPTIMIZATION (ALGORITHM 5 CORE)\n",
    "###############################################################################\n",
    "def spsa_optimization(\n",
    "    df_train_fold,\n",
    "    df_val_fold,\n",
    "    n_iterations=20,\n",
    "    alpha=0.602,\n",
    "    gamma=0.101,\n",
    "    a0=0.2,\n",
    "    c0=0.1,\n",
    "    depth_init=4.0,\n",
    "    lr_init=0.1,\n",
    "    depth_bounds=(2.0, 8.0),\n",
    "    lr_bounds=(0.01, 0.3),\n",
    "    T=T_MAX\n",
    "):\n",
    "    \"\"\"\n",
    "    Run SPSA to minimize the \"actual cost\" from data-driven DP,\n",
    "    w.r.t. 2 continuous hyper-parameters: (depth, learning_rate).\n",
    "    \"\"\"\n",
    "    def evaluate_cost(depth_val, lr_val):\n",
    "        # 1) Train catboost on train_fold\n",
    "        # 2) Predict risk on val_fold\n",
    "        risk_scores = train_and_predict_model(\n",
    "            depth_val, lr_val,\n",
    "            df_train_fold, df_val_fold\n",
    "        )\n",
    "        df_val_eval = df_val_fold.copy()\n",
    "        df_val_eval[\"predicted_risk\"] = risk_scores\n",
    "        \n",
    "        # 3) data-driven DP transitions from df_train_fold\n",
    "        risk_train = train_and_predict_model(\n",
    "            depth_val, lr_val,\n",
    "            df_train_fold, df_train_fold\n",
    "        )\n",
    "        df_train_dp = df_train_fold.copy()\n",
    "        df_train_dp[\"predicted_risk\"] = risk_train\n",
    "        df_train_dp[\"risk_bucket\"]    = df_train_dp[\"predicted_risk\"].apply(assign_buckets)\n",
    "        \n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(df_train_dp, T=T, n_buckets=5)\n",
    "        V, pi_ = train_data_driven_dp(p_trans, p_sick, FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T)\n",
    "        \n",
    "        # apply DP policy to val\n",
    "        df_val_eval[\"risk_bucket\"] = df_val_eval[\"predicted_risk\"].apply(assign_buckets)\n",
    "        dp_policy = make_data_driven_dp_policy(V, pi_, T=T)\n",
    "        stats = simulate_policy(df_val_eval, dp_policy)\n",
    "        \n",
    "        return stats[\"cost\"]\n",
    "\n",
    "    #--- SPSA main loop ---\n",
    "    params = np.array([depth_init, lr_init], dtype=float)\n",
    "    \n",
    "    best_params = params.copy()\n",
    "    best_cost   = float('inf')\n",
    "    log_history = []\n",
    "    \n",
    "    for k in range(1, n_iterations+1):\n",
    "        a_k = 0.2 / (k ** 0.602)\n",
    "        c_k = 0.1 / (k ** 0.101)\n",
    "        \n",
    "        # random perturbation delta in {-1, +1}^2\n",
    "        delta = np.random.choice([-1,1], size=2)\n",
    "        \n",
    "        # param_plus, param_minus\n",
    "        params_plus  = params + c_k * delta\n",
    "        params_minus = params - c_k * delta\n",
    "        \n",
    "        # Clip to bounds\n",
    "        params_plus[0]  = np.clip(params_plus[0], depth_bounds[0], depth_bounds[1])\n",
    "        params_plus[1]  = np.clip(params_plus[1], lr_bounds[0],    lr_bounds[1])\n",
    "        params_minus[0] = np.clip(params_minus[0],depth_bounds[0], depth_bounds[1])\n",
    "        params_minus[1] = np.clip(params_minus[1],lr_bounds[0],    lr_bounds[1])\n",
    "        \n",
    "        # Evaluate cost\n",
    "        cost_plus  = evaluate_cost(params_plus[0],  params_plus[1])\n",
    "        cost_minus = evaluate_cost(params_minus[0], params_minus[1])\n",
    "        \n",
    "        # Gradient approx\n",
    "        g_k = (cost_plus - cost_minus)/(2.0*c_k) * delta\n",
    "        \n",
    "        # Update\n",
    "        params_new = params - a_k * g_k\n",
    "        \n",
    "        # Clip new param\n",
    "        params_new[0] = np.clip(params_new[0], depth_bounds[0], depth_bounds[1])\n",
    "        params_new[1] = np.clip(params_new[1], lr_bounds[0],    lr_bounds[1])\n",
    "        \n",
    "        # Evaluate cost at new param\n",
    "        cost_new = evaluate_cost(params_new[0], params_new[1])\n",
    "        \n",
    "        # Check if better\n",
    "        if cost_new < best_cost:\n",
    "            best_cost   = cost_new\n",
    "            best_params = params_new.copy()\n",
    "        \n",
    "        # Prepare next iteration\n",
    "        params = params_new\n",
    "        \n",
    "        log_history.append({\n",
    "            \"iter\": k,\n",
    "            \"params\": (params[0], params[1]),\n",
    "            \"cost_plus\": cost_plus,\n",
    "            \"cost_minus\": cost_minus,\n",
    "            \"cost_new\": cost_new,\n",
    "            \"best_cost_so_far\": best_cost\n",
    "        })\n",
    "    \n",
    "    return best_params, best_cost, pd.DataFrame(log_history)\n",
    "\n",
    "###############################################################################\n",
    "# 8. ALGORITHM 5 (SPSA) DRIVER\n",
    "###############################################################################\n",
    "def run_experiment_algorithm5(\n",
    "    df_all,\n",
    "    n_splits=4,\n",
    "    seed=42,\n",
    "    n_spsa_iters=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 5 (SPSA) with cross-validation for the unconstrained scenario.\n",
    "    Returns final_table (results on final holdout) and df_folds (CV details).\n",
    "    \"\"\"\n",
    "    # 1) Split data\n",
    "    splits = split_patients_kfold(df_all, n_splits=n_splits, seed=seed)\n",
    "    group_dfs = {}\n",
    "    for group_name, pid_set in splits.items():\n",
    "        sub_df = filter_by_group(df_all, pid_set)\n",
    "        group_dfs[group_name] = sub_df\n",
    "    \n",
    "    test_name = f\"G{n_splits+1}\"\n",
    "    df_test = group_dfs[test_name]\n",
    "    \n",
    "    # We'll store fold results\n",
    "    fold_records = []\n",
    "    \n",
    "    for i_val in range(1, n_splits+1):\n",
    "        val_name = f\"G{i_val}\"\n",
    "        df_val = group_dfs[val_name]\n",
    "        \n",
    "        # train folds = all except i_val (and except final G_{n+1})\n",
    "        train_sets = []\n",
    "        for j in range(1, n_splits+1):\n",
    "            if j != i_val:\n",
    "                train_sets.append(group_dfs[f\"G{j}\"])\n",
    "        df_train_fold = pd.concat(train_sets, ignore_index=True)\n",
    "        \n",
    "        # ---- Run SPSA to find best hyperparams on this fold\n",
    "        best_params, best_cost, df_log = spsa_optimization(\n",
    "            df_train_fold, df_val,\n",
    "            n_iterations=n_spsa_iters\n",
    "        )\n",
    "        depth_star, lr_star = best_params\n",
    "        \n",
    "        # Evaluate final cost on the validation set with those best params:\n",
    "        cost_val = best_cost\n",
    "        \n",
    "        # Optionally compute final AUC on the validation set\n",
    "        risk_scores_val = train_and_predict_model(\n",
    "            depth_star, lr_star,\n",
    "            df_train_fold, df_val\n",
    "        )\n",
    "        auc_val = roc_auc_score(df_val[\"label\"], risk_scores_val)\n",
    "        \n",
    "        fold_records.append({\n",
    "            \"fold\": i_val,\n",
    "            \"best_depth\": depth_star,\n",
    "            \"best_lr\": lr_star,\n",
    "            \"val_cost\": cost_val,\n",
    "            \"val_auc\": auc_val\n",
    "        })\n",
    "    \n",
    "    df_folds = pd.DataFrame(fold_records)\n",
    "    \n",
    "    # pick final hyperparams => e.g. the fold with the minimal val_cost\n",
    "    best_fold_idx = df_folds['val_cost'].idxmin()\n",
    "    best_depth    = df_folds.loc[best_fold_idx, 'best_depth']\n",
    "    best_lr       = df_folds.loc[best_fold_idx, 'best_lr']\n",
    "    \n",
    "    # Retrain final model on union of G1..G_n\n",
    "    train_all = []\n",
    "    for i in range(1, n_splits+1):\n",
    "        train_all.append(group_dfs[f\"G{i}\"])\n",
    "    df_train_all = pd.concat(train_all, ignore_index=True)\n",
    "    \n",
    "    # Final predictions on holdout G_{n_splits+1}\n",
    "    risk_test = train_and_predict_model(\n",
    "        best_depth, best_lr,\n",
    "        df_train_all, df_test\n",
    "    )\n",
    "    df_test_eval = df_test.copy()\n",
    "    df_test_eval[\"predicted_risk\"] = risk_test\n",
    "    \n",
    "    # We'll compare 5 approaches on final holdout:\n",
    "    # (1) Constant threshold\n",
    "    thr_c, stats_c = constant_threshold_search(df_test_eval)\n",
    "    # (2) Dynamic threshold\n",
    "    thr_vec, stats_dyn = dynamic_threshold_random_search(df_test_eval, seed=seed)\n",
    "    # (3) Linear threshold\n",
    "    (bestA,bestB), stats_lin = linear_threshold_search(df_test_eval)\n",
    "    # (4) Wait till end\n",
    "    thr_wte, stats_wte = wait_till_end_search(df_test_eval)\n",
    "    # (5) Data-driven DP\n",
    "    risk_train_final = train_and_predict_model(\n",
    "        best_depth, best_lr,\n",
    "        df_train_all, df_train_all\n",
    "    )\n",
    "    df_train_final_dp = df_train_all.copy()\n",
    "    df_train_final_dp[\"predicted_risk\"] = risk_train_final\n",
    "    df_train_final_dp[\"risk_bucket\"] = df_train_final_dp[\"predicted_risk\"].apply(assign_buckets)\n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_train_final_dp, T=T_MAX, n_buckets=5)\n",
    "    V, pi_ = train_data_driven_dp(p_trans, p_sick, FP=FP_COST, FN=FN_COST, D=D_COST, gamma=GAMMA, T=T_MAX)\n",
    "    \n",
    "    df_test_eval[\"risk_bucket\"] = df_test_eval[\"predicted_risk\"].apply(assign_buckets)\n",
    "    dp_policy_func = make_data_driven_dp_policy(V, pi_, T=T_MAX)\n",
    "    stats_dp = simulate_policy(df_test_eval, dp_policy_func)\n",
    "    \n",
    "    final_table = pd.DataFrame({\n",
    "        \"Method\": [\n",
    "            \"Constant Threshold\",\n",
    "            \"Dynamic Threshold-R\",\n",
    "            \"Linear Threshold\",\n",
    "            \"Wait Till End\",\n",
    "            \"Data-Driven DP (SPSA-catboost)\"\n",
    "        ],\n",
    "        \"Cost\": [\n",
    "            stats_c[\"cost\"],\n",
    "            stats_dyn[\"cost\"],\n",
    "            stats_lin[\"cost\"],\n",
    "            stats_wte[\"cost\"],\n",
    "            stats_dp[\"cost\"]\n",
    "        ],\n",
    "        \"Precision (%)\": [\n",
    "            100*stats_c[\"precision\"],\n",
    "            100*stats_dyn[\"precision\"],\n",
    "            100*stats_lin[\"precision\"],\n",
    "            100*stats_wte[\"precision\"],\n",
    "            100*stats_dp[\"precision\"]\n",
    "        ],\n",
    "        \"Recall (%)\": [\n",
    "            100*stats_c[\"recall\"],\n",
    "            100*stats_dyn[\"recall\"],\n",
    "            100*stats_lin[\"recall\"],\n",
    "            100*stats_wte[\"recall\"],\n",
    "            100*stats_dp[\"recall\"]\n",
    "        ],\n",
    "        \"Avg. Treat Time\": [\n",
    "            stats_c[\"avg_treatment_time\"],\n",
    "            stats_dyn[\"avg_treatment_time\"],\n",
    "            stats_lin[\"avg_treatment_time\"],\n",
    "            stats_wte[\"avg_treatment_time\"],\n",
    "            stats_dp[\"avg_treatment_time\"]\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return final_table, df_folds\n",
    "\n",
    "###############################################################################\n",
    "# 9. AGGREGATION FUNCTION FOR MULTIPLE REPLICATIONS\n",
    "###############################################################################\n",
    "def aggregate_results(list_of_tables):\n",
    "    \"\"\"\n",
    "    Given a list of final_table DataFrames (one per replication),\n",
    "    compute mean ± std for each method and each metric, then return\n",
    "    a single aggregated DataFrame.\n",
    "    \"\"\"\n",
    "    # We'll accumulate values for each method in a dictionary\n",
    "    data_accum = defaultdict(lambda: {\n",
    "        \"Cost\": [],\n",
    "        \"Precision (%)\": [],\n",
    "        \"Recall (%)\": [],\n",
    "        \"Avg. Treat Time\": []\n",
    "    })\n",
    "    \n",
    "    # Collect data across all replications\n",
    "    for df_table in list_of_tables:\n",
    "        for idx in range(len(df_table)):\n",
    "            row = df_table.iloc[idx]\n",
    "            method = row[\"Method\"]\n",
    "            data_accum[method][\"Cost\"].append(row[\"Cost\"])\n",
    "            data_accum[method][\"Precision (%)\"].append(row[\"Precision (%)\"])\n",
    "            data_accum[method][\"Recall (%)\"].append(row[\"Recall (%)\"])\n",
    "            data_accum[method][\"Avg. Treat Time\"].append(row[\"Avg. Treat Time\"])\n",
    "    \n",
    "    # Now compute mean ± std\n",
    "    results = []\n",
    "    method_order = [\n",
    "        \"Constant Threshold\",\n",
    "        \"Dynamic Threshold-R\",\n",
    "        \"Linear Threshold\",\n",
    "        \"Wait Till End\",\n",
    "        \"Data-Driven DP (SPSA-catboost)\"\n",
    "    ]\n",
    "    \n",
    "    for method in method_order:\n",
    "        metrics_dict = data_accum[method]\n",
    "        \n",
    "        cost_arr = np.array(metrics_dict[\"Cost\"])\n",
    "        prec_arr = np.array(metrics_dict[\"Precision (%)\"])\n",
    "        rec_arr  = np.array(metrics_dict[\"Recall (%)\"])\n",
    "        time_arr = np.array(metrics_dict[\"Avg. Treat Time\"])\n",
    "        \n",
    "        cost_mean,  cost_std  = cost_arr.mean(),  cost_arr.std()\n",
    "        prec_mean,  prec_std  = prec_arr.mean(),  prec_arr.std()\n",
    "        rec_mean,   rec_std   = rec_arr.mean(),   rec_arr.std()\n",
    "        time_mean,  time_std  = time_arr.mean(),  time_arr.std()\n",
    "        \n",
    "        cost_str  = f\"{cost_mean:.2f} ± {cost_std:.2f}\"\n",
    "        prec_str  = f\"{prec_mean:.2f} ± {prec_std:.2f}\"\n",
    "        rec_str   = f\"{rec_mean:.2f} ± {rec_std:.2f}\"\n",
    "        time_str  = f\"{time_mean:.2f} ± {time_std:.2f}\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Method\": method,\n",
    "            \"Precision (%)\": prec_str,\n",
    "            \"Cost\": cost_str,\n",
    "            \"Recall (%)\": rec_str,\n",
    "            \"Treatment Time\": time_str  # rename \"Avg. Treat Time\" -> \"Treatment Time\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "###############################################################################\n",
    "# 10. MAIN SCRIPT (Modified to run multiple seeds and report final table)\n",
    "###############################################################################\n",
    "def main():\n",
    "    # 1) Read CSV once\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    \n",
    "    # We will run 30 replications with different seeds (0..29)\n",
    "    n_replications = 30\n",
    "    final_tables = []\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        seed_val = rep\n",
    "        print(f\"\\nRunning replication {rep+1}/{n_replications} with seed={seed_val}\")\n",
    "        \n",
    "        # Run the entire Algorithm 5 pipeline\n",
    "        final_table, df_cv_details = run_experiment_algorithm5(\n",
    "            df_all=df_all,\n",
    "            n_splits=4,\n",
    "            seed=seed_val,\n",
    "            n_spsa_iters=20\n",
    "        )\n",
    "        \n",
    "        # Collect the final holdout results table\n",
    "        final_tables.append(final_table)\n",
    "    \n",
    "    # Aggregate results across all replications\n",
    "    df_agg = aggregate_results(final_tables)\n",
    "    \n",
    "    # Print the aggregated table\n",
    "    print(\"\\n=== FINAL RESULTS (Mean ± Std Dev over 30 Replications, Algorithm 5) ===\")\n",
    "    print(df_agg.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f6f74-17d2-4f34-8433-963c7f0cb962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7afa4-bf5b-4876-bef0-c149593581fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
