{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94415d-07c7-4138-b454-30b9dbe21ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1 size: 3150\n",
      "G2 size: 3150\n",
      "G3 size: 3150\n",
      "G4 size: 3150\n",
      "[ML] Best AUC on G2 = 0.9286, best params = (100, 5, 1)\n",
      "[ADP] Training Q-Network on G3 with TWO Markov transitions (healthy vs. sick) ...\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# File: adp_with_random_forest_markov_two_matrices.py\n",
    "#\n",
    "# Illustrative end-to-end code for:\n",
    "#   - Algorithm 0 (Standard Validation)\n",
    "#   - Random Forest ML model\n",
    "#   - Approximate Dynamic Programming (ADP) with capacity constraints\n",
    "#   - Time-varying risk buckets via TWO Markov transition matrices \n",
    "#       (one for healthy, one for sick)\n",
    "#   - Final metrics on G4: cost, average treatment time, recall, precision\n",
    "###############################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "###############################################################################\n",
    "# STEP 0: Data Import\n",
    "# Assume we have a CSV with 600 synthetic patients:\n",
    "#   columns = [patient_id, time, EIT, NIRS, EIS, label, ...]\n",
    "###############################################################################\n",
    "df = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "\n",
    "RANDOM_SEED_1 = 42\n",
    "RANDOM_SEED_2 = 999\n",
    "RANDOM_SEED_3 = 123\n",
    "\n",
    "###############################################################################\n",
    "# STEP 1: Data Partition -> (G1, G2, G3, G4)  (Algorithm 0)\n",
    "###############################################################################\n",
    "# We'll do a simple 50-50 for (G12 vs G34), then 50-50 again for (G1 vs G2),\n",
    "# and 50-50 for (G3 vs G4). \n",
    "###############################################################################\n",
    "\n",
    "G12, G34 = train_test_split(df, test_size=0.50, random_state=RANDOM_SEED_1, stratify=df['label'])\n",
    "G1, G2 = train_test_split(G12, test_size=0.50, random_state=RANDOM_SEED_2, stratify=G12['label'])\n",
    "G3, G4 = train_test_split(G34, test_size=0.50, random_state=RANDOM_SEED_3, stratify=G34['label'])\n",
    "\n",
    "print(f\"G1 size: {len(G1)}\")\n",
    "print(f\"G2 size: {len(G2)}\")\n",
    "print(f\"G3 size: {len(G3)}\")\n",
    "print(f\"G4 size: {len(G4)}\")\n",
    "\n",
    "###############################################################################\n",
    "# STEP 2: Train Random Forest on G1, pick best hyperparams by AUC on G2\n",
    "###############################################################################\n",
    "def compute_features(subdf):\n",
    "    \"\"\"\n",
    "    Basic feature extraction. \n",
    "    We'll just use [EIT, NIRS, EIS, time] as features.\n",
    "    Adjust as desired.\n",
    "    \"\"\"\n",
    "    feats = subdf[['EIT','NIRS','EIS','time']].values\n",
    "    return feats\n",
    "\n",
    "def prepare_data_for_ml(df_input):\n",
    "    X_ = compute_features(df_input)\n",
    "    y_ = df_input['label'].values\n",
    "    return X_, y_\n",
    "\n",
    "X1, y1 = prepare_data_for_ml(G1)\n",
    "X2, y2 = prepare_data_for_ml(G2)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "best_auc = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for n_est in param_grid['n_estimators']:\n",
    "    for md in param_grid['max_depth']:\n",
    "        for msl in param_grid['min_samples_leaf']:\n",
    "            rf_model = RandomForestClassifier(n_estimators=n_est,\n",
    "                                              max_depth=md,\n",
    "                                              min_samples_leaf=msl,\n",
    "                                              random_state=0)\n",
    "            rf_model.fit(X1, y1)\n",
    "            preds_proba = rf_model.predict_proba(X2)[:,1]\n",
    "            auc_val = roc_auc_score(y2, preds_proba)\n",
    "            if auc_val > best_auc:\n",
    "                best_auc = auc_val\n",
    "                best_params = (n_est, md, msl)\n",
    "\n",
    "print(f\"[ML] Best AUC on G2 = {best_auc:.4f}, best params = {best_params}\")\n",
    "\n",
    "X12 = np.vstack([X1, X2])\n",
    "y12 = np.hstack([y1, y2])\n",
    "\n",
    "final_rf_model = RandomForestClassifier(n_estimators=best_params[0],\n",
    "                                        max_depth=best_params[1],\n",
    "                                        min_samples_leaf=best_params[2],\n",
    "                                        random_state=0)\n",
    "final_rf_model.fit(X12, y12)\n",
    "\n",
    "###############################################################################\n",
    "# STEP 3: Generate risk scores for G3 (and discretize into buckets)\n",
    "###############################################################################\n",
    "def get_risk_scores(df_input, model):\n",
    "    X_, _ = prepare_data_for_ml(df_input)\n",
    "    risk_scores = model.predict_proba(X_)[:,1]\n",
    "    df_out = df_input.copy()\n",
    "    df_out['risk_score'] = risk_scores\n",
    "    return df_out\n",
    "\n",
    "def bucket_risk(score):\n",
    "    if score < 0.2: return 0\n",
    "    elif score < 0.4: return 1\n",
    "    elif score < 0.6: return 2\n",
    "    elif score < 0.8: return 3\n",
    "    else: return 4\n",
    "\n",
    "G3_scored = get_risk_scores(G3, final_rf_model)\n",
    "G3_scored['bucket'] = G3_scored['risk_score'].apply(bucket_risk)\n",
    "\n",
    "###############################################################################\n",
    "# STEP 4: Train ADP (Q-learning) on G3\n",
    "#\n",
    "# We define *two* Markov transition matrices, one for healthy (label=0)\n",
    "# and one for sick (label=1). \n",
    "# Then we proceed with the standard Q-learning setup.\n",
    "###############################################################################\n",
    "\n",
    "# Example cost parameters:\n",
    "FP = 10   # cost for false positive\n",
    "FN = 50   # cost for false negative\n",
    "D  = 1    # cost for per-step delay (if sick)\n",
    "gamma = 0.99\n",
    "T_max = 20\n",
    "\n",
    "# Suppose these are your two Markov transitions:\n",
    "# (A) For healthy patients (label=0):\n",
    "transition_matrix_healthy = np.array([\n",
    "    [0.60, 0.25, 0.10, 0.05, 0.00],  # from bucket 0\n",
    "    [0.10, 0.50, 0.30, 0.10, 0.00],  # from bucket 1\n",
    "    [0.05, 0.10, 0.50, 0.25, 0.10],  # from bucket 2\n",
    "    [0.00, 0.05, 0.20, 0.50, 0.25],  # from bucket 3\n",
    "    [0.00, 0.00, 0.10, 0.20, 0.70]   # from bucket 4\n",
    "])\n",
    "\n",
    "# (B) For sick patients (label=1):\n",
    "transition_matrix_sick = np.array([\n",
    "    [0.30, 0.30, 0.20, 0.15, 0.05],  # from bucket 0 (sick rarely in 0, but example)\n",
    "    [0.10, 0.30, 0.35, 0.20, 0.05],\n",
    "    [0.05, 0.05, 0.50, 0.30, 0.10],\n",
    "    [0.00, 0.05, 0.20, 0.40, 0.35],\n",
    "    [0.00, 0.00, 0.05, 0.25, 0.70]\n",
    "])\n",
    "\n",
    "num_sick_g3 = G3_scored['label'].sum()\n",
    "# For demonstration: capacity is half of the # of sick. \n",
    "# (Interpretation: we only can treat up to N_c patients total over the entire horizon.)\n",
    "N_c = int(0.5 * num_sick_g3) if num_sick_g3 > 0 else 0\n",
    "N_c = max(0, N_c)\n",
    "\n",
    "###############################################################################\n",
    "# (A) Define the environment with TWO Markov transitions\n",
    "###############################################################################\n",
    "class HemorrhageEnvAggregatedWithTransitions:\n",
    "    \"\"\"\n",
    "    An environment that tracks aggregated bucket counts, capacity usage,\n",
    "    and Markov transitions of buckets each time step for UNTREATED patients.\n",
    "    \n",
    "    Once a patient is 'treated', they remain out of the evolution process.\n",
    "    We incorporate false positives, false negatives, and per-step delay cost.\n",
    "\n",
    " \n",
    "    \"\"\"\n",
    "    def __init__(self, df_patients, capacity, max_time=20,\n",
    "                 transition_matrix_healthy=None,\n",
    "                 transition_matrix_sick=None):\n",
    "        \"\"\"\n",
    "        df_patients: Must have columns [patient_id, bucket, label, ...]\n",
    "        capacity: integer capacity (across entire horizon in this code)\n",
    "        max_time: total steps\n",
    "        transition_matrix_healthy: 5x5 array for healthy transitions\n",
    "        transition_matrix_sick:    5x5 array for sick transitions\n",
    "        \"\"\"\n",
    "        self.df = df_patients.copy()\n",
    "        self.capacity = capacity\n",
    "        self.max_time = max_time\n",
    "\n",
    "        # Two separate transition matrices\n",
    "        self.transition_matrix_healthy = transition_matrix_healthy\n",
    "        self.transition_matrix_sick = transition_matrix_sick\n",
    "\n",
    "        # Additional columns\n",
    "        self.patients = self.df[['patient_id','bucket','label']].copy()\n",
    "        self.patients['treated'] = 0\n",
    "        self.patients['treat_time'] = -1\n",
    "\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.patients['treated'] = 0\n",
    "        self.patients['treat_time'] = -1\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        return self._get_aggregated_state()\n",
    "\n",
    "    def _get_aggregated_state(self):\n",
    "        # How many have been treated so far?\n",
    "        treated_count = self.patients['treated'].sum()\n",
    "        cap_rem = max(0, self.capacity - treated_count)\n",
    "        bucket_counts = np.zeros(5, dtype=int)\n",
    "        # Count how many untreated in each bucket:\n",
    "        for b in range(5):\n",
    "            bucket_counts[b] = ((self.patients['bucket'] == b)\n",
    "                                & (self.patients['treated'] == 0)).sum()\n",
    "        # State is (count in bucket0..bucket4, cap_rem, time)\n",
    "        return (bucket_counts[0],\n",
    "                bucket_counts[1],\n",
    "                bucket_counts[2],\n",
    "                bucket_counts[3],\n",
    "                bucket_counts[4],\n",
    "                cap_rem,\n",
    "                self.t)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        1) Treat 'action' patients from highest bucket downward (subject to capacity).\n",
    "        2) Compute immediate cost:\n",
    "           - FP for newly treated healthy\n",
    "           - Per-step delay for sick still untreated\n",
    "           - FN for sick never treated if final step\n",
    "        3) Markov transition for all remaining untreated patients \n",
    "           using the appropriate matrix (healthy vs. sick).\n",
    "        4) Return new aggregated state, cost, done_flag\n",
    "        \"\"\"\n",
    "        global FP, FN, D  \n",
    "\n",
    "        if self.done:\n",
    "            # If already done, no changes\n",
    "            return self._get_aggregated_state(), 0.0, True\n",
    "\n",
    "        # Sort all untreated by bucket desc (treat the highest-risk buckets first)\n",
    "        df_untreated = self.patients[self.patients['treated'] == 0].copy()\n",
    "        df_untreated = df_untreated.sort_values('bucket', ascending=False)\n",
    "\n",
    "        # capacity used so far\n",
    "        treated_so_far = self.patients['treated'].sum()\n",
    "        cap_rem = max(0, self.capacity - treated_so_far)\n",
    "        # we can't treat more than capacity left or # of untreated\n",
    "        num_to_treat = min(action, cap_rem, len(df_untreated))\n",
    "\n",
    "        to_treat_indices = []\n",
    "        if num_to_treat > 0:\n",
    "            to_treat_indices = df_untreated.iloc[:num_to_treat].index\n",
    "            self.patients.loc[to_treat_indices, 'treated'] = 1\n",
    "            self.patients.loc[to_treat_indices, 'treat_time'] = self.t\n",
    "\n",
    "        newly_treated = self.patients.loc[to_treat_indices]\n",
    "\n",
    "        # cost for false positives\n",
    "        cost_fp = 0.0\n",
    "        for _, row in newly_treated.iterrows():\n",
    "            if row['label'] == 0:\n",
    "                cost_fp += FP\n",
    "\n",
    "        # delay cost for sick still untreated\n",
    "        df_sick_untreated = self.patients[(self.patients['label'] == 1)\n",
    "                                          & (self.patients['treated'] == 0)]\n",
    "        cost_delay = D * len(df_sick_untreated)\n",
    "\n",
    "        # if final step => false negatives for all sick untreated\n",
    "        done_next = False\n",
    "        cost_fn = 0.0\n",
    "        if self.t == (self.max_time - 1):\n",
    "            done_next = True\n",
    "            cost_fn = FN * len(df_sick_untreated)\n",
    "\n",
    "        immediate_cost = cost_fp + cost_delay + cost_fn\n",
    "\n",
    "        # increment time\n",
    "        self.t += 1\n",
    "        if self.t >= self.max_time:\n",
    "            done_next = True\n",
    "\n",
    "        # Markov transitions for all STILL untreated patients\n",
    "        if not done_next:\n",
    "            untreated_inds = self.patients[self.patients['treated'] == 0].index\n",
    "            for idx in untreated_inds:\n",
    "                current_b = self.patients.loc[idx, 'bucket']\n",
    "                lab = self.patients.loc[idx, 'label']\n",
    "                # pick appropriate transition matrix\n",
    "                if lab == 0:\n",
    "                    probs = self.transition_matrix_healthy[current_b, :]\n",
    "                else:\n",
    "                    probs = self.transition_matrix_sick[current_b, :]\n",
    "                new_b = np.random.choice(np.arange(5), p=probs)\n",
    "                self.patients.loc[idx, 'bucket'] = new_b\n",
    "\n",
    "        self.done = done_next\n",
    "        next_state = self._get_aggregated_state()\n",
    "        return next_state, immediate_cost, done_next\n",
    "\n",
    "###############################################################################\n",
    "# (B) Define Q-Network\n",
    "###############################################################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=7, action_dim=21, hidden=32):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, action_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        qvals = self.fc3(x)  # shape [batch_size, action_dim]\n",
    "        return qvals\n",
    "\n",
    "###############################################################################\n",
    "# (C) Q-Learning / ADP Loop\n",
    "###############################################################################\n",
    "def state_to_tensor(state):\n",
    "    return torch.tensor([state], dtype=torch.float32)\n",
    "\n",
    "def choose_action_epsilon_greedy(qnet, state, epsilon, max_action):\n",
    "    \"\"\"\n",
    "    Since we are minimizing cost, we pick argmin Q(s,a).\n",
    "    With epsilon probability, pick a random action among {0..max_action}.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, max_action+1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            s_t = state_to_tensor(state)\n",
    "            qvals = qnet(s_t).detach().numpy().flatten()\n",
    "            return np.argmin(qvals)\n",
    "\n",
    "def train_adp_on_G3(df_g3, capacity, max_time=20,\n",
    "                    transition_matrix_healthy=None,\n",
    "                    transition_matrix_sick=None,\n",
    "                    gamma=0.99,\n",
    "                    episodes=2000,\n",
    "                    learning_rate=1e-3,\n",
    "                    epsilon_start=0.2,\n",
    "                    epsilon_decay=0.999,\n",
    "                    batch_size=64,\n",
    "                    replay_size=50000):\n",
    "    \"\"\"\n",
    "    Train a Q-network (ADP) using transitions from the environment that\n",
    "    has two different Markov transition matrices (healthy vs. sick).\n",
    "    \"\"\"\n",
    "    env = HemorrhageEnvAggregatedWithTransitions(\n",
    "        df_patients=df_g3,\n",
    "        capacity=capacity,\n",
    "        max_time=max_time,\n",
    "        transition_matrix_healthy=transition_matrix_healthy,\n",
    "        transition_matrix_sick=transition_matrix_sick\n",
    "    )\n",
    "\n",
    "    max_action = capacity  # we allow actions in [0..capacity]\n",
    "    qnet = QNetwork(state_dim=7, action_dim=max_action+1, hidden=32)\n",
    "    optimizer = optim.Adam(qnet.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    replay_buffer = []\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    def get_target(r, gamma_, next_state, done_):\n",
    "        if done_:\n",
    "            return r\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                qvals_next = qnet(state_to_tensor(next_state)).detach().numpy().flatten()\n",
    "                # Minimizing cost => use min, not max\n",
    "                return r + gamma_ * np.min(qvals_next)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = choose_action_epsilon_greedy(qnet, s, epsilon, max_action)\n",
    "            s_next, cost, done = env.step(a)\n",
    "\n",
    "            # store transition\n",
    "            replay_buffer.append((s, a, cost, s_next, done))\n",
    "            if len(replay_buffer) > replay_size:\n",
    "                replay_buffer.pop(0)\n",
    "\n",
    "            s = s_next\n",
    "\n",
    "            # training step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch_indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "                states_b = []\n",
    "                actions_b = []\n",
    "                targets_b = []\n",
    "\n",
    "                for idx in batch_indices:\n",
    "                    st, ac, c_, sn, dn = replay_buffer[idx]\n",
    "                    y_ = get_target(c_, gamma, sn, dn)\n",
    "                    states_b.append(st)\n",
    "                    actions_b.append(ac)\n",
    "                    targets_b.append(y_)\n",
    "\n",
    "                states_t = torch.tensor(states_b, dtype=torch.float32)\n",
    "                actions_t = torch.tensor(actions_b, dtype=torch.long)\n",
    "                targets_t = torch.tensor(targets_b, dtype=torch.float32)\n",
    "\n",
    "                qvals_all = qnet(states_t)  # shape [batch_size, action_dim]\n",
    "                qvals_chosen = qvals_all.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                loss = loss_fn(qvals_chosen, targets_t)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Epsilon decay\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon, 0.01)\n",
    "\n",
    "    return qnet\n",
    "\n",
    "print(\"[ADP] Training Q-Network on G3 with TWO Markov transitions (healthy vs. sick) ...\")\n",
    "qnet_final = train_adp_on_G3(\n",
    "    df_g3=G3_scored,\n",
    "    capacity=N_c,\n",
    "    max_time=T_max,\n",
    "    transition_matrix_healthy=transition_matrix_healthy,\n",
    "    transition_matrix_sick=transition_matrix_sick,\n",
    "    gamma=gamma,\n",
    "    episodes=2000,\n",
    "    learning_rate=1e-3,\n",
    "    epsilon_start=0.2,\n",
    "    epsilon_decay=0.999,\n",
    "    batch_size=64\n",
    ")\n",
    "print(\"[ADP] Done training Q-network.\")\n",
    "\n",
    "###############################################################################\n",
    "# STEP 5: Evaluate final policy (RF risk + ADP) on G4\n",
    "###############################################################################\n",
    "\n",
    "# 1) Generate risk scores + bucket for G4\n",
    "G4_scored = get_risk_scores(G4, final_rf_model)\n",
    "G4_scored['bucket'] = G4_scored['risk_score'].apply(bucket_risk)\n",
    "\n",
    "def evaluate_policy_on_dataset(df_input, qnet, capacity, max_time=20,\n",
    "                               transition_matrix_healthy=None,\n",
    "                               transition_matrix_sick=None):\n",
    "    \"\"\"\n",
    "    Evaluate the learned policy by running the environment with Markov transitions\n",
    "    and a greedy policy w.r.t. Q(s,a). Minimizing cost => pick argmin Q(s,a).\n",
    "\n",
    "    Returns:\n",
    "      total_cost, avg_treatment_time, recall, precision\n",
    "    \"\"\"\n",
    "    env_eval = HemorrhageEnvAggregatedWithTransitions(\n",
    "        df_patients=df_input,\n",
    "        capacity=capacity,\n",
    "        max_time=max_time,\n",
    "        transition_matrix_healthy=transition_matrix_healthy,\n",
    "        transition_matrix_sick=transition_matrix_sick\n",
    "    )\n",
    "    s = env_eval.reset()\n",
    "    done = False\n",
    "    total_cost = 0.0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            s_t = state_to_tensor(s)\n",
    "            qvals = qnet(s_t).detach().numpy().flatten()\n",
    "            action = np.argmin(qvals)  # Minimizing cost\n",
    "        s_next, cost, done = env_eval.step(action)\n",
    "        total_cost += cost\n",
    "        s = s_next\n",
    "\n",
    "    # final metrics from env_eval\n",
    "    df_final = env_eval.patients\n",
    "    labels = df_final['label'].values\n",
    "    treated = df_final['treated'].values\n",
    "    treat_time = df_final['treat_time'].values\n",
    "\n",
    "    # basic classification outcomes\n",
    "    TP = np.sum((labels == 1) & (treated == 1))\n",
    "    FP_ = np.sum((labels == 0) & (treated == 1))\n",
    "    FN_ = np.sum((labels == 1) & (treated == 0))\n",
    "\n",
    "    recall = TP / (TP + FN_) if (TP + FN_) > 0 else 0.0\n",
    "    precision = TP / (TP + FP_) if (TP + FP_) > 0 else 0.0\n",
    "\n",
    "    # average treatment time among those treated\n",
    "    treated_mask = (treated == 1)\n",
    "    if treated_mask.sum() > 0:\n",
    "        avg_treat_time = treat_time[treated_mask].mean()\n",
    "    else:\n",
    "        avg_treat_time = -1\n",
    "\n",
    "    return total_cost, avg_treat_time, recall, precision\n",
    "\n",
    "final_cost_G4, avg_tt_G4, recall_G4, precision_G4 = evaluate_policy_on_dataset(\n",
    "    df_input=G4_scored,\n",
    "    qnet=qnet_final,\n",
    "    capacity=N_c,\n",
    "    max_time=T_max,\n",
    "    transition_matrix_healthy=transition_matrix_healthy,\n",
    "    transition_matrix_sick=transition_matrix_sick\n",
    ")\n",
    "\n",
    "print(\"===== EVALUATION on G4 (Two Markov transitions) =====\")\n",
    "print(f\"Cost: {final_cost_G4:.2f}\")\n",
    "print(f\"Avg. Treatment Time: {avg_tt_G4:.2f}\")\n",
    "print(f\"Recall: {recall_G4:.4f}\")\n",
    "print(f\"Precision: {precision_G4:.4f}\")\n",
    "\n",
    "# Optionally compute the final RF AUC on G4\n",
    "X4, y4 = prepare_data_for_ml(G4)\n",
    "proba4 = final_rf_model.predict_proba(X4)[:,1]\n",
    "auc_g4 = roc_auc_score(y4, proba4)\n",
    "print(f\"RandomForest AUC on G4: {auc_g4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131a411-54d8-4342-b62f-ba732f094c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# File: adp_with_aggregated_markov.py\n",
    "#\n",
    "# Illustrative code for:\n",
    "#   - Algorithm 0 (Standard Validation)\n",
    "#   - Random Forest ML for risk\n",
    "#   - Aggregated Markov transitions (bucket-level)\n",
    "#   - Capacitated ADP (Q-learning) with approximate cost-to-go\n",
    "#   - Final metrics on G4: cost, treatment time (approx), recall, precision\n",
    "###############################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "###############################################################################\n",
    "# STEP 0: Data Import\n",
    "###############################################################################\n",
    "df = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "\n",
    "RANDOM_SEED_1 = 42\n",
    "RANDOM_SEED_2 = 999\n",
    "RANDOM_SEED_3 = 123\n",
    "\n",
    "###############################################################################\n",
    "# STEP 1: Data Partition -> (G1, G2, G3, G4) (Algorithm 0)\n",
    "###############################################################################\n",
    "G12, G34 = train_test_split(df, test_size=0.50, random_state=RANDOM_SEED_1, stratify=df['label'])\n",
    "G1, G2 = train_test_split(G12, test_size=0.50, random_state=RANDOM_SEED_2, stratify=G12['label'])\n",
    "G3, G4 = train_test_split(G34, test_size=0.50, random_state=RANDOM_SEED_3, stratify=G34['label'])\n",
    "\n",
    "print(f\"G1 size: {len(G1)}\")\n",
    "print(f\"G2 size: {len(G2)}\")\n",
    "print(f\"G3 size: {len(G3)}\")\n",
    "print(f\"G4 size: {len(G4)}\")\n",
    "\n",
    "###############################################################################\n",
    "# STEP 2: Train Random Forest on G1, pick best hyperparams by AUC on G2\n",
    "###############################################################################\n",
    "def compute_features(subdf):\n",
    "    \"\"\" Example feature extraction; adjust as needed. \"\"\"\n",
    "    feats = subdf[['EIT','NIRS','EIS','time']].values\n",
    "    return feats\n",
    "\n",
    "def prepare_data_for_ml(df_input):\n",
    "    X_ = compute_features(df_input)\n",
    "    y_ = df_input['label'].values\n",
    "    return X_, y_\n",
    "\n",
    "X1, y1 = prepare_data_for_ml(G1)\n",
    "X2, y2 = prepare_data_for_ml(G2)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "best_auc = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for n_est in param_grid['n_estimators']:\n",
    "    for md in param_grid['max_depth']:\n",
    "        for msl in param_grid['min_samples_leaf']:\n",
    "            rf_model = RandomForestClassifier(n_estimators=n_est,\n",
    "                                              max_depth=md,\n",
    "                                              min_samples_leaf=msl,\n",
    "                                              random_state=0)\n",
    "            rf_model.fit(X1, y1)\n",
    "            preds_proba = rf_model.predict_proba(X2)[:,1]\n",
    "            auc_val = roc_auc_score(y2, preds_proba)\n",
    "            if auc_val > best_auc:\n",
    "                best_auc = auc_val\n",
    "                best_params = (n_est, md, msl)\n",
    "\n",
    "print(f\"[ML] Best AUC on G2 = {best_auc:.4f}, best params = {best_params}\")\n",
    "\n",
    "# Retrain on G1+G2\n",
    "X12 = np.vstack([X1, X2])\n",
    "y12 = np.hstack([y1, y2])\n",
    "final_rf_model = RandomForestClassifier(n_estimators=best_params[0],\n",
    "                                        max_depth=best_params[1],\n",
    "                                        min_samples_leaf=best_params[2],\n",
    "                                        random_state=0)\n",
    "final_rf_model.fit(X12, y12)\n",
    "\n",
    "###############################################################################\n",
    "# STEP 3: Generate risk scores + buckets for G3\n",
    "###############################################################################\n",
    "def get_risk_scores(df_input, model):\n",
    "    X_, _ = prepare_data_for_ml(df_input)\n",
    "    risk_scores = model.predict_proba(X_)[:,1]\n",
    "    df_out = df_input.copy()\n",
    "    df_out['risk_score'] = risk_scores\n",
    "    return df_out\n",
    "\n",
    "def bucket_risk(score):\n",
    "    if score < 0.2: return 0\n",
    "    elif score < 0.4: return 1\n",
    "    elif score < 0.6: return 2\n",
    "    elif score < 0.8: return 3\n",
    "    else: return 4\n",
    "\n",
    "G3_scored = get_risk_scores(G3, final_rf_model)\n",
    "G3_scored['bucket'] = G3_scored['risk_score'].apply(bucket_risk)\n",
    "\n",
    "###############################################################################\n",
    "# STEP 4: ADP (Q-learning) on aggregated Markov transitions\n",
    "###############################################################################\n",
    "\n",
    "# Cost parameters\n",
    "FP = 10\n",
    "FN = 50\n",
    "D  = 1\n",
    "gamma = 0.99\n",
    "T_max = 20\n",
    "\n",
    "# Resource capacity\n",
    "num_sick_g3 = G3_scored['label'].sum()\n",
    "N_c = int(0.5 * num_sick_g3) if num_sick_g3>0 else 0\n",
    "N_c = max(0, N_c)\n",
    "\n",
    "###############################################################################\n",
    "# (A) Aggregated Markov environment\n",
    "###############################################################################\n",
    "class AggregatedMarkovEnv:\n",
    "    \"\"\"\n",
    "    Keeps track of #healthy and #sick in each of 5 buckets: \n",
    "       bH[i], bS[i], for i=0..4\n",
    "    Plus capacity usage and time.\n",
    "\n",
    "    We do transitions as *expected* flows:\n",
    "      new_bH[j] = sum_{i} (bH[i] * P_H[i,j])  except for those removed by action\n",
    "      similarly for bS.\n",
    "    We'll treat only from the highest-risk buckets downward, \n",
    "    removing either healthy or sick from that bucket in proportion to \n",
    "    the fraction of healthy vs. sick in that bucket.\n",
    "    \n",
    "    This is an approximation that avoids enumerating individual patients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_patients, capacity, max_time=20,\n",
    "                 transition_mat_healthy=None,\n",
    "                 transition_mat_sick=None):\n",
    "        \"\"\"\n",
    "        df_patients: must have columns [bucket, label]\n",
    "        capacity: total resource capacity\n",
    "        max_time: horizon\n",
    "        transition_mat_healthy: 5x5 matrix for healthy transitions\n",
    "        transition_mat_sick:    5x5 matrix for sick transitions\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.max_time = max_time\n",
    "        self.transition_mat_h = transition_mat_healthy\n",
    "        self.transition_mat_s = transition_mat_sick\n",
    "\n",
    "        # We compute aggregated counts:\n",
    "        # bH[i] = how many healthy in bucket i\n",
    "        # bS[i] = how many sick in bucket i\n",
    "        bH = np.zeros(5, dtype=float)\n",
    "        bS = np.zeros(5, dtype=float)\n",
    "\n",
    "        for i in range(5):\n",
    "            # healthy in bucket i\n",
    "            cond = (df_patients['bucket']==i) & (df_patients['label']==0)\n",
    "            bH[i] = cond.sum()\n",
    "\n",
    "            # sick in bucket i\n",
    "            cond = (df_patients['bucket']==i) & (df_patients['label']==1)\n",
    "            bS[i] = cond.sum()\n",
    "\n",
    "        self.bH = bH\n",
    "        self.bS = bS\n",
    "\n",
    "        # how many are \"treated\" so far => we do NOT free capacity in this example\n",
    "        self.treated_so_far = 0.0\n",
    "        # approximate \"treatment time\" tracking:\n",
    "        # We'll store a running sum of ( # newly treated * time ), then \n",
    "        # average later for treated patients. This is an approximation in aggregated form.\n",
    "        self.cumulative_treatment_time = 0.0\n",
    "\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        # No real \"reset\" to random for now; re-init everything from scratch if needed\n",
    "        self.treated_so_far = 0.0\n",
    "        self.cumulative_treatment_time = 0.0\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        cap_rem = max(0, self.capacity - self.treated_so_far)\n",
    "        # state is (bH0..bH4, bS0..bS4, cap_rem, t)\n",
    "        # we'll flatten bH,bS\n",
    "        state = np.concatenate([self.bH, self.bS, [cap_rem, self.t]])\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        1) Treat 'action' patients from the top buckets (4..0).\n",
    "           If bucket i has bH[i] healthy and bS[i] sick => total_i = bH[i]+bS[i].\n",
    "           - The fraction that are healthy is bH[i]/total_i, sick is bS[i]/total_i.\n",
    "           - We'll remove 'treated' from that bucket up to min(action_left, total_i).\n",
    "        2) Compute immediate cost:\n",
    "           - FP = 10 * (# healthy treated)\n",
    "           - Delay = 1 * (sum of bS[i] for all i after action)\n",
    "           - FN at final step = 50 * (sum of bS[i])\n",
    "        3) Markov transitions (expected) for bH, bS if not final step\n",
    "        4) Return (next_state, cost, done)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.done:\n",
    "            return self._get_state(), 0.0, True\n",
    "\n",
    "        # 1) Treat from top bucket down\n",
    "        act_left = action\n",
    "        # how many actually get treated\n",
    "        actually_treated = 0.0\n",
    "        treated_healthy = 0.0\n",
    "        treated_sick = 0.0\n",
    "\n",
    "        # We'll iterate from bucket=4 down to 0\n",
    "        for bucket_i in reversed(range(5)):\n",
    "            if act_left <= 0:\n",
    "                break\n",
    "            total_in_bucket = self.bH[bucket_i] + self.bS[bucket_i]\n",
    "            if total_in_bucket <= 1e-9:\n",
    "                continue\n",
    "\n",
    "            # can treat up to min(act_left, total_in_bucket)\n",
    "            can_treat = min(act_left, total_in_bucket)\n",
    "\n",
    "            # fraction healthy vs sick in that bucket\n",
    "            fracH = self.bH[bucket_i]/total_in_bucket\n",
    "            fracS = self.bS[bucket_i]/total_in_bucket\n",
    "\n",
    "            # remove from bH, bS\n",
    "            treated_H_i = can_treat * fracH\n",
    "            treated_S_i = can_treat * fracS\n",
    "            self.bH[bucket_i] -= treated_H_i\n",
    "            self.bS[bucket_i] -= treated_S_i\n",
    "\n",
    "            treated_healthy += treated_H_i\n",
    "            treated_sick += treated_S_i\n",
    "            actually_treated += can_treat\n",
    "\n",
    "            act_left -= can_treat\n",
    "\n",
    "        # update capacity usage (we don't free capacity in the horizon)\n",
    "        self.treated_so_far += actually_treated\n",
    "        # approximate average treatment time tracking:\n",
    "        # add sum(# newly treated * current_t)\n",
    "        self.cumulative_treatment_time += actually_treated * self.t\n",
    "\n",
    "        # 2) immediate cost\n",
    "        cost_fp = FP * treated_healthy\n",
    "        # delay = D * (sum of bS across all buckets)\n",
    "        sum_sick_still_untreated = self.bS.sum()\n",
    "        cost_delay = D * sum_sick_still_untreated\n",
    "\n",
    "        # if final step, cost_fn for all sick left\n",
    "        cost_fn = 0.0\n",
    "        done_next = False\n",
    "        if self.t == (self.max_time - 1):\n",
    "            done_next = True\n",
    "            cost_fn = FN * sum_sick_still_untreated\n",
    "\n",
    "        immediate_cost = cost_fp + cost_delay + cost_fn\n",
    "\n",
    "        # 3) Markov transitions if not done\n",
    "        self.t += 1\n",
    "        if self.t >= self.max_time:\n",
    "            done_next = True\n",
    "\n",
    "        if not done_next:\n",
    "            # expected transitions\n",
    "            new_bH = np.zeros(5, dtype=float)\n",
    "            new_bS = np.zeros(5, dtype=float)\n",
    "\n",
    "            # for healthy\n",
    "            for i in range(5):\n",
    "                if self.bH[i] > 1e-9:\n",
    "                    for j in range(5):\n",
    "                        new_bH[j] += self.bH[i]*self.transition_mat_h[i,j]\n",
    "            # for sick\n",
    "            for i in range(5):\n",
    "                if self.bS[i] > 1e-9:\n",
    "                    for j in range(5):\n",
    "                        new_bS[j] += self.bS[i]*self.transition_mat_s[i,j]\n",
    "\n",
    "            self.bH = new_bH\n",
    "            self.bS = new_bS\n",
    "\n",
    "        self.done = done_next\n",
    "        next_state = self._get_state()\n",
    "        return next_state, immediate_cost, done_next\n",
    "\n",
    "    def get_avg_treatment_time(self):\n",
    "        \"\"\"\n",
    "        Approx average time = cumulative_treatment_time / total_treated\n",
    "        \"\"\"\n",
    "        total_treated = self.treated_so_far\n",
    "        if total_treated < 1e-9:\n",
    "            return -1\n",
    "        return self.cumulative_treatment_time / total_treated\n",
    "\n",
    "###############################################################################\n",
    "# (B) Example transition matrices for healthy vs. sick\n",
    "# You can estimate these from data or set them manually\n",
    "###############################################################################\n",
    "transition_mat_healthy = np.array([\n",
    "    [0.70, 0.20, 0.10, 0.00, 0.00],  # bucket 0 -> ...\n",
    "    [0.10, 0.70, 0.15, 0.05, 0.00],\n",
    "    [0.05, 0.10, 0.65, 0.15, 0.05],\n",
    "    [0.00, 0.05, 0.15, 0.60, 0.20],\n",
    "    [0.00, 0.00, 0.05, 0.20, 0.75]\n",
    "])\n",
    "transition_mat_sick = np.array([\n",
    "    [0.50, 0.30, 0.15, 0.05, 0.00],\n",
    "    [0.05, 0.50, 0.25, 0.15, 0.05],\n",
    "    [0.00, 0.10, 0.50, 0.30, 0.10],\n",
    "    [0.00, 0.00, 0.10, 0.60, 0.30],\n",
    "    [0.00, 0.00, 0.05, 0.25, 0.70]\n",
    "])\n",
    "\n",
    "###############################################################################\n",
    "# (C) Q-Network\n",
    "###############################################################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=12, action_dim=21, hidden=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, action_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        qvals = self.fc3(x)  # shape [batch_size, action_dim]\n",
    "        return qvals\n",
    "\n",
    "###############################################################################\n",
    "# (D) Q-Learning / ADP training\n",
    "###############################################################################\n",
    "def state_to_tensor(state):\n",
    "    # state shape = (bH0..bH4, bS0..bS4, cap_rem, t) => length=10 + 1 + 1 = 12\n",
    "    return torch.tensor([state], dtype=torch.float32)\n",
    "\n",
    "def choose_action_epsilon_greedy(qnet, state, epsilon, max_action):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, max_action+1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            s_t = state_to_tensor(state)\n",
    "            qvals = qnet(s_t).numpy().flatten()\n",
    "            # Minimizing cost => pick argmin\n",
    "            return np.argmin(qvals)\n",
    "\n",
    "def train_adp_on_G3(df_g3, capacity, max_time=20,\n",
    "                    trans_mat_h=None, trans_mat_s=None,\n",
    "                    gamma=0.99,\n",
    "                    episodes=2000,\n",
    "                    learning_rate=1e-3,\n",
    "                    epsilon_start=0.2,\n",
    "                    epsilon_decay=0.999,\n",
    "                    batch_size=64,\n",
    "                    replay_size=20000):\n",
    "    env = AggregatedMarkovEnv(df_patients=df_g3,\n",
    "                              capacity=capacity,\n",
    "                              max_time=max_time,\n",
    "                              transition_mat_healthy=trans_mat_h,\n",
    "                              transition_mat_sick=trans_mat_s)\n",
    "    # actions = 0..capacity\n",
    "    max_action = capacity\n",
    "    # state_dim=12 => bH[5], bS[5], cap_rem, t\n",
    "    qnet = QNetwork(state_dim=12, action_dim=max_action+1, hidden=64)\n",
    "    optimizer = optim.Adam(qnet.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    replay_buffer = []\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    def get_target(r, gamma_, next_state, done_):\n",
    "        if done_:\n",
    "            return r\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                qvals_next = qnet(state_to_tensor(next_state)).numpy().flatten()\n",
    "                return r + gamma_ * np.min(qvals_next)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        # \"reset\" the environment\n",
    "        env.__init__(df_g3, capacity, max_time, trans_mat_h, trans_mat_s)\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = choose_action_epsilon_greedy(qnet, s, epsilon, max_action)\n",
    "            s_next, cost, done = env.step(a)\n",
    "\n",
    "            # store\n",
    "            replay_buffer.append((s, a, cost, s_next, done))\n",
    "            if len(replay_buffer) > replay_size:\n",
    "                replay_buffer.pop(0)\n",
    "\n",
    "            s = s_next\n",
    "\n",
    "            # training step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch_indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "                states_b = []\n",
    "                actions_b = []\n",
    "                targets_b = []\n",
    "                for idx in batch_indices:\n",
    "                    st, ac, c_, sn, dn = replay_buffer[idx]\n",
    "                    y_ = get_target(c_, gamma, sn, dn)\n",
    "                    states_b.append(st)\n",
    "                    actions_b.append(ac)\n",
    "                    targets_b.append(y_)\n",
    "\n",
    "                states_t = torch.tensor(states_b, dtype=torch.float32)\n",
    "                actions_t = torch.tensor(actions_b, dtype=torch.long)\n",
    "                targets_t = torch.tensor(targets_b, dtype=torch.float32)\n",
    "\n",
    "                qvals_all = qnet(states_t)  # shape [batch_size, action_dim]\n",
    "                qvals_chosen = qvals_all.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                loss = loss_fn(qvals_chosen, targets_t)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # end of episode: epsilon decay\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon, 0.01)\n",
    "\n",
    "    return qnet\n",
    "\n",
    "print(\"[ADP] Training Q-Network on G3 (aggregated Markov) ...\")\n",
    "qnet_final = train_adp_on_G3(\n",
    "    df_g3=G3_scored,\n",
    "    capacity=N_c,\n",
    "    max_time=T_max,\n",
    "    trans_mat_h=transition_mat_healthy,\n",
    "    trans_mat_s=transition_mat_sick,\n",
    "    gamma=gamma,\n",
    "    episodes=2000,\n",
    "    learning_rate=1e-3,\n",
    "    epsilon_start=0.2,\n",
    "    epsilon_decay=0.999,\n",
    "    batch_size=64\n",
    ")\n",
    "print(\"[ADP] Done training Q-network.\")\n",
    "\n",
    "###############################################################################\n",
    "# STEP 5: Evaluate final policy on G4\n",
    "###############################################################################\n",
    "G4_scored = get_risk_scores(G4, final_rf_model)\n",
    "G4_scored['bucket'] = G4_scored['risk_score'].apply(bucket_risk)\n",
    "\n",
    "def evaluate_policy_aggregated(df_input, qnet, capacity, max_time=20,\n",
    "                               trans_mat_h=None, trans_mat_s=None):\n",
    "    \"\"\"\n",
    "    Evaluate the learned policy on a new dataset (G4) in aggregated form.\n",
    "    Returns (cost, avg_treatment_time, recall, precision).\n",
    "    \"\"\"\n",
    "    # 1) Build environment for G4\n",
    "    env_eval = AggregatedMarkovEnv(df_patients=df_input,\n",
    "                                   capacity=capacity,\n",
    "                                   max_time=max_time,\n",
    "                                   transition_mat_healthy=trans_mat_h,\n",
    "                                   transition_mat_sick=trans_mat_s)\n",
    "    s = env_eval.reset()\n",
    "    done = False\n",
    "    total_cost = 0.0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            s_t = state_to_tensor(s)\n",
    "            qvals = qnet(s_t).numpy().flatten()\n",
    "            action = np.argmin(qvals)\n",
    "        s_next, cost, done = env_eval.step(action)\n",
    "        total_cost += cost\n",
    "        s = s_next\n",
    "\n",
    "    # Then compute approximate recall, precision, avg_treatment_time, etc.\n",
    "    # Because we used aggregated counts, we can interpret:\n",
    "    #   total_healthy = sum(env_eval.bH) + (treated_healthy?), \n",
    "    #   total_sick = sum(env_eval.bS) + ...\n",
    "    # We'll approximate final outcomes from the environment's final bH,bS \n",
    "    # and the \"treated_healthy\" & \"treated_sick\" we found over time.\n",
    "\n",
    "    # In this code, we only have final bH, bS. \n",
    "    # The environment doesn't explicitly track \"treated_healthy\" vs. \"treated_sick\" \n",
    "    # as separate accumulators, so let's approximate them from:\n",
    "    #   total_healthy_init, total_sick_init - final remain = treated\n",
    "    total_healthy_init = 0.0\n",
    "    total_sick_init = 0.0\n",
    "    for i in range(5):\n",
    "        # how many healthy in i, sick in i at start\n",
    "        cond_h = (df_input['bucket']==i)&(df_input['label']==0)\n",
    "        cond_s = (df_input['bucket']==i)&(df_input['label']==1)\n",
    "        total_healthy_init += cond_h.sum()\n",
    "        total_sick_init += cond_s.sum()\n",
    "\n",
    "    final_healthy_remain = env_eval.bH.sum()\n",
    "    final_sick_remain = env_eval.bS.sum()\n",
    "\n",
    "    treated_healthy = total_healthy_init - final_healthy_remain\n",
    "    treated_sick = total_sick_init - final_sick_remain\n",
    "\n",
    "    # precision = TP/(TP+FP) => TP = treated_sick, FP = treated_healthy\n",
    "    if (treated_sick+treated_healthy) <= 1e-9:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = treated_sick/(treated_sick+treated_healthy)\n",
    "    # recall = TP/(TP+FN) => FN = final_sick_remain\n",
    "    if (treated_sick+final_sick_remain) <= 1e-9:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = treated_sick/(treated_sick+final_sick_remain)\n",
    "\n",
    "    avg_treat_time = env_eval.get_avg_treatment_time()\n",
    "\n",
    "    return total_cost, avg_treat_time, recall, precision\n",
    "\n",
    "final_cost_G4, avg_tt_G4, recall_G4, precision_G4 = evaluate_policy_aggregated(\n",
    "    df_input=G4_scored,\n",
    "    qnet=qnet_final,\n",
    "    capacity=N_c,\n",
    "    max_time=T_max,\n",
    "    trans_mat_h=transition_mat_healthy,\n",
    "    trans_mat_s=transition_mat_sick\n",
    ")\n",
    "\n",
    "print(\"===== EVALUATION on G4 (aggregated Markov) =====\")\n",
    "print(f\"Cost: {final_cost_G4:.2f}\")\n",
    "print(f\"Avg. Treatment Time: {avg_tt_G4:.2f}\")\n",
    "print(f\"Recall: {recall_G4:.4f}\")\n",
    "print(f\"Precision: {precision_G4:.4f}\")\n",
    "\n",
    "# Optionally, random forest AUC on G4\n",
    "X4, y4 = prepare_data_for_ml(G4)\n",
    "proba4 = final_rf_model.predict_proba(X4)[:,1]\n",
    "auc_g4 = roc_auc_score(y4, proba4)\n",
    "print(f\"RandomForest AUC on G4: {auc_g4:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
