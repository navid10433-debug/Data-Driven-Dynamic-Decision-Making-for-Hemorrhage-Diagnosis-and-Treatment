{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736867fd-57b0-493b-8b6c-f128401a9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running replication 1/30 with seed=0\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 5, 'n_estimators': 100}, AUC=0.8552\n",
      "Best DP gamma on G3 = 0.99, cost=470.00\n",
      "\n",
      "Running replication 2/30 with seed=1\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 100, 'learning_rate': 0.05}, AUC=0.8525\n",
      "Best DP gamma on G3 = 0.99, cost=327.00\n",
      "\n",
      "Running replication 3/30 with seed=2\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.05}, AUC=0.8619\n",
      "Best DP gamma on G3 = 0.99, cost=303.00\n",
      "\n",
      "Running replication 4/30 with seed=3\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8734\n",
      "Best DP gamma on G3 = 0.99, cost=340.00\n",
      "\n",
      "Running replication 5/30 with seed=4\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 5, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8339\n",
      "Best DP gamma on G3 = 0.99, cost=267.00\n",
      "\n",
      "Running replication 6/30 with seed=5\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 5, 'n_estimators': 100}, AUC=0.8459\n",
      "Best DP gamma on G3 = 0.99, cost=410.00\n",
      "\n",
      "Running replication 7/30 with seed=6\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8561\n",
      "Best DP gamma on G3 = 0.99, cost=251.00\n",
      "\n",
      "Running replication 8/30 with seed=7\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): GradientBoosting_{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}, AUC=0.8279\n",
      "Best DP gamma on G3 = 0.99, cost=384.00\n",
      "\n",
      "Running replication 9/30 with seed=8\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 3, 'n_estimators': 50}, AUC=0.8561\n",
      "Best DP gamma on G3 = 0.99, cost=291.00\n",
      "\n",
      "Running replication 10/30 with seed=9\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 3, 'n_estimators': 100}, AUC=0.8675\n",
      "Best DP gamma on G3 = 0.99, cost=316.00\n",
      "\n",
      "Running replication 11/30 with seed=10\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 5, 'iterations': 50, 'learning_rate': 0.05}, AUC=0.8754\n",
      "Best DP gamma on G3 = 0.99, cost=353.00\n",
      "\n",
      "Running replication 12/30 with seed=11\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): GradientBoosting_{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}, AUC=0.8249\n",
      "Best DP gamma on G3 = 0.99, cost=184.00\n",
      "\n",
      "Running replication 13/30 with seed=12\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.05}, AUC=0.8375\n",
      "Best DP gamma on G3 = 0.99, cost=248.00\n",
      "\n",
      "Running replication 14/30 with seed=13\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 3, 'n_estimators': 100}, AUC=0.8463\n",
      "Best DP gamma on G3 = 0.99, cost=289.00\n",
      "\n",
      "Running replication 15/30 with seed=14\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 5, 'n_estimators': 50}, AUC=0.8181\n",
      "Best DP gamma on G3 = 0.99, cost=341.00\n",
      "\n",
      "Running replication 16/30 with seed=15\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8687\n",
      "Best DP gamma on G3 = 0.99, cost=247.00\n",
      "\n",
      "Running replication 17/30 with seed=16\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 5, 'iterations': 100, 'learning_rate': 0.05}, AUC=0.8568\n",
      "Best DP gamma on G3 = 0.99, cost=423.00\n",
      "\n",
      "Running replication 18/30 with seed=17\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 100, 'learning_rate': 0.05}, AUC=0.8375\n",
      "Best DP gamma on G3 = 0.99, cost=220.00\n",
      "\n",
      "Running replication 19/30 with seed=18\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 100, 'learning_rate': 0.1}, AUC=0.8540\n",
      "Best DP gamma on G3 = 0.99, cost=311.00\n",
      "\n",
      "Running replication 20/30 with seed=19\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8258\n",
      "Best DP gamma on G3 = 0.99, cost=299.00\n",
      "\n",
      "Running replication 21/30 with seed=20\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 5, 'iterations': 50, 'learning_rate': 0.05}, AUC=0.8593\n",
      "Best DP gamma on G3 = 0.99, cost=286.00\n",
      "\n",
      "Running replication 22/30 with seed=21\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 3, 'n_estimators': 50}, AUC=0.8381\n",
      "Best DP gamma on G3 = 0.99, cost=183.00\n",
      "\n",
      "Running replication 23/30 with seed=22\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.05}, AUC=0.8449\n",
      "Best DP gamma on G3 = 0.99, cost=371.00\n",
      "\n",
      "Running replication 24/30 with seed=23\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): RandomForest_{'max_depth': 5, 'n_estimators': 50}, AUC=0.8520\n",
      "Best DP gamma on G3 = 0.99, cost=293.00\n",
      "\n",
      "Running replication 25/30 with seed=24\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 5, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8609\n",
      "Best DP gamma on G3 = 0.99, cost=401.00\n",
      "\n",
      "Running replication 26/30 with seed=25\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.05}, AUC=0.8623\n",
      "Best DP gamma on G3 = 0.99, cost=230.00\n",
      "\n",
      "Running replication 27/30 with seed=26\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8569\n",
      "Best DP gamma on G3 = 0.99, cost=285.00\n",
      "\n",
      "Running replication 28/30 with seed=27\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.1}, AUC=0.8421\n",
      "Best DP gamma on G3 = 0.99, cost=354.00\n",
      "\n",
      "Running replication 29/30 with seed=28\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): GradientBoosting_{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}, AUC=0.8462\n",
      "Best DP gamma on G3 = 0.99, cost=323.00\n",
      "\n",
      "Running replication 30/30 with seed=29\n",
      "G1: 150 patients | G2: 150 | G3: 150 | G4: 150\n",
      "Best ML model on (G1->G2): CatBoost_{'depth': 3, 'iterations': 50, 'learning_rate': 0.05}, AUC=0.8590\n",
      "Best DP gamma on G3 = 0.99, cost=283.00\n",
      "\n",
      "=== FINAL RESULTS (Mean ± Std Dev over 30 Replications, Test on G4) ===\n",
      "              Method Precision (%)           Cost    Recall (%) Treatment Time\n",
      "  Constant Threshold  59.42 ± 6.32 472.80 ± 44.38  99.90 ± 0.53    4.88 ± 0.83\n",
      " Dynamic Threshold-R  55.46 ± 9.05 454.17 ± 73.49 100.00 ± 0.00    8.34 ± 0.90\n",
      "    Linear Threshold  41.93 ± 4.53 609.93 ± 45.05 100.00 ± 0.00    2.36 ± 0.37\n",
      "       Wait Till End  99.39 ± 1.45 638.67 ± 92.87  99.22 ± 1.30   20.00 ± 0.00\n",
      "Dynamic Threshold-DP  91.95 ± 5.72 302.03 ± 45.63 100.00 ± 0.00    8.78 ± 1.17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Sklearn models, metrics, etc.\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS\n",
    "###############################################################################\n",
    "FP_COST = 10\n",
    "FN_COST = 50\n",
    "D_COST  = 1\n",
    "T_MAX   = 21   # maximum discrete time steps (0..T_MAX-1)\n",
    "GAMMA_CANDIDATES = [0.95, 0.99]  # Example DP discount factors to try\n",
    "\n",
    "# For demonstration, we'll use a small hyperparameter grid for each ML model.\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "GB_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "CATBOOST_PARAM_GRID = {\n",
    "    'iterations': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3, 5]\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def split_into_four_groups(df, seed=0):\n",
    "    \"\"\"\n",
    "    Shuffle patient IDs and split ~evenly into four groups: G1, G2, G3, G4.\n",
    "    Used for Algorithm 0 (Standard Validation).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pids = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pids)\n",
    "    \n",
    "    n = len(unique_pids)\n",
    "    i1 = int(0.25 * n)\n",
    "    i2 = int(0.50 * n)\n",
    "    i3 = int(0.75 * n)\n",
    "    \n",
    "    G1_pids = unique_pids[: i1]\n",
    "    G2_pids = unique_pids[i1 : i2]\n",
    "    G3_pids = unique_pids[i2 : i3]\n",
    "    G4_pids = unique_pids[i3 : ]\n",
    "    \n",
    "    G1 = df[df['patient_id'].isin(G1_pids)].copy()\n",
    "    G2 = df[df['patient_id'].isin(G2_pids)].copy()\n",
    "    G3 = df[df['patient_id'].isin(G3_pids)].copy()\n",
    "    G4 = df[df['patient_id'].isin(G4_pids)].copy()\n",
    "    \n",
    "    return G1, G2, G3, G4\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "def compute_auc_score(y_true, y_prob):\n",
    "    \"\"\"Compute AUC safely. If only one class, return 0.5.\"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "def train_and_select_best_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Trains multiple models (RandomForest, GB, CatBoost)\n",
    "    over small hyperparam grids, picks best by AUC.\n",
    "    \n",
    "    Returns: (best_model, best_auc, best_model_name)\n",
    "    \"\"\"\n",
    "    best_auc = -1.0\n",
    "    best_model = None\n",
    "    best_name  = None\n",
    "    \n",
    "    # 1) RandomForest\n",
    "    for params in ParameterGrid(RF_PARAM_GRID):\n",
    "        rf = RandomForestClassifier(random_state=0, **params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        val_prob = rf.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = rf\n",
    "            best_name  = f\"RandomForest_{params}\"\n",
    "    \n",
    "    # 2) GradientBoosting\n",
    "    for params in ParameterGrid(GB_PARAM_GRID):\n",
    "        gb = GradientBoostingClassifier(random_state=0, **params)\n",
    "        gb.fit(X_train, y_train)\n",
    "        val_prob = gb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = gb\n",
    "            best_name  = f\"GradientBoosting_{params}\"\n",
    "    \n",
    "    # 3) CatBoost\n",
    "    for params in ParameterGrid(CATBOOST_PARAM_GRID):\n",
    "        cb = CatBoostClassifier(verbose=0, random_state=0, **params)\n",
    "        cb.fit(X_train, y_train)\n",
    "        val_prob = cb.predict_proba(X_val)[:,1]\n",
    "        auc_val  = compute_auc_score(y_val, val_prob)\n",
    "        if auc_val > best_auc:\n",
    "            best_auc   = auc_val\n",
    "            best_model = cb\n",
    "            best_name  = f\"CatBoost_{params}\"\n",
    "    \n",
    "    return best_model, best_auc, best_name\n",
    "\n",
    "###############################################################################\n",
    "# 3. SIMULATE POLICY (Unconstrained)\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df must contain:\n",
    "      - patient_id\n",
    "      - time\n",
    "      - risk_score\n",
    "      - label (0 or 1)\n",
    "    policy_func(patient_rows) -> treat_time (int) or None\n",
    "    \n",
    "    Return dict of cost, precision, recall, avg_treatment_time\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pid, grp in df.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        label = grp['label'].iloc[0]\n",
    "        \n",
    "        treat_time = policy_func(grp)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treated\n",
    "            if label == 1:\n",
    "                cost = FN_COST\n",
    "                tp   = 0\n",
    "            else:\n",
    "                cost = 0\n",
    "                tp   = 0\n",
    "            fp = 0\n",
    "            treat_flag = 0\n",
    "            ttime = None\n",
    "        else:\n",
    "            treat_flag = 1\n",
    "            if label == 1:\n",
    "                # cost = D * treat_time\n",
    "                cost = D_COST * treat_time\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            ttime = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treat_flag,\n",
    "            'treat_time': ttime,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res   = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated']==1]\n",
    "    tp_sum = treated_df['tp'].sum()\n",
    "    fp_sum = treated_df['fp'].sum()\n",
    "    if len(treated_df)>0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df = df_res[df_res['label']==1]\n",
    "    total_sick = len(sick_df)\n",
    "    if total_sick>0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df)>0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt   = valid_tt.mean() if len(valid_tt)>0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 4. BENCHMARK THRESHOLD-BASED POLICIES\n",
    "###############################################################################\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            # treat at first time we see risk_score >= thr\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['risk_score'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_constant_threshold_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(row['time'])\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=20,\n",
    "                                    threshold_candidates=[0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                    n_samples=200,\n",
    "                                    seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    return best_vec, best_stats\n",
    "\n",
    "def make_dynamic_threshold_policy(thr_vec):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < len(thr_vec):\n",
    "                if row['risk_score'] >= thr_vec[t]:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def linear_threshold_search(df,\n",
    "                            A_candidates=np.linspace(-0.05, 0.01, 7),\n",
    "                            B_candidates=np.linspace(0,0.6,2)):\n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A*t + B\n",
    "                    thr = np.clip(thr,0,1)\n",
    "                    if row['risk_score'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            \n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A    = A\n",
    "                best_B    = B\n",
    "                best_stats= stats\n",
    "    return (best_A,best_B), best_stats\n",
    "\n",
    "def make_linear_threshold_policy(A,B):\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = row['time']\n",
    "            thr = A*t + B\n",
    "            thr = np.clip(thr,0,1)\n",
    "            if row['risk_score'] >= thr:\n",
    "                return int(t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0,1,21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            final_t = patient_rows['time'].max()\n",
    "            final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "            if final_row['risk_score'] >= thr:\n",
    "                return int(final_t)\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "def make_wait_till_end_policy(thr):\n",
    "    def policy_func(patient_rows):\n",
    "        final_t = patient_rows['time'].max()\n",
    "        final_row = patient_rows[patient_rows['time']==final_t].iloc[0]\n",
    "        if final_row['risk_score'] >= thr:\n",
    "            return int(final_t)\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. DATA-DRIVEN DP (UNCONSTRAINED)\n",
    "###############################################################################\n",
    "def to_bucket(prob):\n",
    "    \"\"\"Simple function to map prob into a 5-bucket scale [0..4].\"\"\"\n",
    "    b = int(prob * 5)\n",
    "    return min(b, 4)\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=20, n_buckets=5):\n",
    "    \"\"\"\n",
    "    p_trans[t,b,b_next], p_sick[t,b]\n",
    "    df_train has columns: patient_id, time, risk_bucket, label\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    bucket_counts     = np.zeros((T, n_buckets), dtype=float)\n",
    "    sick_counts       = np.zeros((T, n_buckets), dtype=float)\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        grp = grp.sort_values('time')\n",
    "        rows= grp.to_dict('records')\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            lbl = row['label']\n",
    "            \n",
    "            if t < T:\n",
    "                bucket_counts[t,b] += 1\n",
    "                sick_counts[t,b]   += lbl\n",
    "            \n",
    "            if i < len(rows)-1:\n",
    "                nxt = rows[i+1]\n",
    "                t_next = nxt['time']\n",
    "                b_next = nxt['risk_bucket']\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t,b,b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets), dtype=float)\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_,b_,:].sum()\n",
    "            if denom>0:\n",
    "                p_trans[t_,b_,:] = transition_counts[t_,b_,:] / denom\n",
    "            else:\n",
    "                p_trans[t_,b_,b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets), dtype=float)\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_,b_]\n",
    "            if denom>0:\n",
    "                p_sick[t_,b_] = sick_counts[t_,b_] / denom\n",
    "            else:\n",
    "                p_sick[t_,b_] = 0.0\n",
    "    return p_trans, p_sick\n",
    "\n",
    "def train_data_driven_dp_unconstrained(p_trans, p_sick, \n",
    "                                       FP=10, FN=50, D=1, gamma=0.99, T=20):\n",
    "    \"\"\"\n",
    "    Standard DP for unconstrained scenario:\n",
    "      V[t,b] = min( cost_treat_now, cost_wait )\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # boundary at t=T\n",
    "    for b in range(n_buckets):\n",
    "        cost_treat   = p_sick[T-1,b]*(D*(T-1)) + (1-p_sick[T-1,b])*FP\n",
    "        cost_notreat = p_sick[T-1,b]*FN\n",
    "        V[T,b] = min(cost_treat, cost_notreat)\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1-p_sick[t,b])*FP\n",
    "            # wait\n",
    "            if t == T-1:\n",
    "                cost_wait = gamma * V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next]*V[t+1,b_next]\n",
    "                cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    return V, pi_\n",
    "\n",
    "def make_dp_policy(V, pi_, T=20):\n",
    "    \"\"\"Return a policy function that treats if pi[t,b]==1.\"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            b = int(row['risk_bucket'])\n",
    "            if t < T:\n",
    "                if pi_[t,b] == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "###############################################################################\n",
    "# 6. ALGORITHM 0 (STANDARD VALIDATION)\n",
    "###############################################################################\n",
    "def run_algorithm0_unconstrained(df_all, seed=0):\n",
    "    \"\"\"\n",
    "    1) Split df_all -> G1, G2, G3, G4\n",
    "    2) ML hyperparam search on (G1->G2)\n",
    "    3) Retrain best ML on G1+G2\n",
    "    4) DP hyperparam search on G3\n",
    "    5) Final evaluation on G4\n",
    "    \"\"\"\n",
    "    # Step 1: Split\n",
    "    G1, G2, G3, G4 = split_into_four_groups(df_all, seed=seed)\n",
    "    print(f\"G1: {G1['patient_id'].nunique()} patients | G2: {G2['patient_id'].nunique()} | \"\n",
    "          f\"G3: {G3['patient_id'].nunique()} | G4: {G4['patient_id'].nunique()}\")\n",
    "    \n",
    "    # Step 2: ML hyperparam search on (G1->G2)\n",
    "    X_train = G1[['EIT','NIRS','EIS']].values\n",
    "    y_train = G1['label'].values\n",
    "    \n",
    "    X_val   = G2[['EIT','NIRS','EIS']].values\n",
    "    y_val   = G2['label'].values\n",
    "    \n",
    "    best_model, best_auc, best_name = train_and_select_best_model(\n",
    "        X_train, y_train, X_val, y_val\n",
    "    )\n",
    "    print(f\"Best ML model on (G1->G2): {best_name}, AUC={best_auc:.4f}\")\n",
    "    \n",
    "    # Step 3: Retrain best ML on G1+G2\n",
    "    G12 = pd.concat([G1, G2], ignore_index=True)\n",
    "    X_12 = G12[['EIT','NIRS','EIS']].values\n",
    "    y_12 = G12['label'].values\n",
    "    # We'll do the same approach => train_and_select_best_model on itself \n",
    "    #   or just fit best_model to G1+G2 \n",
    "    #   (for simplicity, we directly refit best_model).\n",
    "    \n",
    "    best_model.fit(X_12, y_12)\n",
    "    \n",
    "    # Step 4: DP hyperparam search on G3 => produce risk scores from final ML\n",
    "    G3 = G3.copy()\n",
    "    X_3 = G3[['EIT','NIRS','EIS']].values\n",
    "    prob_3 = best_model.predict_proba(X_3)[:,1]\n",
    "    G3['risk_score'] = prob_3\n",
    "    \n",
    "    # We also need transitions from G12 => so produce risk_score for G12\n",
    "    G12 = G12.copy()\n",
    "    prob_12 = best_model.predict_proba(G12[['EIT','NIRS','EIS']])[:,1]\n",
    "    G12['risk_score'] = prob_12\n",
    "    \n",
    "    # For each candidate gamma => train DP => evaluate cost on G3 => pick best gamma\n",
    "    best_gamma = None\n",
    "    best_cost_dp = float('inf')\n",
    "    best_V = None\n",
    "    best_pi= None\n",
    "    \n",
    "    # Bucket the training data for DP\n",
    "    G12['risk_bucket'] = G12['risk_score'].apply(to_bucket)\n",
    "    \n",
    "    for gamma_ in GAMMA_CANDIDATES:\n",
    "        # estimate transitions\n",
    "        p_trans, p_sick = estimate_transition_and_sick_probs(G12, T=T_MAX, n_buckets=5)\n",
    "        V_temp, pi_temp = train_data_driven_dp_unconstrained(\n",
    "            p_trans, p_sick, FP=FP_COST, FN=FN_COST,\n",
    "            D=D_COST, gamma=gamma_, T=T_MAX\n",
    "        )\n",
    "        # Evaluate on G3\n",
    "        #   also bucket G3\n",
    "        G3_temp = G3.copy()\n",
    "        G3_temp['risk_bucket'] = G3_temp['risk_score'].apply(to_bucket)\n",
    "        \n",
    "        dp_policy_func = make_dp_policy(V_temp, pi_temp, T=T_MAX)\n",
    "        stats_dp = simulate_policy(G3_temp, dp_policy_func)\n",
    "        \n",
    "        if stats_dp['cost'] < best_cost_dp:\n",
    "            best_cost_dp = stats_dp['cost']\n",
    "            best_gamma   = gamma_\n",
    "            best_V = V_temp\n",
    "            best_pi= pi_temp\n",
    "    \n",
    "    print(f\"Best DP gamma on G3 = {best_gamma}, cost={best_cost_dp:.2f}\")\n",
    "    \n",
    "    # Step 5: Evaluate on G4\n",
    "    G4 = G4.copy()\n",
    "    prob_4 = best_model.predict_proba(G4[['EIT','NIRS','EIS']])[:,1]\n",
    "    G4['risk_score'] = prob_4\n",
    "    \n",
    "    # (A) threshold-based policies\n",
    "    thr_const, stats_const = constant_threshold_search(G4)\n",
    "    thr_vec, stats_dyn     = dynamic_threshold_random_search(G4, time_steps=T_MAX)\n",
    "    (A_lin,B_lin), stats_lin = linear_threshold_search(G4)\n",
    "    thr_wte, stats_wte     = wait_till_end_search(G4)\n",
    "    \n",
    "    # (B) final DP policy using best_gamma => we already have best_V, best_pi\n",
    "    # re-check transitions from G12 if needed, but we already found them\n",
    "    # we just build final policy:\n",
    "    dp_policy_final = make_dp_policy(best_V, best_pi, T=T_MAX)\n",
    "    \n",
    "    G4_dp = G4.copy()\n",
    "    G4_dp['risk_bucket'] = G4_dp['risk_score'].apply(to_bucket)\n",
    "    stats_dp = simulate_policy(G4_dp, dp_policy_final)\n",
    "    \n",
    "    # Build final table\n",
    "    table = pd.DataFrame({\n",
    "        'Method': [\n",
    "            'Constant Threshold',\n",
    "            'Dynamic Threshold-R',\n",
    "            'Linear Threshold',\n",
    "            'Wait Till End',\n",
    "            f'Dynamic Threshold-DP (gamma={best_gamma})'\n",
    "        ],\n",
    "        'Precision (%)': [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        'Cost': [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        'Recall (%)': [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        'Treatment Time': [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return table\n",
    "\n",
    "###############################################################################\n",
    "# 7. RUN MULTIPLE REPLICATIONS\n",
    "###############################################################################\n",
    "def run_multiple_replications(df_all, n_replications=30):\n",
    "    \"\"\"\n",
    "    Run Algorithm 0 multiple times with different random seeds.\n",
    "    Compute mean and standard deviation for each metric.\n",
    "    \"\"\"\n",
    "    # Define standard method names for consistent reporting\n",
    "    standard_methods = [\n",
    "        'Constant Threshold',\n",
    "        'Dynamic Threshold-R',\n",
    "        'Linear Threshold',\n",
    "        'Wait Till End',\n",
    "        'Dynamic Threshold-DP'\n",
    "    ]\n",
    "    \n",
    "    # Initialize containers for each metric and method\n",
    "    precision_values = {method: [] for method in standard_methods}\n",
    "    cost_values = {method: [] for method in standard_methods}\n",
    "    recall_values = {method: [] for method in standard_methods}\n",
    "    treatment_time_values = {method: [] for method in standard_methods}\n",
    "    \n",
    "    for i in range(n_replications):\n",
    "        seed = i  # Use a different seed for each replication\n",
    "        print(f\"\\nRunning replication {i+1}/{n_replications} with seed={seed}\")\n",
    "        \n",
    "        # Run algorithm with current seed\n",
    "        table = run_algorithm0_unconstrained(df_all, seed=seed)\n",
    "        \n",
    "        # Extract values for each method\n",
    "        for _, row in table.iterrows():\n",
    "            method = row['Method']\n",
    "            \n",
    "            # Standardize method name (remove gamma value from DP method)\n",
    "            standard_method = method\n",
    "            if 'Dynamic Threshold-DP' in method:\n",
    "                standard_method = 'Dynamic Threshold-DP'\n",
    "            \n",
    "            if standard_method in standard_methods:\n",
    "                precision_values[standard_method].append(row['Precision (%)'])\n",
    "                cost_values[standard_method].append(row['Cost'])\n",
    "                recall_values[standard_method].append(row['Recall (%)'])\n",
    "                treatment_time_values[standard_method].append(row['Treatment Time'])\n",
    "    \n",
    "    # Compute statistics\n",
    "    final_data = []\n",
    "    for method in standard_methods:\n",
    "        if precision_values[method]:  # Check if we have data for this method\n",
    "            precision_mean = np.mean(precision_values[method])\n",
    "            precision_std = np.std(precision_values[method])\n",
    "            cost_mean = np.mean(cost_values[method])\n",
    "            cost_std = np.std(cost_values[method])\n",
    "            recall_mean = np.mean(recall_values[method])\n",
    "            recall_std = np.std(recall_values[method])\n",
    "            treat_time_mean = np.mean(treatment_time_values[method])\n",
    "            treat_time_std = np.std(treatment_time_values[method])\n",
    "            \n",
    "            final_data.append({\n",
    "                'Method': method,\n",
    "                'Precision (%)': f\"{precision_mean:.2f} ± {precision_std:.2f}\",\n",
    "                'Cost': f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "                'Recall (%)': f\"{recall_mean:.2f} ± {recall_std:.2f}\",\n",
    "                'Treatment Time': f\"{treat_time_mean:.2f} ± {treat_time_std:.2f}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(final_data)\n",
    "\n",
    "###############################################################################\n",
    "# 8. MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    \n",
    "    # If needed, filter df_all to time < T_MAX:\n",
    "    df_all = df_all[df_all['time'] < T_MAX].copy()\n",
    "    \n",
    "    # Check required columns:\n",
    "    required = {'patient_id','time','EIT','NIRS','EIS','label'}\n",
    "    if not required.issubset(df_all.columns):\n",
    "        raise ValueError(f\"Your CSV must have columns at least: {required}. Found: {df_all.columns}\")\n",
    "    \n",
    "    # Run Algorithm 0 multiple times\n",
    "    final_results = run_multiple_replications(df_all, n_replications=30)\n",
    "    \n",
    "    print(\"\\n=== FINAL RESULTS (Mean ± Std Dev over 30 Replications, Test on G4) ===\")\n",
    "    print(final_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84028939-cfff-4e71-beca-d92afdd83b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
