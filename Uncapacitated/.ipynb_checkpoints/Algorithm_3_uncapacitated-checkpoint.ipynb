{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6575947-9772-4881-9ec0-3f7334423db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients: 600\n",
      "Columns: ['patient_id', 'time', 'risk_bucket', 'risk_score', 'EIT', 'NIRS', 'EIS', 'label']\n",
      "\n",
      "Running replication 1/30 with seed=0\n",
      "\n",
      "Running replication 2/30 with seed=1\n",
      "\n",
      "Running replication 3/30 with seed=2\n",
      "\n",
      "Running replication 4/30 with seed=3\n",
      "\n",
      "Running replication 5/30 with seed=4\n",
      "\n",
      "Running replication 6/30 with seed=5\n",
      "\n",
      "Running replication 7/30 with seed=6\n",
      "\n",
      "Running replication 8/30 with seed=7\n",
      "\n",
      "Running replication 9/30 with seed=8\n",
      "\n",
      "Running replication 10/30 with seed=9\n",
      "\n",
      "Running replication 11/30 with seed=10\n",
      "\n",
      "Running replication 12/30 with seed=11\n",
      "\n",
      "Running replication 13/30 with seed=12\n",
      "\n",
      "Running replication 14/30 with seed=13\n",
      "\n",
      "Running replication 15/30 with seed=14\n",
      "\n",
      "Running replication 16/30 with seed=15\n",
      "\n",
      "Running replication 17/30 with seed=16\n",
      "\n",
      "Running replication 18/30 with seed=17\n",
      "\n",
      "Running replication 19/30 with seed=18\n",
      "\n",
      "Running replication 20/30 with seed=19\n",
      "\n",
      "Running replication 21/30 with seed=20\n",
      "\n",
      "Running replication 22/30 with seed=21\n",
      "\n",
      "Running replication 23/30 with seed=22\n",
      "\n",
      "Running replication 24/30 with seed=23\n",
      "\n",
      "Running replication 25/30 with seed=24\n",
      "\n",
      "Running replication 26/30 with seed=25\n",
      "\n",
      "Running replication 27/30 with seed=26\n",
      "\n",
      "Running replication 28/30 with seed=27\n",
      "\n",
      "Running replication 29/30 with seed=28\n",
      "\n",
      "Running replication 30/30 with seed=29\n",
      "\n",
      "=== FINAL RESULTS (Mean ± Std Dev over 30 Replications, Algorithm 3) ===\n",
      "              Method Precision (%)            Cost    Recall (%) Treatment Time\n",
      "  Constant Threshold  70.68 ± 8.46  262.57 ± 38.16 100.00 ± 0.00    6.67 ± 0.72\n",
      " Dynamic Threshold-R  58.94 ± 6.93  355.53 ± 44.23 100.00 ± 0.00    8.05 ± 0.62\n",
      "    Linear Threshold  82.68 ± 8.92 522.90 ± 114.33  66.98 ± 7.64    3.84 ± 0.66\n",
      "       Wait Till End  99.78 ± 0.83  508.67 ± 76.28  99.26 ± 1.49   20.00 ± 0.00\n",
      "Dynamic Threshold-DP  88.59 ± 6.98  244.40 ± 37.71 100.00 ± 0.00    8.43 ± 1.34\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "###############################################################################\n",
    "# 1. GLOBAL PARAMETERS & SETTINGS\n",
    "###############################################################################\n",
    "FP_COST = 10    # Penalty for false positive (treating a healthy patient)\n",
    "FN_COST = 50    # Penalty for false negative (never treating a sick patient)\n",
    "D_COST  = 1     # Penalty per time-step of delay in treating a sick patient\n",
    "GAMMA   = 0.99  # Default discount factor for DP (may be overridden in DP tuning)\n",
    "T_MAX   = 20    # Time horizon (discrete steps 0..T_MAX-1 for each patient)\n",
    "\n",
    "FEATURE_COLS = [\"time\", \"EIT\", \"NIRS\", \"EIS\"]\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS FOR DATA SPLITTING & FILTERING\n",
    "###############################################################################\n",
    "def split_patients_kfold(df, n_splits=4, seed=0):\n",
    "    \"\"\"\n",
    "    Shuffle unique patient IDs, then split into (n_splits+1) groups G1,...,G_{n_splits},G_{n_splits+1}.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_pts = df['patient_id'].unique()\n",
    "    rng.shuffle(unique_pts)\n",
    "    \n",
    "    n = len(unique_pts)\n",
    "    splits = {}\n",
    "    \n",
    "    # Partition into n_splits+1 roughly equal groups\n",
    "    for i in range(n_splits + 1):\n",
    "        start_idx = int(i * n / (n_splits + 1))\n",
    "        end_idx   = int((i + 1) * n / (n_splits + 1))\n",
    "        group_name = f\"G{i+1}\"\n",
    "        splits[group_name] = set(unique_pts[start_idx:end_idx])\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def filter_by_group(df, pid_set):\n",
    "    \"\"\"Return the subset of df whose patient_id is in pid_set.\"\"\"\n",
    "    return df[df['patient_id'].isin(pid_set)].copy()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. ML TRAINING & RISK-SCORE PREDICTIONS\n",
    "###############################################################################\n",
    "def train_and_predict_model(\n",
    "    model_type,\n",
    "    hyperparams,\n",
    "    df_train,\n",
    "    df_val,\n",
    "    feature_cols=FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    Train a classification model (CatBoost/RF/GB) on df_train and return predicted\n",
    "    probabilities for df_val. `hyperparams` is a dict of model-specific hyper-parameters.\n",
    "    \"\"\"\n",
    "    X_train = df_train[feature_cols]\n",
    "    y_train = df_train['label']\n",
    "    \n",
    "    if model_type == \"catboost\":\n",
    "        model = CatBoostClassifier(**hyperparams, verbose=False)\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model_type == \"rf\":\n",
    "        model = RandomForestClassifier(**hyperparams, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model_type == \"gb\":\n",
    "        model = GradientBoostingClassifier(**hyperparams, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type={model_type}\")\n",
    "    \n",
    "    X_val = df_val[feature_cols]\n",
    "    risk_scores = model.predict_proba(X_val)[:,1]  # Probability label=1\n",
    "    return risk_scores\n",
    "\n",
    "\n",
    "def select_best_ml_hyperparams_by_auc(\n",
    "    df_train_splits,\n",
    "    val_split_name,\n",
    "    model_list,\n",
    "    param_grid_dict,\n",
    "    feature_cols=FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    Perform a grid search over (model_type, hyperparams) to maximize AUC on the \n",
    "    validation set = df_train_splits[val_split_name].\n",
    "    Return:\n",
    "       best_model_type, best_hyperparams, best_auc, val_preds (predicted_risk for the val set).\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    df_val = df_train_splits[val_split_name].copy()\n",
    "    \n",
    "    # Combine all other folds for training\n",
    "    train_df_list = []\n",
    "    for k, v_df in df_train_splits.items():\n",
    "        if k != val_split_name:\n",
    "            train_df_list.append(v_df)\n",
    "    df_train_full = pd.concat(train_df_list, ignore_index=True)\n",
    "    \n",
    "    X_val = df_val[feature_cols]\n",
    "    y_val = df_val['label'].values\n",
    "    \n",
    "    best_model_type = None\n",
    "    best_hparams    = None\n",
    "    best_auc        = -999\n",
    "    best_preds      = None\n",
    "    \n",
    "    # Grid search across all candidate (model_type, hyperparam)\n",
    "    for model_type in model_list:\n",
    "        for hyperparams in param_grid_dict[model_type]:\n",
    "            # Train on df_train_full, predict on df_val\n",
    "            preds = train_and_predict_model(\n",
    "                model_type=model_type,\n",
    "                hyperparams=hyperparams,\n",
    "                df_train=df_train_full,\n",
    "                df_val=df_val,\n",
    "                feature_cols=feature_cols\n",
    "            )\n",
    "            auc_val = roc_auc_score(y_val, preds)\n",
    "            if auc_val > best_auc:\n",
    "                best_auc = auc_val\n",
    "                best_model_type = model_type\n",
    "                best_hparams    = hyperparams\n",
    "                best_preds      = preds\n",
    "    \n",
    "    return best_model_type, best_hparams, best_auc, best_preds\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. POLICY SIMULATION (Compute cost, precision, recall, etc.)\n",
    "###############################################################################\n",
    "def simulate_policy(df, policy_func):\n",
    "    \"\"\"\n",
    "    df has columns: [patient_id, time, label, predicted_risk].\n",
    "    policy_func(patient_rows) -> an integer in [0..T_MAX-1] for the \n",
    "        time step of treatment, or None if never treated.\n",
    "    Returns a dict with keys {cost, precision, recall, avg_treatment_time}, etc.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pid, patient_rows in df.groupby('patient_id'):\n",
    "        patient_rows = patient_rows.sort_values('time')\n",
    "        label = patient_rows['label'].iloc[0]  # 0 or 1 (healthy vs sick)\n",
    "        treat_time = policy_func(patient_rows)\n",
    "        \n",
    "        if treat_time is None:\n",
    "            # never treat\n",
    "            treated_flag = 0\n",
    "            if label == 1:\n",
    "                cost = FN_COST  # missed a sick patient\n",
    "            else:\n",
    "                cost = 0\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            tt = None\n",
    "        else:\n",
    "            # treat at treat_time\n",
    "            treated_flag = 1\n",
    "            if label == 1:\n",
    "                cost = D_COST * treat_time  # delay cost\n",
    "                tp   = 1\n",
    "                fp   = 0\n",
    "            else:\n",
    "                cost = FP_COST\n",
    "                tp   = 0\n",
    "                fp   = 1\n",
    "            tt = treat_time\n",
    "        \n",
    "        results.append({\n",
    "            'patient_id': pid,\n",
    "            'label': label,\n",
    "            'treated': treated_flag,\n",
    "            'treat_time': tt,\n",
    "            'cost': cost,\n",
    "            'tp': tp,\n",
    "            'fp': fp\n",
    "        })\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    total_cost = df_res['cost'].sum()\n",
    "    \n",
    "    treated_df = df_res[df_res['treated'] == 1]\n",
    "    tp_sum     = treated_df['tp'].sum()\n",
    "    fp_sum     = treated_df['fp'].sum()\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        precision = tp_sum / (tp_sum + fp_sum)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    sick_df   = df_res[df_res['label'] == 1]\n",
    "    total_sick= len(sick_df)\n",
    "    if total_sick > 0:\n",
    "        recall = tp_sum / total_sick\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if len(treated_df) > 0:\n",
    "        valid_tt = treated_df['treat_time'].dropna()\n",
    "        avg_tt   = valid_tt.mean() if len(valid_tt) > 0 else 0.0\n",
    "    else:\n",
    "        avg_tt = 0.0\n",
    "    \n",
    "    return {\n",
    "        'cost': total_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'avg_treatment_time': avg_tt\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. BENCHMARK STRATEGIES (Constant Threshold, Dynamic, Linear, Wait-Till-End)\n",
    "###############################################################################\n",
    "def constant_threshold_search(df, thresholds=None):\n",
    "    \"\"\"\n",
    "    Try a grid of constant thresholds for the entire time horizon,\n",
    "    pick the one minimizing cost on df. Return (best_threshold, best_stats).\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0, 1, 21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                if row['predicted_risk'] >= thr:\n",
    "                    return int(row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_thr, best_stats\n",
    "\n",
    "\n",
    "def dynamic_threshold_random_search(df,\n",
    "                                    time_steps=T_MAX,\n",
    "                                    threshold_candidates=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                    n_samples=100,\n",
    "                                    seed=0):\n",
    "    \"\"\"\n",
    "    Sample random time-varying thresholds (one threshold per time step),\n",
    "    measure cost, pick the best. For demonstration.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    best_vec = None\n",
    "    best_cost= float('inf')\n",
    "    best_stats=None\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        thr_vec = rng.choice(threshold_candidates, size=time_steps)\n",
    "        \n",
    "        def policy_func(patient_rows):\n",
    "            for _, row in patient_rows.iterrows():\n",
    "                t = int(row['time'])\n",
    "                if t < time_steps and row['predicted_risk'] >= thr_vec[t]:\n",
    "                    return t\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_vec  = thr_vec.copy()\n",
    "            best_stats= stats\n",
    "    \n",
    "    return best_vec, best_stats\n",
    "\n",
    "def linear_threshold_search(df, A_candidates=None, B_candidates=None):\n",
    "    \"\"\"\n",
    "    threshold(t) = A*t + B, clipped to [0,1].\n",
    "    Search over A_candidates x B_candidates, pick the best cost.\n",
    "    \"\"\"\n",
    "    if A_candidates is None:\n",
    "        A_candidates = np.linspace(-0.05, 0.05, 11)\n",
    "    if B_candidates is None:\n",
    "        B_candidates = np.linspace(0, 1, 11)\n",
    "    best_A, best_B = None, None\n",
    "    best_cost, best_stats = float('inf'), None\n",
    "    \n",
    "    for A in A_candidates:\n",
    "        for B in B_candidates:\n",
    "            def policy_func(patient_rows):\n",
    "                for _, row in patient_rows.iterrows():\n",
    "                    t = row['time']\n",
    "                    thr = A * t + B\n",
    "                    thr = max(0, min(1, thr))  # clip to [0,1]\n",
    "                    if row['predicted_risk'] >= thr:\n",
    "                        return int(t)\n",
    "                return None\n",
    "            \n",
    "            stats = simulate_policy(df, policy_func)\n",
    "            if stats['cost'] < best_cost:\n",
    "                best_cost = stats['cost']\n",
    "                best_A = A\n",
    "                best_B = B\n",
    "                best_stats = stats\n",
    "    \n",
    "    return (best_A, best_B), best_stats\n",
    "\n",
    "\n",
    "def wait_till_end_search(df, thresholds=None):\n",
    "    \"\"\"\n",
    "    Treat (if at all) only at the final time step, with a single threshold.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0, 1, 21)\n",
    "    best_thr, best_cost, best_stats = None, float('inf'), None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        def policy_func(patient_rows):\n",
    "            # Look at the final time row\n",
    "            final_row = patient_rows.loc[patient_rows['time'].idxmax()]\n",
    "            if final_row['predicted_risk'] >= thr:\n",
    "                return int(final_row['time'])\n",
    "            return None\n",
    "        \n",
    "        stats = simulate_policy(df, policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost = stats['cost']\n",
    "            best_thr  = thr\n",
    "            best_stats= stats\n",
    "    return best_thr, best_stats\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6. DATA-DRIVEN DP (Bucketed)\n",
    "###############################################################################\n",
    "def assign_buckets(prob, n_buckets=5):\n",
    "    \"\"\"\n",
    "    Convert predicted probability into a discrete bucket 0..(n_buckets-1).\n",
    "    E.g. edges for 5 buckets = [0,0.2,0.4,0.6,0.8,1.0]\n",
    "    \"\"\"\n",
    "    edges = np.linspace(0, 1, n_buckets+1)\n",
    "    for b in range(n_buckets):\n",
    "        if edges[b] <= prob < edges[b+1]:\n",
    "            return b\n",
    "    return n_buckets-1  # fallback if prob=1.0\n",
    "\n",
    "def estimate_transition_and_sick_probs(df_train, T=T_MAX, n_buckets=5):\n",
    "    \"\"\"\n",
    "    Given df_train with 'predicted_risk' & 'label' & 'time' (0..T-1),\n",
    "    compute p_trans[t,b,b'] = P(bucket_{t+1}=b' | bucket_t=b),\n",
    "    and p_sick[t,b] = Probability of being sick in (t,b).\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    bucket_counts     = np.zeros((T, n_buckets))\n",
    "    sick_counts       = np.zeros((T, n_buckets))\n",
    "    \n",
    "    df_sorted = df_train.sort_values(['patient_id','time'])\n",
    "    \n",
    "    for pid, grp in df_sorted.groupby('patient_id'):\n",
    "        rows = grp.to_dict('records')\n",
    "        for i, row in enumerate(rows):\n",
    "            t   = int(row['time'])\n",
    "            b   = int(row['risk_bucket'])\n",
    "            lbl = int(row['label'])\n",
    "            if t < T:\n",
    "                bucket_counts[t, b] += 1\n",
    "                sick_counts[t, b]   += lbl\n",
    "            \n",
    "            if i < len(rows) - 1:\n",
    "                row_next = rows[i+1]\n",
    "                t_next = int(row_next['time'])\n",
    "                b_next = int(row_next['risk_bucket'])\n",
    "                if (t_next == t+1) and (t < T-1):\n",
    "                    transition_counts[t, b, b_next] += 1\n",
    "    \n",
    "    p_trans = np.zeros((T-1, n_buckets, n_buckets))\n",
    "    for t_ in range(T-1):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = transition_counts[t_, b_, :].sum()\n",
    "            if denom > 0:\n",
    "                p_trans[t_, b_, :] = transition_counts[t_, b_, :] / denom\n",
    "            else:\n",
    "                # if no data, remain in the same bucket with prob=1\n",
    "                p_trans[t_, b_, b_] = 1.0\n",
    "    \n",
    "    p_sick = np.zeros((T, n_buckets))\n",
    "    for t_ in range(T):\n",
    "        for b_ in range(n_buckets):\n",
    "            denom = bucket_counts[t_, b_]\n",
    "            if denom > 0:\n",
    "                p_sick[t_, b_] = sick_counts[t_, b_] / denom\n",
    "            else:\n",
    "                p_sick[t_, b_] = 0.0\n",
    "    \n",
    "    return p_trans, p_sick\n",
    "\n",
    "\n",
    "def train_data_driven_dp(p_trans, p_sick,\n",
    "                         FP=FP_COST, FN=FN_COST, D=D_COST,\n",
    "                         gamma=GAMMA, T=T_MAX):\n",
    "    \"\"\"\n",
    "    Standard backward DP for the bucket-based approach:\n",
    "      - V[t,b] = min( cost of treating now, cost of waiting )\n",
    "    Return V, pi_ (value function and policy).\n",
    "    \"\"\"\n",
    "    n_buckets = p_sick.shape[1]\n",
    "    V = np.zeros((T+1, n_buckets))\n",
    "    pi_ = np.zeros((T, n_buckets), dtype=int)\n",
    "    \n",
    "    # Terminal cost at t=T\n",
    "    # If we reach time T in bucket b, the next step is \"end\" => we can choose treat or not\n",
    "    for b in range(n_buckets):\n",
    "        cost_treat = p_sick[T-1,b]* (D*(T-1)) + (1 - p_sick[T-1,b])*FP\n",
    "        cost_skip  = p_sick[T-1,b]*FN\n",
    "        V[T,b]     = min(cost_treat, cost_skip)\n",
    "    \n",
    "    # Backward recursion\n",
    "    for t in reversed(range(T)):\n",
    "        for b in range(n_buckets):\n",
    "            # Option A: Treat now\n",
    "            cost_treat = p_sick[t,b]*(D*t) + (1 - p_sick[t,b])*FP\n",
    "            \n",
    "            # Option B: Wait => expected cost of next state\n",
    "            if t == T-1:\n",
    "                # next step is t=T\n",
    "                exp_future = V[T,b]\n",
    "            else:\n",
    "                exp_future = 0.0\n",
    "                for b_next in range(n_buckets):\n",
    "                    exp_future += p_trans[t,b,b_next] * V[t+1,b_next]\n",
    "            cost_wait = gamma * exp_future\n",
    "            \n",
    "            if cost_treat <= cost_wait:\n",
    "                V[t,b]   = cost_treat\n",
    "                pi_[t,b] = 1\n",
    "            else:\n",
    "                V[t,b]   = cost_wait\n",
    "                pi_[t,b] = 0\n",
    "    \n",
    "    return V, pi_\n",
    "\n",
    "def make_data_driven_dp_policy(V, pi_, T=T_MAX):\n",
    "    \"\"\"\n",
    "    Returns a function that uses pi_[t,b] to decide when to treat.\n",
    "    \"\"\"\n",
    "    def policy_func(patient_rows):\n",
    "        for _, row in patient_rows.iterrows():\n",
    "            t = int(row['time'])\n",
    "            if t < T:\n",
    "                b = int(row['risk_bucket'])\n",
    "                action = pi_[t,b]  # 0=wait, 1=treat\n",
    "                if action == 1:\n",
    "                    return t\n",
    "        return None\n",
    "    return policy_func\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. DP Hyper-Parameter Search for Algorithm 3\n",
    "###############################################################################\n",
    "def dp_param_search(df_train_fold, df_val_fold,\n",
    "                    dp_param_grid,  # list of dicts, e.g. [{'gamma':0.95}, {'gamma':0.99}]\n",
    "                    T=T_MAX):\n",
    "    \"\"\"\n",
    "    Given a training fold & validation fold, we try each DP param set in dp_param_grid,\n",
    "    build a DP policy, and measure cost on the validation fold.\n",
    "    \n",
    "    Return the best_dp_params, the cost, and the predicted risk for the validation set\n",
    "    (the validation set already should have 'predicted_risk' from the chosen ML).\n",
    "    \"\"\"\n",
    "    # For DP, we need to:\n",
    "    #  - compute discrete buckets in the training fold\n",
    "    #  - estimate transitions\n",
    "    #  - run DP for each param set\n",
    "    #  - apply the resulting policy on the validation fold\n",
    "    #  - measure cost\n",
    "    \n",
    "    # 1) Assign buckets to training fold\n",
    "    df_train_fold = df_train_fold.copy()\n",
    "    df_train_fold[\"risk_bucket\"] = df_train_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    # 2) Estimate transitions\n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_train_fold, T=T)\n",
    "    \n",
    "    best_params = None\n",
    "    best_cost   = float('inf')\n",
    "    best_stats  = None\n",
    "    \n",
    "    # Assign buckets to val fold too (for policy simulation)\n",
    "    df_val_fold = df_val_fold.copy()\n",
    "    df_val_fold[\"risk_bucket\"] = df_val_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    for param_dict in dp_param_grid:\n",
    "        gamma_ = param_dict.get(\"gamma\", GAMMA)\n",
    "        # Potentially we could also vary D, FP, FN, etc. if included in the dictionary\n",
    "        D_  = param_dict.get(\"D\", D_COST)\n",
    "        FP_ = param_dict.get(\"FP\", FP_COST)\n",
    "        FN_ = param_dict.get(\"FN\", FN_COST)\n",
    "        \n",
    "        # 3) Train DP\n",
    "        V, pi_ = train_data_driven_dp(p_trans, p_sick,\n",
    "                                      FP=FP_, FN=FN_, D=D_, gamma=gamma_, T=T)\n",
    "        dp_policy_func = make_data_driven_dp_policy(V, pi_, T=T)\n",
    "        \n",
    "        # 4) Evaluate on df_val_fold\n",
    "        stats = simulate_policy(df_val_fold, dp_policy_func)\n",
    "        if stats['cost'] < best_cost:\n",
    "            best_cost  = stats['cost']\n",
    "            best_params= param_dict\n",
    "            best_stats = stats\n",
    "    \n",
    "    return best_params, best_stats\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. ALGORITHM 3: SEQUENTIAL OPTIMIZATION\n",
    "###############################################################################\n",
    "def run_experiment_algorithm3(\n",
    "    df_all,\n",
    "    n_splits=4,\n",
    "    seed=42,\n",
    "    model_list=(\"catboost\",\"rf\",\"gb\"),\n",
    "    ml_param_grid=None,\n",
    "    dp_param_grid=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Implement Algorithm 3 (Sequential Optimization):\n",
    "      1) Cross-validate ML hyperparams (AUC-based).\n",
    "      2) For each fold's chosen ML model, cross-validate DP hyperparams (cost-based).\n",
    "      3) Summarize the sets of (mu) found in each fold => define mu_{all}^*.\n",
    "      4) Use all folds again to pick final ML hyperparams (by AUC).\n",
    "      5) Then, with that ML fixed, pick final DP hyperparams from mu_{all}^* by cost.\n",
    "      6) Retrain on G1..G_n with chosen ML, produce predicted_risk, run final DP,\n",
    "         evaluate on G_{n+1}.\n",
    "    \n",
    "    Because there are multiple ways to interpret the text-block pseudo-code,\n",
    "    this function follows the step-by-step logic common in \"sequential\" \n",
    "    (non-decision-aware => then DP decision) style. \n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    if ml_param_grid is None:\n",
    "        # A small default grid for demonstration\n",
    "        ml_param_grid = {\n",
    "            \"catboost\": [\n",
    "                {\"iterations\":50, \"depth\":3, \"learning_rate\":0.1},\n",
    "                {\"iterations\":50, \"depth\":4, \"learning_rate\":0.05},\n",
    "            ],\n",
    "            \"rf\": [\n",
    "                {\"n_estimators\":50, \"max_depth\":3},\n",
    "                {\"n_estimators\":100,\"max_depth\":5},\n",
    "            ],\n",
    "            \"gb\": [\n",
    "                {\"n_estimators\":50, \"max_depth\":3, \"learning_rate\":0.1},\n",
    "                {\"n_estimators\":100,\"max_depth\":3, \"learning_rate\":0.05},\n",
    "            ]\n",
    "        }\n",
    "    if dp_param_grid is None:\n",
    "        # Example DP param grid: vary gamma, or vary others\n",
    "        dp_param_grid = [\n",
    "            {\"gamma\": 0.95},\n",
    "            {\"gamma\": 0.99},\n",
    "        ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nRunning Algorithm 3 (Sequential Optimization) with {n_splits} folds...\")\n",
    "    \n",
    "    # 1) Split data => G1..G_{n_splits}, G_{n_splits+1}\n",
    "    splits = split_patients_kfold(df_all, n_splits=n_splits, seed=seed)\n",
    "    group_dfs = {}\n",
    "    for group_name, pid_set in splits.items():\n",
    "        sub_df = filter_by_group(df_all, pid_set)\n",
    "        group_dfs[group_name] = sub_df\n",
    "    \n",
    "    test_name = f\"G{n_splits+1}\"\n",
    "    df_test   = group_dfs[test_name]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (A) CROSS-VALIDATE ML => pick best ML hyperparams for each fold\n",
    "    # -------------------------------------------------------------------------\n",
    "    ml_cv_details = []\n",
    "    \n",
    "    for i_val in range(1, n_splits+1):\n",
    "        val_name = f\"G{i_val}\"\n",
    "        \n",
    "        # (A1) Find best ML hyperparams by AUC\n",
    "        best_model_type, best_hparams, best_auc, val_preds = select_best_ml_hyperparams_by_auc(\n",
    "            df_train_splits=group_dfs,\n",
    "            val_split_name=val_name,\n",
    "            model_list=model_list,\n",
    "            param_grid_dict=ml_param_grid,\n",
    "            feature_cols=FEATURE_COLS\n",
    "        )\n",
    "        \n",
    "        # (A2) Store predicted_risk for that validation set\n",
    "        df_val = group_dfs[val_name].copy()\n",
    "        df_val[\"predicted_risk\"] = val_preds\n",
    "        \n",
    "        # Save it back\n",
    "        group_dfs[val_name] = df_val\n",
    "        \n",
    "        ml_cv_details.append({\n",
    "            \"fold\": i_val,\n",
    "            \"best_model_type\": best_model_type,\n",
    "            \"best_hparams\": best_hparams,\n",
    "            \"AUC_val\": best_auc\n",
    "        })\n",
    "    \n",
    "    df_ml_cv_details = pd.DataFrame(ml_cv_details)\n",
    "    # Summarize which ML hyperparams got chosen by each fold ...\n",
    "    # (We will re-check them in the next step.)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (B) For each fold's chosen ML, do a DP hyper-param search => pick DP param\n",
    "    # -------------------------------------------------------------------------\n",
    "    dp_cv_details = []\n",
    "    \n",
    "    for i_val in range(1, n_splits+1):\n",
    "        val_name  = f\"G{i_val}\"\n",
    "        best_rec  = df_ml_cv_details[df_ml_cv_details['fold'] == i_val].iloc[0]\n",
    "        ml_model_type = best_rec[\"best_model_type\"]\n",
    "        ml_hparams    = best_rec[\"best_hparams\"]\n",
    "        \n",
    "        # 1) Retrain that ML on \"training folds except G_i_val\" => get predicted_risk\n",
    "        #    for the union (train_folds) = G\\G_i\n",
    "        train_folds = []\n",
    "        for j in range(1, n_splits+1):\n",
    "            if j != i_val:\n",
    "                train_folds.append(group_dfs[f\"G{j}\"])\n",
    "        df_train_fold = pd.concat(train_folds, ignore_index=True).copy()\n",
    "        \n",
    "        # Train & predict on df_train_fold itself for DP transitions\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        \n",
    "        X_train_f = df_train_fold[FEATURE_COLS]\n",
    "        y_train_f = df_train_fold['label']\n",
    "        \n",
    "        # Rebuild the model\n",
    "        if ml_model_type == \"catboost\":\n",
    "            final_model = CatBoostClassifier(**ml_hparams, verbose=False)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        elif ml_model_type == \"rf\":\n",
    "            final_model = RandomForestClassifier(**ml_hparams, random_state=42)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        else:\n",
    "            final_model = GradientBoostingClassifier(**ml_hparams, random_state=42)\n",
    "            final_model.fit(X_train_f, y_train_f)\n",
    "        \n",
    "        # Store predictions in df_train_fold\n",
    "        df_train_fold[\"predicted_risk\"] = final_model.predict_proba(X_train_f)[:,1]\n",
    "        \n",
    "        # 2) DP hyper-param search on this fold, using the same \"train => val\" logic\n",
    "        #    Validation set is group_dfs[val_name], which already has *some* predicted risk \n",
    "        #    but that risk was from the *best model for i_val.* We should unify it carefully.\n",
    "        \n",
    "        # Actually, to be consistent: The DP sees the same final model that we have for i_val.\n",
    "        # So let's do a fresh predicted risk for df_val as well. (Because we want consistent \n",
    "        # train->val usage for DP.)\n",
    "        \n",
    "        df_val_fold = group_dfs[val_name].copy()\n",
    "        X_val_fold  = df_val_fold[FEATURE_COLS]\n",
    "        df_val_fold[\"predicted_risk\"] = final_model.predict_proba(X_val_fold)[:,1]\n",
    "        \n",
    "        best_dp_params, best_dp_stats = dp_param_search(\n",
    "            df_train_fold=df_train_fold,\n",
    "            df_val_fold=df_val_fold,\n",
    "            dp_param_grid=dp_param_grid,\n",
    "            T=T_MAX\n",
    "        )\n",
    "        \n",
    "        dp_cv_details.append({\n",
    "            \"fold\": i_val,\n",
    "            \"chosen_ML_model\": ml_model_type,\n",
    "            \"chosen_ML_hparams\": ml_hparams,\n",
    "            \"chosen_DP_params\": best_dp_params,\n",
    "            \"dp_val_cost\": best_dp_stats[\"cost\"],\n",
    "            \"dp_val_prec\": best_dp_stats[\"precision\"],\n",
    "            \"dp_val_rec\":  best_dp_stats[\"recall\"],\n",
    "            \"dp_val_avgTT\": best_dp_stats[\"avg_treatment_time\"]\n",
    "        })\n",
    "    \n",
    "    df_dp_cv_details = pd.DataFrame(dp_cv_details)\n",
    "    \n",
    "    # Collect all DP param sets that got chosen: mu(j) for j=1..n\n",
    "    mu_all_star = []\n",
    "    for _, row_ in df_dp_cv_details.iterrows():\n",
    "        # each fold might have chosen a dictionary like {\"gamma\":0.95}\n",
    "        mu_all_star.append(row_[\"chosen_DP_params\"])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (C) Now do a second pass to pick the final ML hyperparams \\lambda^*\n",
    "    #     across all folds (by AUC).\n",
    "    # -------------------------------------------------------------------------\n",
    "    # The simplest approach: we do a standard cross-validation again for ML \n",
    "    # but ignoring DP for the moment, because this is \"sequential\" approach.\n",
    "    # => Essentially the same method we used in step (A), but summarizing now \n",
    "    #    across all folds. We'll pick the single best (model_type, hyperparams)\n",
    "    #    that leads to highest average AUC across G1..G_n.\n",
    "    \n",
    "    # We'll accumulate fold-level AUC for each candidate, then pick the best overall.\n",
    "    candidate_list = []\n",
    "    for model_type in model_list:\n",
    "        for hyperparams in ml_param_grid[model_type]:\n",
    "            candidate_list.append((model_type, hyperparams))\n",
    "    \n",
    "    results_auc_cv = []\n",
    "    for (mtype, mhp) in candidate_list:\n",
    "        fold_aucs = []\n",
    "        for i_val in range(1, n_splits+1):\n",
    "            val_name = f\"G{i_val}\"\n",
    "            # Train on G\\G_i\n",
    "            train_folds = []\n",
    "            for j in range(1, n_splits+1):\n",
    "                if j != i_val:\n",
    "                    train_folds.append(group_dfs[f\"G{j}\"])\n",
    "            df_train_fold = pd.concat(train_folds, ignore_index=True)\n",
    "            \n",
    "            # Train model\n",
    "            X_train_f = df_train_fold[FEATURE_COLS]\n",
    "            y_train_f = df_train_fold['label']\n",
    "            \n",
    "            if mtype == \"catboost\":\n",
    "                tmp_model = CatBoostClassifier(**mhp, verbose=False)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            elif mtype == \"rf\":\n",
    "                tmp_model = RandomForestClassifier(**mhp, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            else:\n",
    "                tmp_model = GradientBoostingClassifier(**mhp, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            \n",
    "            # Predict on validation G_i\n",
    "            df_val_fold = group_dfs[val_name]\n",
    "            X_val_fold  = df_val_fold[FEATURE_COLS]\n",
    "            val_preds   = tmp_model.predict_proba(X_val_fold)[:,1]\n",
    "            \n",
    "            auc_val = roc_auc_score(df_val_fold['label'], val_preds)\n",
    "            fold_aucs.append(auc_val)\n",
    "        \n",
    "        avg_auc = np.mean(fold_aucs)\n",
    "        results_auc_cv.append({\n",
    "            \"model_type\": mtype,\n",
    "            \"hyperparams\": mhp,\n",
    "            \"avg_auc\": avg_auc\n",
    "        })\n",
    "    \n",
    "    df_results_auc_cv = pd.DataFrame(results_auc_cv)\n",
    "    # pick best by avg_auc\n",
    "    best_row = df_results_auc_cv.loc[df_results_auc_cv['avg_auc'].idxmax()]\n",
    "    final_ml_type   = best_row[\"model_type\"]\n",
    "    final_ml_params = best_row[\"hyperparams\"]\n",
    "    final_ml_auc    = best_row[\"avg_auc\"]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (D) Next, with that final ML type/params fixed, we pick the best DP hyperparams \n",
    "    #     from the union mu_all_star we collected above.\n",
    "    #     We'll evaluate each candidate in mu_all_star with a new cross-validation \n",
    "    #     pass for cost, but with the final ML in place.\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Because multiple folds might produce duplicates in mu_all_star, we can deduplicate:\n",
    "    import json\n",
    "    unique_mu = []\n",
    "    seen_strs = set()\n",
    "    for mu_dict in mu_all_star:\n",
    "        s = json.dumps(mu_dict, sort_keys=True)\n",
    "        if s not in seen_strs:\n",
    "            seen_strs.add(s)\n",
    "            unique_mu.append(mu_dict)\n",
    "    \n",
    "    dp_candidates = unique_mu\n",
    "    \n",
    "    # Evaluate each dp_candidates in cross-validation with final ML\n",
    "    #  => for each fold i_val, we do: train final ML on G\\G_i => predict => \n",
    "    #     run the DP with param from dp_candidates => measure cost => average across folds\n",
    "    results_dp_cv = []\n",
    "    \n",
    "    for dp_params in dp_candidates:\n",
    "        fold_costs = []\n",
    "        for i_val in range(1, n_splits+1):\n",
    "            val_name = f\"G{i_val}\"\n",
    "            # Train final ML on G\\G_i\n",
    "            train_folds = []\n",
    "            for j in range(1, n_splits+1):\n",
    "                if j != i_val:\n",
    "                    train_folds.append(group_dfs[f\"G{j}\"])\n",
    "            df_train_fold = pd.concat(train_folds, ignore_index=True).copy()\n",
    "            \n",
    "            X_train_f = df_train_fold[FEATURE_COLS]\n",
    "            y_train_f = df_train_fold['label']\n",
    "            \n",
    "            if final_ml_type == \"catboost\":\n",
    "                tmp_model = CatBoostClassifier(**final_ml_params, verbose=False)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            elif final_ml_type == \"rf\":\n",
    "                tmp_model = RandomForestClassifier(**final_ml_params, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            else:\n",
    "                tmp_model = GradientBoostingClassifier(**final_ml_params, random_state=42)\n",
    "                tmp_model.fit(X_train_f, y_train_f)\n",
    "            \n",
    "            df_train_fold[\"predicted_risk\"] = tmp_model.predict_proba(X_train_f)[:,1]\n",
    "            \n",
    "            # Build DP for dp_params\n",
    "            df_val_fold = group_dfs[val_name].copy()\n",
    "            X_val_fold  = df_val_fold[FEATURE_COLS]\n",
    "            df_val_fold[\"predicted_risk\"] = tmp_model.predict_proba(X_val_fold)[:,1]\n",
    "            \n",
    "            # train DP on df_train_fold\n",
    "            df_train_fold[\"risk_bucket\"] = df_train_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "            p_trans, p_sick = estimate_transition_and_sick_probs(df_train_fold, T=T_MAX)\n",
    "            \n",
    "            gamma_ = dp_params.get(\"gamma\", GAMMA)\n",
    "            D_  = dp_params.get(\"D\", D_COST)\n",
    "            FP_ = dp_params.get(\"FP\", FP_COST)\n",
    "            FN_ = dp_params.get(\"FN\", FN_COST)\n",
    "            \n",
    "            V, pi_ = train_data_driven_dp(\n",
    "                p_trans, p_sick,\n",
    "                FP=FP_, FN=FN_, D=D_, gamma=gamma_, T=T_MAX\n",
    "            )\n",
    "            policy_func = make_data_driven_dp_policy(V, pi_, T=T_MAX)\n",
    "            \n",
    "            # evaluate cost on df_val_fold\n",
    "            df_val_fold[\"risk_bucket\"] = df_val_fold[\"predicted_risk\"].apply(assign_buckets)\n",
    "            stats = simulate_policy(df_val_fold, policy_func)\n",
    "            fold_costs.append(stats['cost'])\n",
    "        \n",
    "        avg_cost = np.mean(fold_costs)\n",
    "        results_dp_cv.append({\n",
    "            \"dp_params\": dp_params,\n",
    "            \"avg_cost\": avg_cost\n",
    "        })\n",
    "    \n",
    "    df_results_dp_cv = pd.DataFrame(results_dp_cv)\n",
    "    best_dp_idx = df_results_dp_cv['avg_cost'].idxmin()\n",
    "    final_dp_params = df_results_dp_cv.loc[best_dp_idx, \"dp_params\"]\n",
    "    final_dp_cost   = df_results_dp_cv.loc[best_dp_idx, \"avg_cost\"]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final chosen ML: {final_ml_type} {final_ml_params}, avg AUC={final_ml_auc:.3f}\")\n",
    "        print(f\"Final chosen DP params: {final_dp_params}, avg cost={final_dp_cost:.3f}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # (E) Retrain on G1..G_n with final ML => evaluate on G_{n+1}\n",
    "    # -------------------------------------------------------------------------\n",
    "    train_all = []\n",
    "    for i in range(1, n_splits+1):\n",
    "        train_all.append(group_dfs[f\"G{i}\"])\n",
    "    df_train_all = pd.concat(train_all, ignore_index=True).copy()\n",
    "    \n",
    "    X_train_all = df_train_all[FEATURE_COLS]\n",
    "    y_train_all = df_train_all['label']\n",
    "    \n",
    "    if final_ml_type == \"catboost\":\n",
    "        final_model = CatBoostClassifier(**final_ml_params, verbose=False)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    elif final_ml_type == \"rf\":\n",
    "        final_model = RandomForestClassifier(**final_ml_params, random_state=42)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    else:\n",
    "        final_model = GradientBoostingClassifier(**final_ml_params, random_state=42)\n",
    "        final_model.fit(X_train_all, y_train_all)\n",
    "    \n",
    "    # Predict risk for train to build DP transitions\n",
    "    df_train_all[\"predicted_risk\"] = final_model.predict_proba(X_train_all)[:,1]\n",
    "    df_train_all[\"risk_bucket\"]    = df_train_all[\"predicted_risk\"].apply(assign_buckets)\n",
    "    \n",
    "    # Build DP with final_dp_params\n",
    "    gamma_ = final_dp_params.get(\"gamma\", GAMMA)\n",
    "    D_  = final_dp_params.get(\"D\", D_COST)\n",
    "    FP_ = final_dp_params.get(\"FP\", FP_COST)\n",
    "    FN_ = final_dp_params.get(\"FN\", FN_COST)\n",
    "    \n",
    "    p_trans, p_sick = estimate_transition_and_sick_probs(df_train_all, T=T_MAX)\n",
    "    V, pi_ = train_data_driven_dp(\n",
    "        p_trans, p_sick,\n",
    "        FP=FP_, FN=FN_, D=D_, gamma=gamma_, T=T_MAX\n",
    "    )\n",
    "    dp_final_policy = make_data_driven_dp_policy(V, pi_, T=T_MAX)\n",
    "    \n",
    "    # Evaluate on G_{n+1}\n",
    "    df_test_eval = df_test.copy()\n",
    "    X_test_eval  = df_test_eval[FEATURE_COLS]\n",
    "    df_test_eval[\"predicted_risk\"] = final_model.predict_proba(X_test_eval)[:,1]\n",
    "    \n",
    "    # Benchmark methods:\n",
    "    best_thr_const, stats_const = constant_threshold_search(df_test_eval)\n",
    "    best_dyn_vec, stats_dyn     = dynamic_threshold_random_search(df_test_eval)\n",
    "    (bestA,bestB), stats_lin    = linear_threshold_search(df_test_eval)\n",
    "    best_thr_wte, stats_wte     = wait_till_end_search(df_test_eval)\n",
    "    \n",
    "    df_test_eval[\"risk_bucket\"] = df_test_eval[\"predicted_risk\"].apply(assign_buckets)\n",
    "    stats_dp = simulate_policy(df_test_eval, dp_final_policy)\n",
    "    \n",
    "    final_table = pd.DataFrame({\n",
    "        \"Method\": [\n",
    "            \"Constant Threshold\",\n",
    "            \"Dynamic Threshold-R\",\n",
    "            \"Linear Threshold\",\n",
    "            \"Wait Till End\",\n",
    "            \"Dynamic Threshold-DP\"\n",
    "        ],\n",
    "        \"Precision (%)\": [\n",
    "            100*stats_const['precision'],\n",
    "            100*stats_dyn['precision'],\n",
    "            100*stats_lin['precision'],\n",
    "            100*stats_wte['precision'],\n",
    "            100*stats_dp['precision']\n",
    "        ],\n",
    "        \"Cost\": [\n",
    "            stats_const['cost'],\n",
    "            stats_dyn['cost'],\n",
    "            stats_lin['cost'],\n",
    "            stats_wte['cost'],\n",
    "            stats_dp['cost']\n",
    "        ],\n",
    "        \"Recall (%)\": [\n",
    "            100*stats_const['recall'],\n",
    "            100*stats_dyn['recall'],\n",
    "            100*stats_lin['recall'],\n",
    "            100*stats_wte['recall'],\n",
    "            100*stats_dp['recall']\n",
    "        ],\n",
    "        \"Treatment Time\": [\n",
    "            stats_const['avg_treatment_time'],\n",
    "            stats_dyn['avg_treatment_time'],\n",
    "            stats_lin['avg_treatment_time'],\n",
    "            stats_wte['avg_treatment_time'],\n",
    "            stats_dp['avg_treatment_time']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"ml_cv_details\": df_ml_cv_details,\n",
    "        \"dp_cv_details\": df_dp_cv_details,\n",
    "        \"ml_final_choice\": (final_ml_type, final_ml_params, final_ml_auc),\n",
    "        \"dp_final_choice\": (final_dp_params, final_dp_cost),\n",
    "        \"test_results_table\": final_table\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 9. RUN MULTIPLE REPLICATIONS\n",
    "###############################################################################\n",
    "def run_multiple_replications(df_all, n_replications=30, n_splits=4):\n",
    "    \"\"\"\n",
    "    Run Algorithm 3 multiple times with different random seeds.\n",
    "    Compute mean and standard deviation for each metric.\n",
    "    \"\"\"\n",
    "    # Define standard method names for consistent reporting\n",
    "    standard_methods = [\n",
    "        'Constant Threshold',\n",
    "        'Dynamic Threshold-R',\n",
    "        'Linear Threshold',\n",
    "        'Wait Till End',\n",
    "        'Dynamic Threshold-DP'\n",
    "    ]\n",
    "    \n",
    "    # Initialize containers for each metric and method\n",
    "    precision_values = {method: [] for method in standard_methods}\n",
    "    cost_values = {method: [] for method in standard_methods}\n",
    "    recall_values = {method: [] for method in standard_methods}\n",
    "    treatment_time_values = {method: [] for method in standard_methods}\n",
    "    \n",
    "    for i in range(n_replications):\n",
    "        seed = i  # Use a different seed for each replication\n",
    "        print(f\"\\nRunning replication {i+1}/{n_replications} with seed={seed}\")\n",
    "        \n",
    "        # Run Algorithm 3 with current seed\n",
    "        results = run_experiment_algorithm3(\n",
    "            df_all=df_all, \n",
    "            n_splits=n_splits, \n",
    "            seed=seed,\n",
    "            verbose=False  # Turn off verbose output for cleaner console\n",
    "        )\n",
    "        \n",
    "        # Extract final test results table\n",
    "        test_table = results[\"test_results_table\"]\n",
    "        \n",
    "        # Extract values for each method\n",
    "        for _, row in test_table.iterrows():\n",
    "            method = row['Method']\n",
    "            \n",
    "            if method in standard_methods:\n",
    "                precision_values[method].append(row['Precision (%)'])\n",
    "                cost_values[method].append(row['Cost'])\n",
    "                recall_values[method].append(row['Recall (%)'])\n",
    "                treatment_time_values[method].append(row['Treatment Time'])\n",
    "    \n",
    "    # Compute statistics\n",
    "    final_data = []\n",
    "    for method in standard_methods:\n",
    "        if precision_values[method]:  # Check if we have data for this method\n",
    "            precision_mean = np.mean(precision_values[method])\n",
    "            precision_std = np.std(precision_values[method])\n",
    "            cost_mean = np.mean(cost_values[method])\n",
    "            cost_std = np.std(cost_values[method])\n",
    "            recall_mean = np.mean(recall_values[method])\n",
    "            recall_std = np.std(recall_values[method])\n",
    "            treat_time_mean = np.mean(treatment_time_values[method])\n",
    "            treat_time_std = np.std(treatment_time_values[method])\n",
    "            \n",
    "            final_data.append({\n",
    "                'Method': method,\n",
    "                'Precision (%)': f\"{precision_mean:.2f} ± {precision_std:.2f}\",\n",
    "                'Cost': f\"{cost_mean:.2f} ± {cost_std:.2f}\",\n",
    "                'Recall (%)': f\"{recall_mean:.2f} ± {recall_std:.2f}\",\n",
    "                'Treatment Time': f\"{treat_time_mean:.2f} ± {treat_time_std:.2f}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(final_data)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 10. MAIN SCRIPT \n",
    "###############################################################################\n",
    "def main():\n",
    "   \n",
    "    df_all = pd.read_csv(\"synthetic_patients_with_features.csv\")\n",
    "    print(f\"Total patients: {df_all['patient_id'].nunique()}\")\n",
    "    print(f\"Columns: {list(df_all.columns)}\")\n",
    "\n",
    "    # 2) Run multiple replications\n",
    "    n_replications = 1\n",
    "    n_splits = 4\n",
    "    final_results = run_multiple_replications(df_all, n_replications=n_replications, n_splits=n_splits)\n",
    "    \n",
    "    # 3) Print final results\n",
    "    print(f\"\\n=== FINAL RESULTS (Mean ± Std Dev over {n_replications} Replications, Algorithm 3) ===\")\n",
    "    print(final_results.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb5d54-3ab0-4933-a0ee-e35b40628dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
